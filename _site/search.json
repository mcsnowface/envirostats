[
  {
    "objectID": "about.html#stefan-schreiber-ph.d.-p.biol.",
    "href": "about.html#stefan-schreiber-ph.d.-p.biol.",
    "title": "Who we are",
    "section": "Stefan Schreiber, Ph.D., P.Biol.",
    "text": "Stefan Schreiber, Ph.D., P.Biol.\nStefan holds a Ph.D. in Forest Biology and Management from the University of Alberta and is a certified Professional Biologist with the Alberta Society of Professional Biologists. Prior to his time in Canada, Stefan pursued his Bachelor’s and Master’s degree in Biology at Ruhr-University Bochum, specializing in plant ecology and molecular phylogeny. His passion are trees, specifically understanding their adaptive mechanisms to diverse environments. In addition to his academic pursuits, Stefan sharpened his skills in quantitative methods and statistics, establishing a robust foundation in R programming throughout his career. Beyond his work life, Stefan cherishes every moment with his wife and three children. He also has a passion for ultra-endurance sports, embodying a resilient spirit both in his academic and work-related endeavors, as well as personal pursuits."
  },
  {
    "objectID": "about.html#sanja-schreiber-ph.d.-m.sc.-b.sc",
    "href": "about.html#sanja-schreiber-ph.d.-m.sc.-b.sc",
    "title": "Who we are",
    "section": "Sanja Schreiber, Ph.D., M.Sc., B.Sc",
    "text": "Sanja Schreiber, Ph.D., M.Sc., B.Sc\nSanja is a clinician with a Ph.D. in Rehabilitation Science and a Postdoctorate in Evidence Synthesis, both from University of Alberta. She has extensive expertise in clinical trial design and methodology, conduct, and reporting, with an interest in bridging gaps between research and practice. She is an expert in evidence synthesis methodologies, including scoping reviews, systematic reviews, Cochrane systematic reviews, overviews of reviews and living systematic reviews, where she uses software technology to expedite review production and minimize human error. Employing the same rigorous methodology utilized in medical research, she directs environmental evidence syntheses production at EnviroStats Solutions, facilitating evidence-based policymaking and addressing complex environmental decisions. Outside of work, Sanja loves travel adventures with her husband and three children."
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Quantitative Environmental Services",
    "section": "",
    "text": "At EnviroStats Solutions, our focus centers on applying the most suitable and current analyses and visualizations to environmental data. We are well-versed in both traditional statistical methods and Bayesian statistics. Our day-to-day modeling involves a range of techniques:\n\nLinear models\nGeneralized linear models\nLinear mixed effects models\nGeneralized linear mixed effects models\nModels accommodating zero and one inflated data\nClassification and regression tree analyses\nMultivariate analyses, inclusive principal component analysis, linear discriminant analysis, and non-linear multidimensional scaling techniques\nGenerating Shiny Web Apps for easy visualization of data\nEvidence syntheses through scoping and systematic reviews using narrative or meta-analyses\n\nIn addition to our analytical work, we have ample experience in offering tailored R teaching workshops. These workshops cater to your specific requirements, providing participants with fundamental and advanced training in R, statistics, and data visualization."
  },
  {
    "objectID": "services.html#our-services",
    "href": "services.html#our-services",
    "title": "Quantitative Environmental Services",
    "section": "",
    "text": "At EnviroStats Solutions, our focus centers on applying the most suitable and current analyses and visualizations to environmental data. We are well-versed in both traditional statistical methods and Bayesian statistics. Our day-to-day modeling involves a range of techniques:\n\nLinear models\nGeneralized linear models\nLinear mixed effects models\nGeneralized linear mixed effects models\nModels accommodating zero and one inflated data\nClassification and regression tree analyses\nMultivariate analyses, inclusive principal component analysis, linear discriminant analysis, and non-linear multidimensional scaling techniques\nGenerating Shiny Web Apps for easy visualization of data\nEvidence syntheses through scoping and systematic reviews using narrative or meta-analyses\n\nIn addition to our analytical work, we have ample experience in offering tailored R teaching workshops. These workshops cater to your specific requirements, providing participants with fundamental and advanced training in R, statistics, and data visualization."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#why-the-normal-distribution-matters",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#why-the-normal-distribution-matters",
    "title": "Understanding Frequentist Inference",
    "section": "Why the Normal Distribution Matters",
    "text": "Why the Normal Distribution Matters\nNotice how closely the empirical sampling distribution above matches the theoretical normal curve (red line). This is no coincidence – it’s a consequence of the Central Limit Theorem (CLT), which states that the sampling distribution of the mean tends toward a normal distribution as sample size increases, even if the underlying data aren’t normally distributed, as long as the observations are independent and identically distributed (Casella & Berger, 2002).\nThis distribution, also known as the Gaussian distribution after Carl Friedrich Gauss who studied it extensively in the early 19th century (though it was discovered earlier by de Moivre (De Moivre, 1738; Stigler, 1986)), has remarkable properties that make it central to statistical inference.\nThis normality of the sampling distribution is fundamental because:\n\nIt’s mathematically well-defined: The normal distribution is symmetric with known properties, allowing us to calculate precise probabilities.\nIt enables p-value calculations: We can determine exactly how extreme our observed statistic is under the null hypothesis.\nIt justifies confidence intervals: We know what multiplier to use (from the t-distribution) to achieve desired coverage probabilities.\nIt provides known error rates: Our Type I error rate (\\(\\alpha\\)) is controlled at exactly the level we specify.\n\nWhen assumptions fail: If the \\(i.i.d.\\) assumptions are violated – such as with strong dependence between observations, severe skewness combined with small sample sizes, or non-identical distributions – the sampling distribution may not follow this normal shape. In such cases, our p-values, confidence intervals, and error rates become unreliable. This is why checking assumptions and considering robust or alternative methods is crucial when working with real data.\nNow that we understand that the sampling distribution is approximately normal (thanks to the CLT), we can leverage this mathematical property to make two types of inferential statements. First, we can test specific hypotheses about parameter values. Second, we can construct intervals that quantify our uncertainty. Let’s examine hypothesis testing first."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#how-confidence-intervals-are-constructed",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#how-confidence-intervals-are-constructed",
    "title": "Understanding Frequentist Inference",
    "section": "How Confidence Intervals Are Constructed",
    "text": "How Confidence Intervals Are Constructed\nRecall that the standard error (SE) measures the expected variability of the sample mean across repeated samples. We calculate it as \\(\\text{SE} = s/\\sqrt{n}\\), where \\(s\\) is the sample standard deviation and \\(n\\) is the sample size.\nTo build a confidence interval, we take our sample mean and add/subtract a margin of error:\n\\[\\text{CI} = \\bar{x} \\pm (\\text{critical value} \\times \\text{SE})\\]\nThe critical value is a multiplier that determines how wide our interval should be. Think of it as answering the question: “How many standard errors away from our estimate should we go to capture the true parameter with 95% confidence?”\nBut here’s where a subtle complication arises…"
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#from-theory-to-practice-why-we-need-the-t-distribution",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#from-theory-to-practice-why-we-need-the-t-distribution",
    "title": "Understanding Frequentist Inference",
    "section": "From Theory to Practice: Why We Need the t-Distribution",
    "text": "From Theory to Practice: Why We Need the t-Distribution\nSo far, we’ve been using the formula \\(\\text{SE} = s/\\sqrt{n}\\) as if it were straightforward. But there’s a subtle issue here: we’re using the sample standard deviation \\(s\\) to estimate the population standard deviation \\(\\sigma\\). This creates a problem.\nWhen we simulate the sampling distribution (as we did earlier), we know the true population parameters. But in real data analysis, we never know \\(\\sigma\\) – we can only estimate it from our sample. This estimation adds an extra source of uncertainty that the normal distribution doesn’t account for.\nEnter the t-distribution. William Sealy Gosset (writing under the pseudonym “Student”) discovered that when we replace \\(\\sigma\\) with \\(s\\), the sampling distribution is no longer exactly normal – it follows what we now call Student’s t-distribution (Student, 1908). The key features are:\n\nIt has heavier tails than the normal distribution (more probability in the extremes).\nIt’s wider than the normal distribution, reflecting our additional uncertainty.\nAs sample size increases, it converges to the normal distribution.\nIt has a parameter called “degrees of freedom” (\\(df = n - 1\\)), which controls how wide it is.\n\nFor small samples, using the normal distribution would give us confidence intervals that are too narrow – they’d capture the true parameter less than 95% of the time. The t-distribution corrects for this, giving us appropriately wider intervals that maintain the correct coverage probability.\nLet’s see this difference visually:\n\n\nShow code\nlibrary(tidyverse)\n\n# Generate data comparing normal and t-distributions\nx &lt;- seq(-4, 4, length.out = 500)\ndf_compare &lt;- tibble(\n  x = x,\n  Normal = dnorm(x),\n  `t (df=5)` = dt(x, df = 5),      # Small sample: heavier tails\n  `t (df=30)` = dt(x, df = 30)     # Larger sample: approaches normal\n) |&gt; \n  pivot_longer(cols = -x, names_to = \"distribution\", values_to = \"density\")\n\n# Plot comparison\nggplot(df_compare, aes(x = x, y = density, color = distribution)) +\n  geom_line(linewidth = 1) +\n  labs(\n    title = \"Comparing Normal and t-Distributions\",\n    subtitle = \"The t-distribution has heavier tails, especially with small sample sizes\",\n    x = \"Value\",\n    y = \"Density\",\n    color = \"Distribution\"\n  ) +\n  scale_color_manual(values = c(\"Normal\" = \"black\", \"t (df=5)\" = \"#d95f02\", \"t (df=30)\" = \"#1b9e77\")) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nNotice how the t-distribution with 5 degrees of freedom (orange) has much heavier tails than the normal distribution (black). With 30 degrees of freedom (green), it’s already quite close to normal. This is why the t-distribution matters most for small samples."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#what-a-confidence-interval-really-means",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#what-a-confidence-interval-really-means",
    "title": "Understanding Frequentist Inference",
    "section": "What a Confidence Interval Really Means",
    "text": "What a Confidence Interval Really Means\nNow that we understand why we use the t-distribution instead of the normal distribution, let’s clarify what our resulting confidence intervals actually mean – and more importantly, what they don’t mean.\nA 95% confidence interval does not mean there is a 95% probability that the true mean lies within a particular observed interval. In the frequentist framework, the parameter is fixed (though unknown), so it either is or isn’t in any particular interval – there’s no probability involved once we’ve observed our data.\nInstead, the 95% refers to the procedure: if we repeated the entire sampling and interval-construction process infinitely many times, about 95% of those intervals would contain the true population mean. Our single observed interval is just one realization from this process.\nNow let’s see this in action with a simulation:\n\n\nShow code\nlibrary(tidyverse)\n\n# Function to simulate one sample and compute its 95% CI\nsimulation &lt;- function(n, mu, stdev) {\n  s &lt;- rnorm(n, mu, stdev)\n  sample_mean &lt;- mean(s)\n  sample_sd &lt;- sd(s)\n  sample_se &lt;- sample_sd / sqrt(n)\n  confint_95 &lt;- qt(0.975, df = n - 1) * sample_se  # t-critical value × SE\n  tibble(\n    n = n,\n    sample_mean = sample_mean,\n    sample_sd = sample_sd,\n    sample_se = sample_se,\n    confint_95 = confint_95,\n    lower_bound = sample_mean - confint_95,\n    upper_bound = sample_mean + confint_95,\n    contains_mu = mu &gt;= sample_mean - confint_95 & mu &lt;= sample_mean + confint_95\n  )\n}\n\n# Set parameters and run simulation\nset.seed(777)\nn &lt;- 10; mu &lt;- 0; stdev &lt;- 5\nsim &lt;- 100\n\n# Generate 100 confidence intervals\ndraws &lt;- map(1:sim, ~ simulation(n, mu, stdev)) |&gt;  \n  bind_rows() |&gt;  \n  mutate(experiment_id = 1:sim)\n\n# Calculate observed coverage rate\ncoverage &lt;- mean(draws$contains_mu)\n\n# Visualize confidence intervals\nggplot(draws, aes(x = sample_mean, y = experiment_id, color = contains_mu)) + \n  geom_point(size = 3) + \n  geom_errorbarh(aes(xmin = lower_bound, xmax = upper_bound), linewidth = 1, alpha = 0.5) + \n  geom_vline(xintercept = mu, linetype = \"dashed\", color = \"grey40\") +\n  scale_color_manual(\n    name = \"95% Confidence Interval\",\n    values = c(\"TRUE\" = \"#1b9e77\", \"FALSE\" = \"#d95f02\"),\n    labels = c(\"Captures true mean\", \"Misses true mean\")\n  ) +\n  labs(\n    title = \"95% Confidence Intervals Across 100 Simulated Experiments\",\n    subtitle = sprintf(\"Teal = captures μ, Orange = misses μ | Observed coverage: %.0f%%\", \n                      coverage * 100),\n    x = \"Sample mean\",\n    y = \"Experiment ID\",\n    caption = \"Dashed line marks the true population mean (μ = 0)\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nThe practical challenge: While this frequentist interpretation is logically sound, it doesn’t directly answer the question most researchers want answered: “What can I say about my single observed interval?” This tension between the procedure’s long-run properties and what we can infer from one dataset is an inherent limitation of the frequentist approach."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#interpreting-the-plots",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#interpreting-the-plots",
    "title": "Understanding Frequentist Inference",
    "section": "Interpreting the Plots",
    "text": "Interpreting the Plots\nTop panel (\\(n = 30\\) - Underpowered):\n\nThe orange curve shows the sampling distribution if \\(H_0: \\mu = 100\\) is true.\nThe green curve shows the sampling distribution if \\(H_1: \\mu = 103\\) is true.\nStandard error is \\(\\text{SE} \\approx 1.83\\), creating relatively wide, overlapping distributions.\nThe orange shaded regions (in the tails under \\(H_0\\)) show Type I error rate \\(\\alpha = 0.05\\).\nThe teal shaded region (under \\(H_1\\), within the non-rejection region) shows Type II error rate \\(\\beta \\approx 0.63\\) - meaning if the true mean is 103, we’d fail to detect this effect 63% of the time.\nThe purple shaded regions (under \\(H_1\\), in the rejection regions) show power \\(\\approx 0.37\\) - only a 37% chance of correctly detecting the true effect.\n\nBottom panel (\\(n = 200\\) - Overpowered):\n\nSame hypotheses, but now \\(\\text{SE} \\approx 0.71\\) (much smaller due to very large sample size).\nThe distributions are extremely narrow with almost no overlap.\nThe teal region (\\(\\beta\\)) is now virtually invisible, with \\(\\beta \\approx 0.01\\) – we’d almost never miss the true effect.\nThe purple regions (power) dominate the \\(H_1\\) distribution, with power \\(\\approx 0.99\\) – nearly 100% chance of correctly detecting the true effect.\n\nKey observations:\nThe comparison reveals an important trade-off in study design. This tension between statistical and practical significance is particularly problematic in fields where underpowered studies are common (Button et al., 2013), leading to inflated effect size estimates and poor replication rates:\n\nToo small (\\(n = 30\\)): High risk of missing real effects, wasting research effort and potentially leading to false conclusions.\nToo large (\\(n = 200\\)): Excessive power means collecting far more data than necessary, wasting time, money, and resources.\n\nWhile the underpowered study clearly has problems, the overpowered study also raises concerns. Beyond a certain point, additional data provides diminishing returns – power increases very little while costs continue to mount. Moreover, with very large samples, even trivially small effects become “statistically significant,” which doesn’t necessarily mean they’re practically meaningful.\nThe Goldilocks principle of sample size: We need a sample size that’s “just right” – large enough to reliably detect effects that matter, but not so large that we waste resources or detect effects too small to care about. This is why prospective power analysis is essential for efficient research design."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#planning-sample-size-through-prospective-power-analysis",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#planning-sample-size-through-prospective-power-analysis",
    "title": "Understanding Frequentist Inference",
    "section": "Planning Sample Size Through Prospective Power Analysis",
    "text": "Planning Sample Size Through Prospective Power Analysis\nThe side-by-side comparison above revealed a critical challenge: too little power (\\(n = 30\\)) risks missing real effects, while excessive power (\\(n = 200\\)) wastes resources and may detect effects too trivial to matter. This raises a practical question: rather than guessing what constitutes an adequate sample size, how can we systematically determine the optimal n for our specific goals?\nThe answer lies in prospective power analysis – plotting the relationship between sample size and power to find the sweet spot:\n\n\nShow code\nlibrary(tidyverse)\n\n# Define parameters (matching previous visualization)\nmu_null &lt;- 100\nmu_alt &lt;- 103\nsd_pop &lt;- 10\nalpha &lt;- 0.05\n\n# Function to calculate statistical power for a given sample size\ncalculate_power &lt;- function(n, mu_null, mu_alt, sd_pop, alpha = 0.05) {\n  se &lt;- sd_pop / sqrt(n)\n  t_crit &lt;- qt(1 - alpha / 2, df = n - 1)\n  lower_crit &lt;- mu_null - t_crit * se\n  upper_crit &lt;- mu_null + t_crit * se\n  \n  # Power = probability of rejecting H0 when H1 is true\n  power &lt;- pnorm(lower_crit, mean = mu_alt, sd = se) + \n           (1 - pnorm(upper_crit, mean = mu_alt, sd = se))\n  return(power)\n}\n\n# Calculate power across range of sample sizes\nsample_sizes &lt;- seq(10, 200, by = 5)\npower_values &lt;- map_dbl(sample_sizes, ~calculate_power(.x, mu_null, mu_alt, sd_pop))\npower_df &lt;- tibble(n = sample_sizes, power = power_values)\n\n# Find sample size needed for conventional 80% power\nn_for_80_power &lt;- sample_sizes[which.min(abs(power_values - 0.80))]\n\n# Calculate power at our example sample sizes\npower_at_n30 &lt;- calculate_power(30, mu_null, mu_alt, sd_pop)\npower_at_n200 &lt;- calculate_power(200, mu_null, mu_alt, sd_pop)\n\n# Plot power curve\nggplot(power_df, aes(x = n, y = power)) +\n  geom_line(color = \"#1b9e77\", linewidth = 1.2) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"grey40\") +\n  annotate(\"point\", x = 30, y = power_at_n30, color = \"#d95f02\", size = 4) +\n  annotate(\"point\", x = 200, y = power_at_n200, color = \"#d95f02\", size = 4) +\n  annotate(\"point\", x = n_for_80_power, y = 0.80, color = \"#7570b3\", size = 4) +\n  annotate(\"text\", x = 30, y = power_at_n30 - 0.08, \n           label = sprintf(\"n=30\\nPower≈%.0f%%\", power_at_n30*100), \n           color = \"#d95f02\", size = 3.5, fontface = \"bold\") +\n  annotate(\"text\", x = 200, y = power_at_n200 - 0.08, \n            label = sprintf(\"n=200\\nPower≈%.0f%%\", power_at_n200*100), \n            color = \"#d95f02\", size = 3.5, fontface = \"bold\") +\n  annotate(\"text\", x = n_for_80_power, y = 0.88, \n           label = sprintf(\"n≈%d needed\\nfor 80%% power\", n_for_80_power), \n           color = \"#7570b3\", size = 3.5, fontface = \"bold\") +\n  scale_x_continuous(limits = c(0, 210)) +\n  labs(\n    title = \"Statistical Power as a Function of Sample Size\",\n    subtitle = expression(paste(\"Effect size: \", mu[1], \" - \", mu[0], \" = 3 (Cohen's d ≈ 0.3)\")),\n    x = \"Sample size (n)\",\n    y = expression(paste(\"Power (1 - \", beta, \")\")),\n    caption = \"Orange points = our examples; Purple point = sample size for 80% power\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\nKey insights:\n\nAt \\(n = 30\\), power is only about 38% – inadequate for reliable detection.\nAt \\(n = 200\\), power is nearly 100% – far more than needed, representing wasted resources.\nTo achieve conventional 80% power for this effect size, we’d need approximately \\(n \\approx 85\\).\nLarger sample sizes increase power by decreasing the standard error (\\(\\text{SE} = s/\\sqrt{n}\\)). Smaller SE means narrower. sampling distributions, which creates better separation between the null and alternative distributions, reducing overlap and making true effects easier to detect.\nThis is why prospective power analysis is crucial for study design – it helps us determine the sample size needed to detect effects of meaningful magnitude with adequate probability (Cohen, 1988).\n\nPractical lesson: The error and power visualization showed us the problem (low power with \\(n = 30\\)), and this plot shows us the solution (increase sample size to approximately 85). This demonstrates why researchers should conduct power analyses before collecting data, not after!\nNow that we understand how to plan studies and interpret individual tests, we need to confront a more subtle challenge: what happens when we conduct not just one test, but many? And beyond statistical significance, how do we assess whether effects actually matter in practice?"
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#multiple-testing-inflates-false-positives",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#multiple-testing-inflates-false-positives",
    "title": "Understanding Frequentist Inference",
    "section": "Multiple Testing Inflates False Positives",
    "text": "Multiple Testing Inflates False Positives\nImportant distinction: This problem applies to multiple tests on the same data, not to independent replication studies. If different researchers independently test the same hypothesis on different datasets, those p-values shouldn’t be corrected for multiple testing – that’s replication, which is exactly what science needs!\nWhen conducting multiple hypothesis tests within the same study or dataset, a critical issue emerges: false positives accumulate. If you conduct 100 tests on the same dataset where all null hypotheses are true and use a significance level of \\(\\alpha = 0.05\\), you expect about 5 false positives, as predicted by the Type I error rate.\nExamples where correction is needed:\n\nTesting 20 different biomarkers to see which predict disease.\nComparing 5 treatment groups (resulting in 10 pairwise comparisons).\nTesting the same intervention across 15 different outcomes.\nExploratory analysis where you examine many potential relationships.\n\nWhy this matters:\n\nWithout correction, the probability of getting at least one false positive increases dramatically.\nFor example, with 20 tests on the same dataset at \\(\\alpha = 0.05\\), there’s a 64% chance of at least one false positive even if all null hypotheses are true (\\(1 - (1-0.05)^{20} = 1 - 0.95^{20} \\approx 0.64\\)).\nThis is why multiple testing corrections (Bonferroni, FDR control (Benjamini & Hochberg, 1995), Holm-Bonferroni) are essential.\n\nPractical implication: When you conduct multiple tests, either:\n\nApply appropriate corrections (and accept reduced power).\nClearly distinguish exploratory vs. confirmatory analyses.\nPre-register your primary hypothesis to avoid the “multiple testing” critique.\n\nThe “replication crisis” in many sciences is partly attributable to (Ioannidis, 2005):\n\nRunning many tests but only reporting the significant ones.\nNot correcting for multiple comparisons.\nTreating exploratory findings as if they were confirmatory.\n\nA single \\(p &lt; 0.05\\) result should never be considered definitive evidence. Replication, effect sizes, and consideration of prior plausibility are all crucial."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#statistical-vs.-practical-significance",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#statistical-vs.-practical-significance",
    "title": "Understanding Frequentist Inference",
    "section": "Statistical vs. Practical Significance",
    "text": "Statistical vs. Practical Significance\nAnother crucial distinction: statistical significance is not the same as practical importance. With a large enough sample size, even tiny, meaningless differences will yield \\(p &lt; 0.05\\). For example:\n\n\nShow code\nlibrary(tidyverse)\nlibrary(knitr)\n\n# Simulate a statistically significant but practically meaningless result\nset.seed(999)\nn_large &lt;- 10000\nsmall_effect &lt;- 0.05  # Negligible effect: 0.05 SD difference\n\n# Generate two groups with trivial difference\ngroup1 &lt;- rnorm(n_large, mean = 0, sd = 1)\ngroup2 &lt;- rnorm(n_large, mean = small_effect, sd = 1)\n\n# Conduct t-test\nt_result &lt;- t.test(group1, group2)\n\n# Calculate effect size\ncohens_d &lt;- small_effect / 1\n\n# Summarize results\nresults_df &lt;- tibble(\n  Metric = c(\"Sample size per group\", \"Cohen's d (effect size)\", \"p-value\", \"Mean difference\"),\n  Value = c(\n    format(n_large, big.mark = \",\"),\n    sprintf(\"%.3f\", cohens_d),\n    sprintf(\"%.6f\", t_result$p.value),\n    sprintf(\"%.3f\", diff(t_result$estimate))\n  )\n)\n\nkable(results_df, align = c(\"l\", \"r\"))\n\n\n\n\n\nMetric\nValue\n\n\n\n\nSample size per group\n10,000\n\n\nCohen’s d (effect size)\n0.050\n\n\np-value\n0.000049\n\n\nMean difference\n0.058\n\n\n\n\n\nThis hypothetical example shows that with 10,000 observations per group, we can detect an effect size of only 0.05 standard deviations with high confidence – but such a small effect is likely meaningless in most practical contexts.\nKey principle: Always report and interpret effect sizes alongside p-values (Cohen, 1994). Ask not just “Is there an effect?” but “How large is the effect, and does that matter?”\nThis tension between statistical and practical significance naturally raises another question: under what conditions can we trust these inferential procedures? Throughout this post, we’ve repeatedly invoked assumptions like independence and normality. It’s time to examine what happens when these assumptions break down."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#when-is-frequentist-inference-robust",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#when-is-frequentist-inference-robust",
    "title": "Understanding Frequentist Inference",
    "section": "When is frequentist inference robust?",
    "text": "When is frequentist inference robust?\n\nModerate departures from normality are often acceptable for means (due to Central Limit Theorem), especially with larger samples (\\(n &gt; 30\\) as a rough guideline).\nUnequal variances can be addressed with Welch’s \\(t\\)-test instead of Student’s \\(t\\)-test.\nMild dependence might inflate Type I error rates but not completely invalidate inference."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#when-do-we-need-alternative-approaches",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#when-do-we-need-alternative-approaches",
    "title": "Understanding Frequentist Inference",
    "section": "When do we need alternative approaches?",
    "text": "When do we need alternative approaches?\n\nStrong dependence (time series, clustered data) requires specialized methods like mixed models (Pinheiro & Bates, 2000) or generalized estimating equations (Liang & Zeger, 1986).\nSevere non-normality with small samples might require non-parametric tests, or better yet, generalized linear models (GLMs) which can directly model non-normal response distributions without transformation (Bolker et al., 2009).\nHeteroscedasticity (unequal variances) with unequal group sizes can seriously distort inferences, but can be modeled explicitly using generalized least squares or variance structures in mixed models (Pinheiro & Bates, 2000).\n\nPractical advice: Always examine your data visually, check diagnostic plots, and consider whether your assumptions are reasonable. Statistical methods are tools that work best when their assumptions are at least approximately met."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#key-takeaways",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#key-takeaways",
    "title": "Understanding Frequentist Inference",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nFrequentist inference is about procedures, not single-case probabilities.\nThe sampling distribution and standard error are the foundation – they quantify how much our estimates would vary across repeated samples.\nThe t-distribution accounts for the additional uncertainty when we estimate the population standard deviation from sample data.\nConfidence intervals and hypothesis tests are mathematically linked procedures using the same sampling distribution machinery.\nP-values measure compatibility with a specific null model, not truth or importance.\nEffect sizes matter as much as (or more than) p-values for scientific interpretation.\nSample size planning via power analysis is essential for efficient research – too little power wastes effort, too much power wastes resources.\nMultiple testing inflates false positive rates – correct for this when appropriate.\nAssumptions matter, but many methods are reasonably robust to moderate violations.\nReplication and cumulative evidence are more reliable than single p-values.\n\nAll these tools rely on the framework of hypothetical repeated sampling and assumptions about the data-generating process. They give us a powerful and well-understood framework for reasoning about populations from samples – but they must be interpreted carefully and in proper context.\nA natural question emerges: Is there an alternative framework that addresses some of these interpretational challenges – particularly the indirect nature of probability statements in the frequentist approach? This brings us to Bayesian inference."
  },
  {
    "objectID": "mission.html",
    "href": "mission.html",
    "title": "Quantitative Environmental Services",
    "section": "",
    "text": "Solid scientific inference and the foundations of evidence-based decision-making are rooted in data that precisely address the core questions and objectives at hand. This underscores the critical importance of meticulously planned study designs and targeted data collection. As the stakes rise, so does the demand for confidence. Building this confidence hinges on:\n\nThoughtful Experimental Design:\nCrafting the right experimental framework tailored to your inquiry.\nAccurate Data Analysis:\nApplying sound analytical techniques that draw out reliable insights.\nTransparent Visualization:\nConveying findings truthfully and effectively.\n\nEnviroStats Solutions stands ready to provide expert guidance and consultation in data analysis, visualization, and interpretation, catering to those navigating the landscape of environmental challenges. Backed by an extensive background in both applied and foundational research, we offer comprehensive support for evidence-driven decision-making — guiding you from initial experimental design to conclusive recommendations.\nOur commitment extends beyond consultation. We’ll share our proficiency in data analysis and environmental statistics through tailor-made workshops and training sessions that align with your specific requirements. With over a decade of hands-on experience as dedicated users of the R Environment for Statistical Computing, we’re dedicated to equipping you for success."
  },
  {
    "objectID": "mission.html#our-mission",
    "href": "mission.html#our-mission",
    "title": "Quantitative Environmental Services",
    "section": "",
    "text": "Solid scientific inference and the foundations of evidence-based decision-making are rooted in data that precisely address the core questions and objectives at hand. This underscores the critical importance of meticulously planned study designs and targeted data collection. As the stakes rise, so does the demand for confidence. Building this confidence hinges on:\n\nThoughtful Experimental Design:\nCrafting the right experimental framework tailored to your inquiry.\nAccurate Data Analysis:\nApplying sound analytical techniques that draw out reliable insights.\nTransparent Visualization:\nConveying findings truthfully and effectively.\n\nEnviroStats Solutions stands ready to provide expert guidance and consultation in data analysis, visualization, and interpretation, catering to those navigating the landscape of environmental challenges. Backed by an extensive background in both applied and foundational research, we offer comprehensive support for evidence-driven decision-making — guiding you from initial experimental design to conclusive recommendations.\nOur commitment extends beyond consultation. We’ll share our proficiency in data analysis and environmental statistics through tailor-made workshops and training sessions that align with your specific requirements. With over a decade of hands-on experience as dedicated users of the R Environment for Statistical Computing, we’re dedicated to equipping you for success."
  },
  {
    "objectID": "posts/2024-04-10-evidence-synthesis/index.html",
    "href": "posts/2024-04-10-evidence-synthesis/index.html",
    "title": "The Power of Evidence Synthesis",
    "section": "",
    "text": "Implementation of research findings into practice is generally slow-paced. It is estimated that that it takes 17–20 years to get clinical innovations into practice (Morris, Wooding, and Grant 2011), whereas fewer than 50% of them ever make it into general usage (Bauer and Kirchner 2020). While primary studies are an important first step in generating evidence, knowledge synthesis (evidence synthesis) serve as a bridge that expedite this process, ensuring that valuable insights from primary research reach those who can benefit most from them.\nTake the medical field, for example. Here, evidence synthesis is not just a helpful tool; it’s a cornerstone of clinical decision-making. By systematically accessing, evaluating, and consolidating scientific information, evidence synthesis provides a recognized standard of rigor, objectivity, and transparency. It’s the compass guiding healthcare professionals toward the most effective treatments and interventions.\nInterestingly, while evidence synthesis has long been a staple in medical (https://www.cochrane.org/) and social sciences (https://www.campbellcollaboration.org), its adoption in other fields, like environmental sciences, has been surprisingly slow. Yet, recent discussions in journals such as Trends in Ecology & Evolution (Dicks, Walsh, and Sutherland 2014) and Conservation Biology (Kadykalo et al. 2021) highlight its potential in addressing complex questions surrounding policy-making and controversial decisions. By grounding such decisions in the most reliable evidence available, evidence synthesis becomes a vital tool in navigating the intricate landscape of environmental challenges.\nMaintaining the integrity of evidence synthesis requires clear standards and meticulous attention to detail. Every step of the process, from article search strategy and their selection criteria, to identifying potential biases in the evidence and scrutinizing the synthesis itself, must be conducted with precision. This meticulous process ensures trust and confidence in the findings, allowing end-users to rely on the synthesized evidence with certainty.\nIn the environmental sciences, two common forms of evidence synthesis exist:\n\nScoping reviews (evidence maps) and\nSystematic reviews\n\nScoping reviews are ideal for assessing the breadth of knowledge on a specific topic and identifying gaps in the literature. On the other hand, systematic reviews compile all existing evidence on a particular research question, offering a comprehensive overview of the available literature. Systematic reviews are considered at the top of hierarchy of evidence because they offer higher-quality evidence compared to expert opinions, observational or experimental studies (Figure 1).\n\n\n\n\n\n\nFigure 1: Hierarchy of evidence pyramid\n\n\n\nWhen primary studies exhibit considerable heterogeneity, a narrative synthesis of evidence provides a cohesive description that draws connections between findings. Conversely, when primary studies share similar design and conduct, a quantitative data synthesis like meta-analysis serves as a powerful tool for drawing robust conclusions.\nIn conclusion, evidence synthesis provides clarity in ever-growing research. Whether guiding medical practitioners toward optimal patient care or informing policymakers on environmental matters, its role in bridging the gap between research and practice cannot be overstated. By embracing evidence synthesis, researchers can avoid redundant studies, make better use of existing data, and ensure that decisions are based on the most reliable and comprehensive evidence available. This can help minimize waste in research by directing resources towards studies that are more likely to yield meaningful results and contribute significantly to the advancement of knowledge.\n\nPlease do not hesitate to reach out (%20info at envirostats dot ca) if you have any questions!\n\n\n\n\nReferences\n\nBauer, Mark S., and JoAnn Kirchner. 2020. “Implementation Science: What Is It and Why Should I Care?” Psychiatry Research, VSI:Implementation Science, 283 (January): 112376. https://doi.org/10.1016/j.psychres.2019.04.025.\n\n\nDicks, Lynn V., Jessica C. Walsh, and William J. Sutherland. 2014. “Organising Evidence for Environmental Management Decisions: A ‘4S’ Hierarchy.” Trends in Ecology & Evolution 29 (11): 607–13. https://doi.org/10.1016/j.tree.2014.09.004.\n\n\nKadykalo, Andrew N., Rachel T. Buxton, Peter Morrison, Christine M. Anderson, Holly Bickerton, Charles M. Francis, Adam C. Smith, and Lenore Fahrig. 2021. “Bridging Research and Practice in Conservation.” Conservation Biology 35 (6): 1725–37. https://doi.org/10.1111/cobi.13732.\n\n\nMorris, Zoë Slote, Steven Wooding, and Jonathan Grant. 2011. “The Answer Is 17 Years, What Is the Question: Understanding Time Lags in Translational Research.” Journal of the Royal Society of Medicine 104 (12): 510–20. https://doi.org/10.1258/jrsm.2011.110180."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Quantitative Environmental Services",
    "section": "Welcome!",
    "text": "Welcome!\nEnviroStats Solutions is a dynamic Edmonton-based consulting firm dedicated to delivering efficient and rigorous environmental statistics, data analysis, visualization, and interpretation services. Our focus revolves around the environmental domain, providing practical insights that drive meaningful decisions.\nBeyond our analytical services, we also offer professional training in statistics, data management, and visualization, all centered within the versatile R programming environment. These workshops are designed to empower you to confidently navigate the world of data.\nEager to know more about our journey and the impact we’re making? Don’t hesitate to reach out (%20info at envirostats dot ca)!"
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "",
    "text": "When in doubt, just update your beliefs. ;P"
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#the-fishing-story-bayesian-reasoning-in-daily-life",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#the-fishing-story-bayesian-reasoning-in-daily-life",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "The Fishing Story: Bayesian Reasoning in Daily Life",
    "text": "The Fishing Story: Bayesian Reasoning in Daily Life\nMy oldest son recently got into fishing and asked where we could try along the North Saskatchewan River. We didn’t just pick a random spot – we built a prior. We looked at fishing forums, asked in tackle shops, and thought about where fish like to hide (around structure like fallen trees). We also preferred places near parking so the walk wasn’t too long. That’s all prior information we used to narrow down our choices.\nThen we went out and tested those beliefs. After a few hours, my son had caught three small perch near a spot with a big willow tree and nothing at a location under the bridge. We updated our beliefs: the willow tree location moved up in our mental ranking, while the bridge spot moved down. Next weekend, we’ll use this updated knowledge as our new prior – starting at the willow tree instead of exploring randomly.\nThis process – combining what you know ahead of time with what you observe – is exactly how Bayesian inference works.\nA frequentist approach would be different. You’d go to a spot with no prior assumptions and fish for a fixed period – say 2 hours. If you caught 3 fish, you might construct a confidence interval: “If I fished here many times for 2 hours each, 95% of my confidence intervals would contain the true catch rate.” Notice this doesn’t tell you “there’s a 95% chance the true rate is between X and Y” – it’s a statement about the procedure, not the parameter.\nBayesian reasoning is always conditional on the data actually observed – we update beliefs based on what really happened. Frequentist inference requires imagining repeated experiments under identical conditions, which is straightforward in controlled experiments but often artificial in observational studies or unique situations."
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#types-of-priors",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#types-of-priors",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "Types of Priors",
    "text": "Types of Priors\nThis fishing example used what we’d call a weakly informative prior – we had some knowledge but weren’t certain. Let’s formalize the different types of priors you might use:\nNon-informative (flat) priors:\n\nDeliberately vague, letting data dominate\nExample: Uniform(0, 1) for a coin’s probability of heads when you have no prior information\nCan seem “objective” but often encode strong assumptions on transformed scales\nMay lead to improper posteriors or numerical instability\n\nWeakly informative priors:\n\nStill wide, but rule out absurd values\nHelp stabilize estimation, especially with small samples\nExample: Normal(0, 10) for a standardized regression slope – expecting effects around zero but allowing substantial deviations. In original units, this might translate to “a 1-year increase in education increases log-income by somewhere between -20 and +20, with values near 0 most plausible.”\nOften the best default choice (Gelman, 2006; Gelman et al., 2017)\n\nInformative priors:\n\nBased on previous research or expert knowledge\nCan substantially improve inference when justified\nExample: Beta(30, 70) for disease prevalence when meta-analysis of previous studies found approximately 30% prevalence rates\nRequire careful justification and sensitivity analysis\n\n\n\n\n\n\n\n🐰 Wait, What’s a Normal(0, 10)? A Quick Field Guide Through The Burrows 🗺\n\n\n\n\n\n\nCommon Distribution Families\nWhen we specify priors, we’re choosing probability distributions that describe our beliefs about parameter values before seeing data. Here are the key concepts:\nUniform(a, b) or \\(\\theta \\sim \\text{Uniform}(a, b)\\): Every value between \\(a\\) and \\(b\\) is equally likely. Example: \\(\\text{Uniform}(0, 1)\\) means we believe a probability parameter could be anywhere from 0 to 1 with equal plausibility. This is “flat” – no value is preferred over another.\nNormal(μ, σ) or \\(\\theta \\sim \\mathcal{N}(\\mu, \\sigma^2)\\): The familiar bell curve, centered at mean \\(\\mu\\) with standard deviation \\(\\sigma\\). Note that in mathematical notation, the second parameter is often the variance \\(\\sigma^2\\), not the standard deviation. Example: \\(\\theta \\sim \\mathcal{N}(0, 100)\\) for a regression slope says we expect the effect to be around 0 with standard deviation \\(\\sigma = 10\\), but values within roughly \\(-20\\) to \\(+20\\) are plausible.\nBeta(α, β) or \\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\): Restricted to values between 0 and 1, making it perfect for probabilities or proportions. The shape depends on \\(\\alpha\\) and \\(\\beta\\): \\(\\text{Beta}(1, 1)\\) is uniform, \\(\\text{Beta}(2, 2)\\) is slightly peaked in the middle, \\(\\text{Beta}(10, 10)\\) is strongly concentrated around 0.5.\nGamma(α, β) or \\(\\theta \\sim \\text{Gamma}(\\alpha, \\beta)\\): Restricted to positive values, often used for scale parameters or rates. Warning: different software uses different parameterizations (shape/rate vs. shape/scale), so always check documentation!\nBinomial(n, p) or \\(Y \\sim \\text{Binomial}(n, p)\\): Counts the number of successes in \\(n\\) independent trials, each with probability \\(p\\) of success. This is a discrete distribution with possible values 0, 1, 2, …, \\(n\\). Example: \\(\\text{Binomial}(10, 0.3)\\) represents flipping a weighted coin 10 times where each flip has a 30% chance of heads – you might observe anywhere from 0 to 10 heads, with 3 being most likely.\nPoisson(λ) or \\(\\theta \\sim \\text{Poisson}(\\lambda)\\): For count data (non-negative integers: 0, 1, 2, 3, …) with no upper limit. The parameter \\(\\lambda\\) (lambda) is both the mean and variance. Example: \\(\\text{Poisson}(5)\\) is centered around 5 counts, with most probability on values between 1 and 10. Commonly used for rare events or occurrences over time/space.\nWant to see what these distributions look like (and much more)? Check out this interactive distribution explorer.\n\n\nProbability Density vs. Probability Mass\nFor continuous parameters (like temperature, height, or regression slopes), we use probability density functions (PDFs). The density at a point doesn’t give you a probability directly – instead, probability comes from integrating the density over an interval. For example, with \\(\\text{Normal}(0, 1)\\), we can’t say “the probability \\(\\theta = 0\\) is \\(X\\)”, but we can say “the probability \\(\\theta\\) is between \\(-0.1\\) and \\(0.1\\) is \\(Y\\)”.\nFor discrete parameters (like counts or categories), we use probability mass functions (PMFs). Here, each specific value does have a probability. For example, with a \\(\\text{Poisson}(5)\\) distribution, we can say “the probability of observing exactly 3 events is 0.14”.\nIn Bayesian inference, most parameters are continuous, so we work with probability densities. When we say “the prior is \\(\\text{Normal}(0, 10)\\)”, we mean the prior density follows that normal distribution.\n\n\nWhat Makes a Prior “Informative”?\nThe key is how much the prior constrains possible parameter values:\n\nFlat/non-informative: \\(\\text{Normal}(0, 1000)\\) for a regression slope barely constrains anything – it says slopes from \\(-2000\\) to \\(+2000\\) are all plausible.\nWeakly informative: \\(\\text{Normal}(0, 10)\\) gently suggests the effect is moderate, ruling out absurdly large values while remaining open-minded.\nInformative: \\(\\text{Normal}(5, 1)\\) strongly expects the parameter to be near 5, with most probability mass between 3 and 7.\n\nThe “right” prior depends on your actual knowledge and the scale of your problem. A slope of 1000 might be absurd for predicting human height from weight, but perfectly reasonable for predicting income from education years."
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#implicit-assumptions-in-frequentist-methods",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#implicit-assumptions-in-frequentist-methods",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "Implicit Assumptions in Frequentist Methods",
    "text": "Implicit Assumptions in Frequentist Methods\nAn important realization: Frequentist methods also make implicit assumptions, they’re just less visible (Robert, 2007):\n\nChoosing which test to use (t-test vs. Mann-Whitney)\nSelecting significance levels (\\(\\alpha = 0.05\\) vs. \\(0.01\\))\nDeciding when to stop collecting data\nWhich variables to include in a model\nHow to handle outliers or missing data\n\nFor example, when you choose to use a t-test instead of a nonparametric test, you’re implicitly assuming normality – that’s an assumption about your data, just never stated as a probability distribution. When you select \\(\\alpha = 0.05\\) rather than \\(0.01\\), you’re making a judgment about the relative costs of Type I and Type II errors.\nBy making priors explicit, Bayesian analysis gives you control and transparency. You can examine whether your assumptions are reasonable, test sensitivity to different priors, and clearly communicate what you’re assuming. As Berger (2006) argues, explicit modeling of prior information often leads to more honest and reproducible science than pretending we can analyze data without any prior assumptions.\n\n\n\n\n\n\n🐰 But before we dive deeper, let’s address some frequent misunderstandings 🧭\n\n\n\n\n\nCommon Misconceptions About Bayesian Inference\n“Priors are subjective, so Bayesian inference is unscientific.”\nAll statistical methods involve choices (which test to use, what \\(\\alpha\\)-level, when to stop collecting data). Bayesian methods make these assumptions explicit and transparent. Moreover, with sufficient data, different reasonable priors converge to similar posteriors (Berger, 2006).\n“You need strong prior beliefs to use Bayesian methods.”\nNot at all. Weakly informative priors that gently constrain parameters to reasonable ranges often work best (Gelman et al., 2017). You can be quite uncertain in your prior while still gaining the benefits of the Bayesian framework.\n“Bayesian credible intervals are the same as confidence intervals.”\nThey often give numerically similar results but have fundamentally different interpretations. A credible interval directly states the probability the parameter lies within it; a confidence interval describes properties of the procedure across repeated samples (Morey et al., 2016).\n“The prior ‘overwhelms’ the data.”\nFor reasonable priors and moderate sample sizes, the data dominate. The prior matters most when data are sparse – which is exactly when incorporating external knowledge is most valuable."
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#the-scenario-testing-a-new-seed-batch",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#the-scenario-testing-a-new-seed-batch",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "The Scenario: Testing a New Seed Batch",
    "text": "The Scenario: Testing a New Seed Batch\nImagine you’re a seed supplier evaluating a new batch of Echinacea purpurea (Purple Coneflower) seeds from a different grower. You plant 20 seeds under controlled conditions and observe that 12 germinate successfully. What can you conclude about the true germination rate of this batch?\nUnlike flipping a coin, you’re not starting from complete ignorance. You have relevant prior knowledge:\n\nPublished studies report 70-85% germination for fresh Echinacea seeds under optimal conditions\nYour company’s historical data from other suppliers shows similar rates\nHowever, germination can vary by seed source, storage conditions, and growing season\nSeeds from new suppliers sometimes underperform until growing practices are optimized\n\nThis is a perfect scenario for Bayesian inference – you have genuine prior information to incorporate, but also meaningful uncertainty to resolve with data."
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#the-beta-binomial-model",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#the-beta-binomial-model",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "The Beta-Binomial Model",
    "text": "The Beta-Binomial Model\nThe natural Bayesian model for germination data is:\n\nPrior: \\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\) (germination probability)\nLikelihood: Number germinating \\(\\sim \\text{Binomial}(n, \\theta)\\)\nPosterior: \\(\\theta \\sim \\text{Beta}(\\alpha + \\text{germinated}, \\beta + \\text{failed})\\)\n\nThe Beta distribution is perfect here because:\n\nIt’s defined on \\([0,1]\\), matching the range of probabilities\nIt’s the conjugate prior for the Binomial – the posterior is also Beta, making calculations simple\nIt’s flexible, able to represent various beliefs from uniform (no information) to highly concentrated\n\nUnderstanding Beta parameters intuitively: You can think of the Beta prior \\(\\text{Beta}(\\alpha, \\beta)\\) as if you’d already observed pseudo-data from previous experiments. Specifically:\n\n\\(\\alpha - 1\\) = number of prior “germinations” you’ve seen\n\\(\\beta - 1\\) = number of prior “failures” you’ve seen\n\nFor example, \\(\\text{Beta}(15, 5)\\) is like having previously tested \\(14 + 4 = 18\\) seeds and observed 14 germinate (with a 78% success rate). The prior is just data you saw before. \\(\\text{Beta}(1, 1)\\) is like having tested 0 seeds – complete ignorance.\nThe beautiful part: When you observe new data, you simply add your actual observations to these pseudo-observations:\n\\[\n\\text{Beta}(\\alpha, \\beta) + \\text{Data}(k, n-k) = \\text{Beta}(\\alpha + k, \\beta + n - k)\n\\]\nwhere \\(n\\) = total trials, \\(k\\) = germinations, and \\(n-k\\) = failures\n\n\n\n\n\n\n🐰 Another Burrow to Explore: Conjugate Priors Explained 🔦\n\n\n\n\n\n\nWhat’s a Conjugate Prior?\nA conjugate prior is a prior distribution that, when combined with a particular likelihood, produces a posterior in the same family. For the Binomial likelihood, the Beta distribution is conjugate – meaning:\n\nBeta prior + Binomial data = Beta posterior (still a Beta!)\n\nThis is special because most combinations don’t work this way. Usually, prior × likelihood gives you some complicated function that’s hard to work with. But with conjugate pairs, the math stays clean. That said, modern computational tools like MCMC (Markov Chain Monte Carlo) let us combine any prior with any likelihood – we’re no longer restricted to conjugate pairs for mathematical convenience. Conjugate priors are still useful (fast and intuitive), but computational methods have made Bayesian inference practical even when the math isn’t neat.\n\n\nThe Simple Updating Rule\nHere’s the beautiful part of using the Beta-Binomial model.. If your prior is \\(\\text{Beta}(\\alpha, \\beta)\\) and you observe \\(k\\) successes in \\(n\\) trials, your posterior is:\n\\[\n\\text{Posterior} = \\text{Beta}(\\alpha_{\\text{new}}, \\beta_{\\text{new}})\n\\]\nwhere:\n\\[\n\\begin{align}\n\\alpha_{\\text{new}} &= \\alpha + k \\quad \\text{(old } \\alpha \\text{ + successes observed)} \\\\\n\\beta_{\\text{new}} &= \\beta + (n-k) \\quad \\text{(old } \\beta \\text{ + failures observed)}\n\\end{align}\n\\]\nIn plain English: Take your starting \\(\\alpha\\) and add the number of seeds that germinated. Take your starting \\(\\beta\\) and add the number that failed. Done!\n\n\nWhy Does This Work? The Prior as Pseudo-Data\nHere’s the key insight: You can think of the Beta prior as if you’d already conducted some germination trials before your actual experiment.\nThe parameters \\(\\alpha\\) and \\(\\beta\\) translate to pseudo-observations like this:\n\n\\(\\alpha - 1\\) = number of “prior germinations”\n\\(\\beta - 1\\) = number of “prior failures”\n\nWhy the “minus 1”? It’s a mathematical quirk of how the Beta distribution is defined. Just subtract 1 from each parameter to convert to counts.\nExamples:\n\n\\(\\text{Beta}(1, 1)\\): That’s \\(1-1=0\\) prior germinations and \\(1-1=0\\) prior failures. You’re starting with a blank slate – no prior information.\n\\(\\text{Beta}(15, 5)\\): That’s \\(15-1=14\\) prior germinations and \\(5-1=4\\) prior failures. It’s as if you’d already tested 18 seeds and seen 14 germinate. This encodes a belief that germination rate is probably around 75-80%.\n\\(\\text{Beta}(40, 10)\\): That’s \\(40-1=39\\) prior germinations and \\(10-1=9\\) prior failures. This represents stronger prior knowledge (48 pseudo-observations) with similar 80% germination expectation, but with more certainty.\n\n\n\nBayesian Updating is Just Adding New Data to Old Data\nWhen you observe real data, you’re literally adding your new observations to your pseudo-observations.\nFull example:\nYou start with prior \\(\\text{Beta}(15, 5)\\) based on historical data. Converting to counts:\n\nPrior pseudo-germinations: \\(15 - 1 = 14\\)\nPrior pseudo-failures: \\(5 - 1 = 4\\)\n\nThen you actually test seeds and observe 12 germinations and 8 failures. Add them together:\n\nTotal germinations: \\(14 + 12 = 26\\)\nTotal failures: \\(4 + 8 = 12\\)\n\nNow convert back to Beta parameters (add 1 to each count):\n\nNew \\(\\alpha = 26 + 1 = 27\\)\nNew \\(\\beta = 12 + 1 = 13\\)\nPosterior: \\(\\text{Beta}(27, 13)\\)\n\nOr use the shortcut: Just add observed counts directly to the old parameters:\n\n\\(\\alpha_{\\text{new}} = 15 + 12 = 27\\)\n\\(\\beta_{\\text{new}} = 5 + 8 = 13\\)\n\nSame answer, less thinking about the “-1” and “+1”!\n\n\n\n\nLet’s visualize different prior beliefs:\n\n\nShow code\nlibrary(tidyverse)\n\n# Create sequence of probability values\ntheta &lt;- seq(0, 1, length.out = 200)\n\n# Define three different priors representing different states of knowledge\nprior_df &lt;- bind_rows(\n  # Uniform prior: no prior knowledge\n  tibble(\n    theta = theta,\n    density = dbeta(theta, 1, 1),\n    prior_type = \"Beta(1,1): Uniform\\n(No prior knowledge)\"\n  ),\n  # Weakly informative: general knowledge about Echinacea\n  tibble(\n    theta = theta,\n    density = dbeta(theta, 15, 5),\n    prior_type = \"Beta(15,5): Centered at 0.75\\n(Literature suggests 70-85%)\"\n  ),\n  # Informative prior: strong historical data from your company\n  tibble(\n    theta = theta,\n    density = dbeta(theta, 40, 10),\n    prior_type = \"Beta(40,10): Strong belief at 0.80\\n(Extensive company records)\"\n  )\n)\n\n# Plot the three priors\nggplot(prior_df, aes(x = theta, y = density, color = prior_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"#1b9e77\", \"#d95f02\", \"#7570b3\")) +\n  labs(\n    title = \"Different Prior Beliefs About Echinacea Germination Rate\",\n    subtitle = \"Beta distributions representing various states of prior knowledge\",\n    caption = \"Three different prior beliefs about germination rates for Purple Coneflower seeds.\",\n    x = \"Germination rate (θ)\",\n    y = \"Density\",\n    color = \"Prior Distribution\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"top\",\n        legend.text = element_text(size = 8),\n        legend.title = element_text(size = 9)\n        )"
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#bayesian-updating-in-action",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#bayesian-updating-in-action",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "Bayesian Updating in Action",
    "text": "Bayesian Updating in Action\nNow let’s see how these priors update when we observe 12 germinations out of 20 seeds:\n\n\nShow code\nlibrary(tidyverse)\n\n# Create sequence of probability values\ntheta &lt;- seq(0, 1, length.out = 200)\n\n# Our observed data\nn_germinated &lt;- 12\nn_failed &lt;- 8\n\n# Calculate prior and posterior densities for each prior\n# Prior 1: Uniform (Beta(1,1))\nprior1_prior &lt;- dbeta(theta, 1, 1)\nprior1_post &lt;- dbeta(theta, 1 + n_germinated, 1 + n_failed)\n\n# Prior 2: Literature-based (Beta(15,5))\nprior2_prior &lt;- dbeta(theta, 15, 5)\nprior2_post &lt;- dbeta(theta, 15 + n_germinated, 5 + n_failed)\n\n# Prior 3: Strong company data (Beta(40,10))\nprior3_prior &lt;- dbeta(theta, 40, 10)\nprior3_post &lt;- dbeta(theta, 40 + n_germinated, 10 + n_failed)\n\n# Combine all priors and posteriors into one dataframe\nupdating_df &lt;- bind_rows(\n  tibble(theta = theta, density = prior1_prior, distribution = \"Prior\", prior_type = \"No Prior Knowledge\"),\n  tibble(theta = theta, density = prior1_post, distribution = \"Posterior\", prior_type = \"No Prior Knowledge\"),\n  tibble(theta = theta, density = prior2_prior, distribution = \"Prior\", prior_type = \"Literature-Based Prior\"),\n  tibble(theta = theta, density = prior2_post, distribution = \"Posterior\", prior_type = \"Literature-Based Prior\"),\n  tibble(theta = theta, density = prior3_prior, distribution = \"Prior\", prior_type = \"Strong Company Data\"),\n  tibble(theta = theta, density = prior3_post, distribution = \"Posterior\", prior_type = \"Strong Company Data\")\n)\n\n# Plot priors and posteriors for comparison\nggplot(updating_df, aes(x = theta, y = density, \n                        color = distribution, linetype = distribution)) +\n  geom_line(linewidth = 1) +\n  facet_wrap(~ prior_type, ncol = 1) +\n  # Add vertical line at observed proportion (12/20 = 0.60)\n  geom_vline(xintercept = 12/20, linetype = \"dashed\", color = \"grey40\") +\n  scale_color_manual(values = c(\"Prior\" = \"#7570b3\", \"Posterior\" = \"#d95f02\")) +\n  labs(\n    title = \"Bayesian Updating: How Germination Data Changes Our Beliefs\",\n    subtitle = \"Data: 12 seeds germinated out of 20 | Dashed line shows observed rate (0.60)\",\n    caption = \"How different priors update with the same data.\",\n    color = \"Distribution\", \n    linetype = \"Distribution\",\n    x = \"Germination rate (θ)\",\n    y = \"Density\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nKey insights:\n\nAll posteriors shift toward the observed data (\\(12/20 = 0.60\\)), regardless of starting beliefs\nDifferent priors lead to different posteriors – your starting beliefs matter, especially with limited data\nStronger priors (more peaked) require more data to substantially shift, while weak priors let the data dominate quickly\nWith enough data, all reasonable priors eventually converge to similar posteriors\nThis illustrates a fundamental principle: Bayesian inference represents a compromise between prior beliefs and observed data\nThe weight given to each depends on their relative certainty – weak priors defer to data, strong data overwhelms priors"
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#credible-intervals-direct-probability-statements",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#credible-intervals-direct-probability-statements",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "Credible Intervals: Direct Probability Statements",
    "text": "Credible Intervals: Direct Probability Statements\nNow let’s quantify our uncertainty about the germination rate using the posterior distribution.\nUnlike frequentist confidence intervals, Bayesian credible intervals have a direct probability interpretation (Kruschke, 2014; Morey et al., 2016).\nA 95% credible interval is simply the range containing 95% of the posterior probability. We’ll calculate an equal-tailed interval, which places 2.5% of probability in each tail (there’s also a “highest posterior density” interval that finds the narrowest 95% region, but for symmetric posteriors like ours, they’re nearly identical).\nIn Bayesian inference, the posterior distribution is the final product – it fully describes our updated beliefs about the parameter after seeing the data. Any interval you report (e.g., 80%, 89%, 90%, or 95%) is just a way of summarizing that posterior. The chosen level is not dictated by the method; it’s a communication choice, not a fixed error-control convention.\nIn contrast, frequentist confidence intervals are tied to a pre-specified error rate (the \\(\\alpha\\)-level, such as 0.05 for 95% coverage). The level determines the long-run frequency properties of the procedure: if repeated infinitely, 95% of intervals constructed this way would contain the true value of \\(\\theta\\). Changing the level changes the frequentist procedure itself.\nTo see this difference concretely:\n\nBayesian 89%: “I’m reporting 89% of my posterior. I could have reported 80% or 95 – all are valid summaries of the same posterior distribution.”\nFrequentist 95%: “I chose \\(\\alpha = 0.05\\) to control long-run error rates. This determines the procedure’s coverage properties. But the coverage is about the procedure in general, not about this specific interval capturing this specific parameter.”\n\nAs McElreath (2020, p. 58) notes, “It is not easy to defend the choice of 95% (5%), outside of pleas of convention.” Gelman & Carlin (2014) and Kruschke (2014) make the same point: since the posterior fully represents uncertainty, the interval percentage is a matter of reporting style, not of statistical principle. So why not go with 89% throughout this series? It’s such a beautiful number :)\n\n\nShow code\nlibrary(tidyverse)\n\n# Create sequence of probability values\ntheta &lt;- seq(0, 1, length.out = 200)\n\n# Our observed data\nn_germinated &lt;- 12\nn_failed &lt;- 8\n\n# Calculate posterior (using uniform prior)\nalpha_post &lt;- 1 + n_germinated  # 13\nbeta_post &lt;- 1 + n_failed       # 9\n\n# Calculate 89% credible interval (equal-tailed)\nlower &lt;- qbeta(0.055, alpha_post, beta_post)\nupper &lt;- qbeta(0.945, alpha_post, beta_post)\npost_mean &lt;- alpha_post / (alpha_post + beta_post)\n\n# Create data for plotting\nposterior_data &lt;- tibble(theta = theta, density = dbeta(theta, alpha_post, beta_post))\n# Extract only the data within the credible interval for shading\nci_data &lt;- posterior_data %&gt;% filter(theta &gt;= lower & theta &lt;= upper)\n\n# Plot posterior with shaded credible interval\nggplot(posterior_data, aes(x = theta, y = density)) +\n  geom_line(linewidth = 1.2, color = \"#d95f02\") +\n  # Shade the 89% credible interval\n  geom_area(data = ci_data, aes(x = theta, y = density), \n            fill = \"#d95f02\", alpha = 0.3) +\n  # Add posterior mean line\n  geom_vline(xintercept = post_mean, linetype = \"dashed\", color = \"grey40\") +\n  # Annotate with posterior mean\n  annotate(\"text\", x = post_mean, y = max(posterior_data$density) * 1.05,\n           label = sprintf(\"Posterior mean = %.3f\", post_mean),\n           hjust = 0.5, size = 4) +\n  # Annotate with credible interval bounds\n  annotate(\"text\", x = post_mean, y = max(posterior_data$density) * 0.3,\n           label = sprintf(\"89%% Credible Interval:\\n[%.3f, %.3f]\", lower, upper),\n           hjust = 0.5, size = 4, color = \"#d95f02\", fontface = \"bold\") +\n  labs(\n    title = \"89% Bayesian Credible Interval for Germination Rate\",\n    subtitle = \"The shaded region contains 89% of the posterior probability\",\n    x = \"Germination rate (θ)\",\n    y = \"Posterior density\",\n    caption = \"This is the interpretation most people intuitively expect from any interval estimate.\\nAnd there is no reason why it should be 95% other than convention.\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\nKey insights:\n\nThe crucial difference between confidence and credible intervals lies in their interpretation\nA confidence interval (frequentist) means “If we repeated this procedure infinitely, 95% of intervals would capture \\(\\theta\\)” – a statement about the procedure, not the parameter\nA credible interval (Bayesian) means “There is a 95% probability that \\(\\theta\\) lies in this interval” – a direct statement about the parameter itself\nFor this seed batch, we can say with 89% confidence: “The true germination rate is between 42% and 75%”\nThis is the statement seed suppliers and customers actually care about – it directly quantifies our uncertainty about this specific batch’s quality\nThe Bayesian statement is what most people think a confidence interval means, and in Bayesian inference, it actually does mean that\n\n\n\n\n\n\n\n🎉 Progress Check!\n\n\n\nYou’ve learned:\n\n✅ How priors encode beliefs\n✅ How Bayes’ theorem updates those beliefs\n\n✅ How to construct credible intervals\n✅ What makes 89% such a beautiful number\n\nStill to come:\n\nMaking predictions with uncertainty\nSequential learning without penalties\n\nYou’re over halfway there! The hard conceptual work is done – now we get to see the payoff.\n\nQuick breather - Bad Statistics Joke: Why did the Bayesian seed always germinate?\nBecause it had prior experience! 🌱\n(I’ll see myself out… but first, let’s talk about posterior predictions!) 😅"
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#posterior-predictions-what-happens-next",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#posterior-predictions-what-happens-next",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "Posterior Predictions: What Happens Next?",
    "text": "Posterior Predictions: What Happens Next?\nOne of Bayesian inference’s most powerful features is that we can use our posterior distribution to predict future observations, accounting for all our uncertainty (Gabry et al., 2019; Gelman et al., 1996). This is huge – instead of just estimating “the germination rate is probably around 59%,” we can directly answer questions like “If I plant 10 seeds from this batch in a customer’s garden, how many will germinate?”\nFor a seed supplier, this is far more useful than a point estimate. You need to:\n\nSet realistic customer expectations\nDecide whether to accept or reject the batch\nDetermine appropriate pricing\nEstimate how many seeds to include per packet\n\n\nThe Question\nWe tested 20 seeds and observed 12 germinations, giving us a posterior \\(\\text{Beta}(13, 9)\\) for the germination rate \\(\\theta\\). Now suppose a customer plants 10 seeds from this batch in their garden. How many should we expect to germinate?\n\n\nTwo Sources of Uncertainty\nA good prediction must account for:\n\nParameter uncertainty: We’re not completely sure what \\(\\theta\\) is (we have a posterior distribution, not a single value)\nSampling variability: Even if we knew \\(\\theta\\) exactly, germination is inherently variable – weather, soil conditions, planting depth, and random chance all matter. We wouldn’t get exactly \\(10 \\times \\theta\\) germinations even with perfect knowledge of \\(\\theta\\).\n\nThe posterior predictive distribution accounts for both by:\n\nDrawing a plausible \\(\\theta\\) value from our posterior\nSimulating seed germinations using that \\(\\theta\\)\nRepeating this many times to build up a distribution of predictions\n\nThink of it this way: Imagine asking “How many Echinacea seedlings will emerge in my garden?” rather than “What is the true germination rate of this batch?” The first question (prediction) naturally incorporates both your uncertainty about the batch quality and the randomness inherent in any particular planting.\nLet’s see this in action:\n\n\nShow code\nlibrary(tidyverse)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Calculate posterior (using uniform prior)\nalpha_post &lt;- 1 + n_germinated  # 13\nbeta_post &lt;- 1 + n_failed       # 9\n\n# Number of seeds customer will plant\nn_future &lt;- 10\nn_sims &lt;- 10000\n\n# Simulate from posterior predictive distribution\n# Step 1: Draw theta values from posterior Beta(13, 9)\ntheta_samples &lt;- rbeta(n_sims, alpha_post, beta_post)\n\n# Step 2: For each theta, simulate seed germinations\nfuture_germinations &lt;- rbinom(n_sims, size = n_future, prob = theta_samples)\n\n# Create dataframe for plotting\npredictive_df &lt;- tibble(future_germinations = future_germinations)\n\n# Calculate posterior mean prediction (for comparison)\nposterior_mean_theta &lt;- alpha_post / (alpha_post + beta_post)\npoint_prediction &lt;- n_future * posterior_mean_theta\n\n# Plot the posterior predictive distribution\nggplot(predictive_df, aes(x = future_germinations)) +\n  geom_bar(aes(y = after_stat(count) / sum(after_stat(count))),\n           fill = \"#d95f02\", alpha = 0.7) +\n  geom_vline(xintercept = point_prediction, \n             linetype = \"dashed\", color = \"#1b9e77\", linewidth = 1) +\n  annotate(\"text\", x = point_prediction + 1.6, y = 0.21,\n           label = sprintf(\"Point estimate:\\n%.1f germinations\", point_prediction),\n           color = \"#1b9e77\", size = 3.5) +\n  scale_x_continuous(breaks = 0:10) +\n  labs(\n    title = \"Predicting Germinations for a Customer's Planting\",\n    subtitle = sprintf(\"Based on posterior from 12 germinations in 20 seeds | Mean prediction: %.1f germinations\", \n                      mean(future_germinations)),\n    x = \"Number of germinations out of 10 seeds planted\",\n    y = \"Probability\",\n    caption = \"The distribution captures both parameter uncertainty and natural variation in germination.\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(plot.subtitle = element_text(size = 10))\n\n\n\n\n\n\n\n\n\n\n\nShow code\nlibrary(tidyverse)\nlibrary(knitr)\n\n# Summary statistics for the posterior predictive distribution\nsummary_stats &lt;- tibble(\n  Metric = c(\"Mean prediction\", \n             \"Most likely outcome\", \n             \"89% Prediction Interval\"),\n  Value = c(\n    sprintf(\"%.1f germinations\", mean(future_germinations)),\n    sprintf(\"%d germinations (%.1f%% probability)\",\n            as.numeric(names(sort(table(future_germinations), decreasing = TRUE)[1])),\n            100 * max(table(future_germinations)) / n_sims),\n    sprintf(\"[%d, %d] germinations\",\n            quantile(future_germinations, 0.055),\n            quantile(future_germinations, 0.945))\n  )\n)\n\nkable(summary_stats, \n      caption = \"Posterior predictive summary for 10 seeds from this batch.\",\n      align = c(\"l\", \"r\"))\n\n\n\nPosterior predictive summary for 10 seeds from this batch.\n\n\nMetric\nValue\n\n\n\n\nMean prediction\n5.9 germinations\n\n\nMost likely outcome\n6 germinations (20.5% probability)\n\n\n89% Prediction Interval\n[3, 9] germinations\n\n\n\n\n\n Key insights:\n\nThe distribution is wider than you might expect – this honest uncertainty reflects our uncertainty about \\(\\theta\\) plus natural variation in germination\nIt’s naturally discrete (whole numbers only), matching the reality that you can’t have fractional germinations\nWhile the mean is near our point estimate (\\(10 \\times 0.59 \\approx 5.9\\) germinations), the full distribution shows all plausible outcomes\nWe can say: “There’s an 89% chance a customer planting 10 seeds will see between 3 and 8 germinate”\n\nFor practical decision-making, we can say: “There’s an 89% chance a customer planting 10 seeds will see between 3 and 8 germinate.” This helps you:\n\nSet customer expectations: Don’t promise 80% germination when the data suggest 60%\nMake batch acceptance decisions: Is 3-8 germinations per 10 seeds acceptable?\nAdjust seed packet counts: Maybe include 15 seeds instead of 10 to ensure customers get enough plants\nDecide on further testing: The wide uncertainty (3-8 is a big range) suggests testing more seeds would be valuable\n\nThis also enables model checking: if you test another batch and get results far outside your predictive distribution, something’s wrong with your model assumptions (Gelman et al., 1996). Perhaps germination varies by seed lot more than you thought, or environmental factors matter more than your simple model assumes.\nThis same logic – priors, likelihoods, posteriors, and predictions – extends to any statistical model, from simple proportions to complex regression and beyond.\n\n\n\n\n\n\n🐰 Emerging from the Burrow: Posterior Predictions 🌅\n\n\n\n\n\n\nPosterior and Prior Predictive Distributions\nMathematically, the posterior predictive distribution is:\n\\[\nP(\\tilde{y} \\mid y_{\\text{obs}}) = \\int P(\\tilde{y} \\mid \\theta)\\, P(\\theta \\mid y_{\\text{obs}})\\, d\\theta\n\\]\nIn words: the probability of future (or new) data \\(\\tilde{y}\\) is the average of the likelihoods, weighted by how plausible each parameter value \\(\\theta\\) is under the posterior.\nFor each possible \\(\\theta\\):\n\nCompute how likely the future data are under that parameter: \\(P(\\tilde{y} \\mid \\theta)\\)\n\nWeight that likelihood by the posterior probability: \\(P(\\theta \\mid y_{\\text{obs}})\\)\n\nIntegrate over all possible \\(\\theta\\)\n\nThis expresses the predictive uncertainty that combines both parameter uncertainty and data variability.\n\n\nWhy We Simulate Instead of Integrate\nFor simple models (like the Beta-Binomial), the integral above has a closed form. But in most real problems, it doesn’t – so we simulate instead.\nPosterior Predictive Simulation Algorithm:\n\nDraw parameter samples \\(\\theta^{(i)} \\sim P(\\theta \\mid y_{\\text{obs}})\\)\n\nFor each draw, simulate future data \\(\\tilde{y}^{(i)} \\sim P(\\tilde{y} \\mid \\theta^{(i)})\\)\n\nRepeat many times\n\nThe collection \\(\\{\\tilde{y}^{(1)}, \\tilde{y}^{(2)}, \\ldots\\}\\) approximates \\(P(\\tilde{y} \\mid y_{\\text{obs}})\\)\n\nThis process is called ancestral sampling because we first sample “ancestors” (parameters) and then simulate “descendants” (data). It works for any Bayesian model, no matter how complex.\n\n\nPrior Predictive Checks: Simulating Before Observing Data\nBefore seeing data, we can simulate from the prior predictive distribution:\n\\[\nP(\\tilde{y}) = \\int P(\\tilde{y} \\mid \\theta)\\, P(\\theta)\\, d\\theta\n\\]\nThis answers:\n\n“If my prior beliefs were true, what kinds of data would I expect to see?”\n\nIf your prior predictive simulations yield implausible outcomes (e.g., negative germination rates or rates greater than 100%), that’s a clear sign your prior needs revision (Gabry et al., 2019). We’ll use this technique extensively in the next post when working with regression models."
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#demonstration-sequential-germination-testing",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#demonstration-sequential-germination-testing",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "Demonstration: Sequential Germination Testing",
    "text": "Demonstration: Sequential Germination Testing\nImagine you’re evaluating a large shipment of Echinacea seeds. Rather than testing all at once, you test them in batches of 10 seeds as time and resources permit. Let’s watch how your beliefs evolve:\n\n\nShow code\nlibrary(tidyverse)\nlibrary(knitr)\n\nset.seed(456)\n\n# Simulate testing 100 seeds total from a batch with 60% germination rate\nn_seeds &lt;- 100\ntrue_germ_rate &lt;- 0.60\ngerminations &lt;- rbinom(n_seeds, 1, true_germ_rate)\n\n# Start with a weakly informative prior based on literature: Beta(15,5)\n# This represents belief that Echinacea typically germinates around 75%\nalpha &lt;- 15  # prior \"germinations\"\nbeta &lt;- 5    # prior \"failures\"\n\n# Create a dataframe to track how our beliefs evolve\nposterior_evolution &lt;- tibble(\n  seeds_tested = 0:n_seeds,\n  alpha_param = numeric(n_seeds + 1),\n  beta_param = numeric(n_seeds + 1),\n  mean = numeric(n_seeds + 1),\n  lower = numeric(n_seeds + 1),\n  upper = numeric(n_seeds + 1)\n)\n\n# Record initial prior\nposterior_evolution$alpha_param[1] &lt;- alpha\nposterior_evolution$beta_param[1] &lt;- beta\nposterior_evolution$mean[1] &lt;- alpha / (alpha + beta)\nposterior_evolution$lower[1] &lt;- qbeta(0.055, alpha, beta)\nposterior_evolution$upper[1] &lt;- qbeta(0.945, alpha, beta)\n\n# Update beliefs after each seed test\n# This is the sequential update formula: α_new = α_old + germinated, β_new = β_old + failed\nfor (i in 1:n_seeds) {\n  if (germinations[i] == 1) {\n    alpha &lt;- alpha + 1  # observed a germination\n  } else {\n    beta &lt;- beta + 1    # observed a failure\n  }\n  \n  # Record the updated posterior\n  posterior_evolution$alpha_param[i+1] &lt;- alpha\n  posterior_evolution$beta_param[i+1] &lt;- beta\n  posterior_evolution$mean[i+1] &lt;- alpha / (alpha + beta)\n  posterior_evolution$lower[i+1] &lt;- qbeta(0.055, alpha, beta)\n  posterior_evolution$upper[i+1] &lt;- qbeta(0.945, alpha, beta)\n}\n\n# Visualize the evolution of our beliefs\nggplot(posterior_evolution, aes(x = seeds_tested)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper), fill = \"#d95f02\", alpha = 0.3) +\n  geom_line(aes(y = mean), color = \"#d95f02\", linewidth = 1.2) +\n  geom_hline(yintercept = true_germ_rate, linetype = \"dashed\", color = \"grey40\") +\n  annotate(\"text\", x = 85, y = true_germ_rate + 0.05, \n           label = \"True germination rate (0.60)\", size = 4, color = \"grey40\") +\n  labs(\n    title = \"Sequential Bayesian Learning: No Penalty for Looking\",\n    subtitle = \"Posterior mean and 89% credible interval converge to truth as data accumulate\",\n    x = \"Number of seeds tested\",\n    y = \"Estimated germination rate (θ)\",\n    caption = \"Orange line = posterior mean | Shaded region = 89% credible interval\"\n  ) +\n  theme_minimal(base_size = 13)"
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#what-this-shows",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#what-this-shows",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "What This Shows",
    "text": "What This Shows\n\nStarting point matters initially: We began with a prior centered around 75% germination (based on literature), but as data accumulate, we learn the true rate is closer to 60%\nUncertainty shrinks: The credible interval (shaded region) narrows as we test more seeds\nBeliefs converge: Our estimate approaches the true germination rate as evidence accumulates\nNo stopping penalty: We could have stopped at seed 20, examined results, then continued – our final answer would be identical\nCoherent updates: Each seed test updates our beliefs in a mathematically principled way"
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#why-this-matters-for-seed-testing",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#why-this-matters-for-seed-testing",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "Why This Matters for Seed Testing",
    "text": "Why This Matters for Seed Testing\nThis property makes Bayesian methods particularly valuable for:\n\nBatch quality control: Test small samples continuously as shipments arrive\nAdaptive testing: Stop early if germination is clearly unacceptable (save time and resources)\nSeasonal monitoring: Update beliefs about supplier quality over multiple growing seasons\nDecision-making under uncertainty: Make accept/reject decisions as soon as you have sufficient evidence\n\nYou can make decisions based on evidence accumulated so far without worrying about invalidating your statistical inference. This is exactly how quality control works in practice – you don’t wait for a predetermined sample size if the batch is obviously failing."
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#making-decisions-along-the-way",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#making-decisions-along-the-way",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "Making Decisions Along the Way",
    "text": "Making Decisions Along the Way\nAs a seed supplier, you might have decision rules like:\n\nAfter 30 seeds: If mean germination &lt; 50%, reject the batch immediately\nAfter 50 seeds: If 89% credible interval entirely below 65%, reject the batch\nAfter 100 seeds: Make final accept/reject decision\n\nWith Bayesian inference, you can implement these rules without any statistical penalties. Your posterior after 100 seeds is valid regardless of how many times you peeked at the results."
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#the-frequentist-problem-optional-stopping",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#the-frequentist-problem-optional-stopping",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "The Frequentist Problem: Optional Stopping",
    "text": "The Frequentist Problem: Optional Stopping\nWhat would happen in frequentist inference?\nIn a frequentist framework, the validity of p-values depends on your sampling plan. If you peek at your data and decide whether to continue based on what you see, you inflate your Type I error rate (the probability of falsely rejecting a true null hypothesis).\n\nTwo Scenarios\n\nPre-planned sequential testing (e.g., “I will check after every 20 seeds.”):\n\nYou must adjust your significance level to control overall Type I error\nUse methods like Bonferroni correction: \\(\\alpha= 0.05/5 = 0.01\\) for 5 planned looks\nOr apply formal \\(\\alpha\\)-spending functions (used in clinical trials)\nThis “spends” your \\(\\alpha\\) budget across multiple looks\n\nOptional stopping (e.g., “I’ll keep testing until I see something interesting”):\n\nEven worse: your actual Type I error rate becomes unknown and is inflated\nYou’re giving yourself unlimited chances to reject the null by accident\nThe p-value assumes a fixed sample size and stopping rule – as defined in your power analysis – and changing these invalidates it\nThis is why “p-hacking” (testing until you find \\(p &lt; 0.05\\)) is problematic\n\n\nSuppose your company’s policy requires a minimum 70% germination rate for acceptable seed batches. In the frequentist framework, we test the null hypothesis \\(H_0 : \\theta = 0.70\\) against the alternative \\(H_1 : \\theta &lt; 0.70\\). However, because we’re checking the results multiple times (after every 20 seeds) over the course of testing 100 seeds, we face the multiple testing problem. Each time we look at the data and perform a test, we increase the chance of falsely rejecting an actually acceptable batch.\n\n\nThe Frequentist Dilemma\n\nWithout correction (\\(\\alpha = 0.05\\) at each look): You inflate your Type I error rate – you’re more likely to reject good batches by chance\nWith correction (Bonferroni \\(\\alpha = 0.01\\)): You lose power – it’s harder to detect truly poor batches\nMost importantly: The p-value’s validity depends on whether you planned these looks in advance and adjusted properly\nIf you decide to test more seeds after seeing results, your p-values become invalid\n\n\n\nThe Bayesian Advantage\nYour posterior after testing 100 seeds is identical whether you:\n\nTested all 100 seeds without looking at interim results\nChecked after every single seed\n\nStopped at 50 seeds, went on vacation, then tested 50 more\nDecided to continue based on disappointing early results\n\nThe math doesn’t care about your peeking behavior – only the data you actually observed. Your inference remains valid regardless of your stopping rule."
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "",
    "text": "When in doubt, just fit straight lines. ;P"
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#the-data",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#the-data",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "The Data",
    "text": "The Data\n\n\nShow code\nlibrary(rstanarm)\nlibrary(tidybayes)\nlibrary(bayesplot)\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Load and prepare data\nscale_factor &lt;- 10  # Scale DBH by 10 cm units\n\n# Convert to metric and scale for better interpretation\ntrees &lt;- datasets::trees %&gt;%\n  mutate(\n    dbh = Girth * 2.54,                           # inches -&gt; cm\n    height = Height * 0.3048,                     # feet -&gt; meters\n    volume = Volume * 0.0283168,                  # cubic feet -&gt; cubic meters\n    dbh_scaled = (dbh - mean(dbh)) / scale_factor # center and scale by 10 cm\n  )\n\n# Quick look at the relationship\nggplot(trees, aes(x = dbh, y = height)) +\n  geom_point(size = 3, alpha = 0.7) +\n  labs(\n    title = \"Tree Height vs. Diameter\", \n    subtitle = \"Black Cherry Trees (n = 31)\",\n    x = \"Diameter at Breast Height (cm)\", \n    y = \"Height (m)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe relationship looks roughly linear and positive – we expect taller trees to have larger diameters."
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#the-model",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#the-model",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "The Model",
    "text": "The Model\nWe’ll predict tree height from diameter:\n\\[\n\\begin{aligned}\ny_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot x_i \\\\\n\\alpha &\\sim \\text{Normal}(20, 3) \\\\\n\\beta &\\sim \\text{Normal}(1, 1) \\\\\n\\sigma &\\sim \\text{Exponential}(0.5)\n\\end{aligned}\n\\]\nIn words:\n\nWhat we’re predicting: \\(y_i\\) is the observed height (in meters) for tree \\(i\\)\nThe model structure: Each tree’s height comes from a Normal distribution with mean \\(\\mu_i\\) and standard deviation \\(\\sigma\\)\nThe relationship: Mean height \\(\\mu_i\\) depends linearly on diameter through \\(\\alpha + \\beta \\cdot x_i\\), where \\(x_i\\) is the scaled diameter (DBH) for tree \\(i\\)\nWhat the priors represent: Our uncertainty about the model parameters before seeing the data\n\n\\(\\text{Normal}(20, 3)\\) for intercept (\\(\\alpha\\)): expected height at mean DBH is around 20 meters\n\\(\\text{Normal}(1, 1)\\) for slope (\\(\\beta\\)): height increases by roughly 1 meter per 10 cm increase in DBH\n\\(\\text{Exponential}(0.5)\\) for residual standard deviation (\\(\\sigma\\)): constrains variation to be positive\n\nWhy these priors work: They encode basic domain knowledge (trees are positive heights, larger diameter → taller) while remaining weak enough that the data can easily override them if reality differs"
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#why-prior-choice-matters-revisiting-a-key-lesson",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#why-prior-choice-matters-revisiting-a-key-lesson",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "Why Prior Choice Matters: Revisiting a Key Lesson",
    "text": "Why Prior Choice Matters: Revisiting a Key Lesson\nIn our seed germination example, we compared different priors (uniform, weakly informative, and strongly informative) and saw how they updated with data. That example involved a single parameter bounded between 0 and 1, making it relatively straightforward to specify sensible priors.\nBut regression introduces new challenges: multiple parameters on different scales that interact. This is where the “flat priors are objective” misconception becomes especially dangerous.\nRecall from the frequentist post that all statistical methods involve choices – which test to use, what \\(\\alpha\\)-level, when to stop data collection. The Bayesian framework makes these choices explicit through prior specification. But what happens when we try to be “objective” by using very vague priors in regression?"
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#visualizing-our-priors-as-distributions",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#visualizing-our-priors-as-distributions",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "Visualizing Our Priors as Distributions",
    "text": "Visualizing Our Priors as Distributions\nLet’s also look at the prior distributions themselves:\n\n\nShow code\n# Generate samples from our priors\nset.seed(789)\nn_draws &lt;- 10000\n\nprior_samples &lt;- tibble(\n  intercept = rnorm(n_draws, mean = 20, sd = 3),\n  slope = rnorm(n_draws, mean = 1, sd = 1),\n  sigma = rexp(n_draws, rate = 0.5)\n)\n\n# Visualize the prior distributions\np1 &lt;- ggplot(prior_samples, aes(x = intercept)) +\n  geom_histogram(bins = 50, fill = \"#7570b3\", alpha = 0.7) +\n  labs(\n    title = \"Prior for Intercept\",\n    subtitle = \"Normal(20, 3)\",\n    x = \"Intercept α (m)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\np2 &lt;- ggplot(prior_samples, aes(x = slope)) +\n  geom_histogram(bins = 50, fill = \"#1b9e77\", alpha = 0.7) +\n  labs(\n    title = \"Prior for Slope\",\n    subtitle = \"Normal(1, 1)\",\n    x = \"Slope β (m per 10 cm DBH)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\np3 &lt;- ggplot(prior_samples, aes(x = sigma)) +\n  geom_histogram(bins = 50, fill = \"#d95f02\", alpha = 0.7) +\n  xlim(0, 10) +\n  labs(\n    title = \"Prior for Residual SD\",\n    subtitle = \"Exponential(0.5)\",\n    x = \"Sigma σ (m)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n(p1 | p2 | p3) +\n  plot_annotation(\n    title = \"Prior Distributions for Height Model Parameters\",\n    subtitle = \"What we believe before seeing the data\"\n  )\n\n\n\n\n\n\n\n\n\nWhat these priors say:\n\nIntercept: Centered at 20 m (typical height for trees with average DBH), allowing variation between roughly 14-26 m\nSlope: Centered at 1 m per 10 cm DBH increase, allowing values between roughly -1 to 3 m\nSigma: Most mass between 0.5-6 m, weakly favoring smaller residual variation\n\nThese are weakly informative priors – they gently constrain parameters to reasonable scales without being dogmatic."
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#fitting-the-model",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#fitting-the-model",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "Fitting the Model",
    "text": "Fitting the Model\n\n\nShow code\nlibrary(knitr)\nlibrary(tibble)\n\n# Fit Bayesian linear regression\nfit_height &lt;- stan_glm(\n  height ~ dbh_scaled,\n  data = trees,\n  family = gaussian(),\n  prior_intercept = normal(20, 3),\n  prior = normal(1, 1),\n  prior_aux = exponential(0.5),\n  chains = 4,\n  iter = 2000,\n  seed = 123,\n  refresh = 0\n)\n\n# Display posterior summary as formatted table\nsummary(fit_height, probs = c(0.025, 0.975), digits = 3) %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"Parameter\") %&gt;%\n  kable(caption = \"Posterior Summary Statistics\", digits = 3)\n\n\n\nPosterior Summary Statistics\n\n\nParameter\nmean\nmcse\nsd\n2.5%\n97.5%\nn_eff\nRhat\n\n\n\n\n(Intercept)\n23.136\n0.006\n0.313\n22.518\n23.754\n2965\n1.001\n\n\ndbh_scaled\n1.225\n0.006\n0.382\n0.442\n1.965\n3669\n1.000\n\n\nsigma\n1.737\n0.004\n0.235\n1.359\n2.252\n3094\n1.002\n\n\nmean_PPD\n23.133\n0.008\n0.435\n22.274\n23.985\n2849\n1.000\n\n\nlog-posterior\n-65.312\n0.034\n1.314\n-68.777\n-63.826\n1514\n1.001\n\n\n\n\n\nParameter Interpretation:\n\nmean: Average value across all posterior samples – the central estimate of the parameter\nmcse: Monte Carlo Standard Error – uncertainty in the mean due to finite sampling (smaller is better)\nsd: Standard deviation of the posterior – measures overall uncertainty about the parameter\n2.5%, 97.5%: Bounds of the 95% credible interval – we’re 95% confident the true value lies between these\nn_eff: Effective sample size – number of independent samples after accounting for autocorrelation\nRhat: Convergence diagnostic – compares within-chain and between-chain variance\n\nWhat to look for:\n\nRhat ≈ 1.00: Chains have converged (values &gt; 1.01 suggest problems)\nn_eff &gt; 1000: Enough independent samples for reliable inference\n\nmcse much smaller than sd: Monte Carlo error is negligible compared to posterior uncertainty\nCredible interval makes sense: Check that the 2.5%-97.5% range is scientifically plausible\n\n\n\nShow code\nmcmc_trace(fit_height, pars = c(\"(Intercept)\", \"dbh_scaled\", \"sigma\"))\n\n\n\n\n\n\n\n\n\nGood trace plots look like “fuzzy caterpillars” – random noise with no trends, all chains overlapping."
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#making-direct-probability-statements",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#making-direct-probability-statements",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "Making Direct Probability Statements",
    "text": "Making Direct Probability Statements\nNow we can ask specific questions about the relationship:\n\n\nShow code\n# Extract posterior samples\nposterior_height &lt;- as_draws_df(fit_height)\n\n# Direct probability statements\nprob_slope_positive &lt;- mean(posterior_height$dbh_scaled &gt; 0)\nprob_slope_gt_1 &lt;- mean(posterior_height$dbh_scaled &gt; 1.0)\nprob_slope_gt_1.5 &lt;- mean(posterior_height$dbh_scaled &gt; 1.5)\n\n# Credible interval\nslope_ci &lt;- quantile(posterior_height$dbh_scaled, c(0.025, 0.975))\n\n# Create summary table\ntibble(\n  Question = c(\n    \"Is the slope positive?\",\n    \"Is the slope greater than 1.0 m per 10 cm?\",\n    \"Is the slope greater than 1.5 m per 10 cm?\",\n    \"95% Credible Interval\"\n  ),\n  Answer = c(\n    sprintf(\"P(β &gt; 0) = %.3f\", prob_slope_positive),\n    sprintf(\"P(β &gt; 1.0) = %.3f\", prob_slope_gt_1),\n    sprintf(\"P(β &gt; 1.5) = %.3f\", prob_slope_gt_1.5),\n    sprintf(\"[%.2f, %.2f] m per 10 cm\", slope_ci[1], slope_ci[2])\n  )\n) %&gt;%\n  knitr::kable(caption = \"Direct probability statements about the height-diameter relationship\")\n\n\n\nDirect probability statements about the height-diameter relationship\n\n\nQuestion\nAnswer\n\n\n\n\nIs the slope positive?\nP(β &gt; 0) = 0.999\n\n\nIs the slope greater than 1.0 m per 10 cm?\nP(β &gt; 1.0) = 0.738\n\n\nIs the slope greater than 1.5 m per 10 cm?\nP(β &gt; 1.5) = 0.232\n\n\n95% Credible Interval\n[0.44, 1.97] m per 10 cm\n\n\n\n\n\n\n\n\n\n\n\nThis is what makes Bayesian inference powerful\n\n\n\nWe can make direct probability statements about parameters. Based on these results, we can say: “There is a 73.8% probability that height increases more than 1 meter for every 10 cm increase in diameter.”\nThese are the statements researchers and decision-makers actually want – direct quantification of uncertainty about the parameter itself."
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#visualizing-the-posterior",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#visualizing-the-posterior",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "Visualizing the Posterior",
    "text": "Visualizing the Posterior\n\n\nShow code\n# Create prediction grid\nnewdata &lt;- tibble(\n  dbh_scaled = seq(min(trees$dbh_scaled), max(trees$dbh_scaled), length.out = 100)\n)\n\n# Sample 100 posterior draws for visualization\nset.seed(456)\nsample_draws &lt;- sample(1:nrow(posterior_height), size = 100)\n\nfitted_draws_height &lt;- crossing(\n  newdata,\n  .draw = sample_draws\n) %&gt;%\n  mutate(\n    mu = posterior_height$`(Intercept)`[.draw] + posterior_height$dbh_scaled[.draw] * dbh_scaled,\n    dbh = dbh_scaled * scale_factor + mean(trees$dbh)\n  )\n\nggplot() +\n  # Posterior mean regression lines (blue lines)\n  geom_line(\n    data = fitted_draws_height,\n    aes(x = dbh, y = mu, group = .draw),\n    alpha = 0.2,\n    color = \"blue\"\n  ) +  \n  # Observed data\n  geom_point(\n    data = trees,\n    aes(dbh, height),\n    size = 2,\n    alpha = 0.7\n  ) +\n  labs(\n    title = \"Posterior: Regression Lines Show Our Updated Beliefs\",\n    subtitle = \"Each blue line is one plausible relationship given the data\",\n    x = \"Diameter at Breast Height (cm)\",\n    y = \"Height (m)\",\n    caption = \"100 posterior draws | All lines have positive slopes -- our data confirmed trees grow up!\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nKey insight: Every line has a positive slope because the data overwhelmingly support a positive relationship. The variation in lines represents our remaining uncertainty about the exact slope and intercept."
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#posterior-predictive-check",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#posterior-predictive-check",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "Posterior Predictive Check",
    "text": "Posterior Predictive Check\n\n\nShow code\npp_check(fit_height, ndraws = 100) +\n  labs(\n    title = \"Posterior Predictive Check: Does Our Model Make Sense?\",\n    subtitle = \"Comparing real data to simulated data from the model\",\n    x = \"Height (m)\",\n    y = \"Density\",\n    caption = \"Dark line = observed data | Light lines = 100 simulated datasets from posterior\"\n  )\n\n\n\n\n\n\n\n\n\nThe observed data (dark line) falls well within the range of simulated datasets – our model captures the essential features of tree heights."
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#what-weve-learned-from-this-example",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#what-weve-learned-from-this-example",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "What We’ve Learned from This Example",
    "text": "What We’ve Learned from This Example\nThis simple example demonstrated several key principles:\n\nPrior predictive checks reveal problems: Flat priors allow nonsense predictions\nWeakly informative priors encode domain knowledge: We ruled out negative heights without being overly restrictive\nDirect probability statements: We can say “P(slope &gt; 1) = 0.85” directly\nFull uncertainty visualization: Posterior draws show all plausible relationships\nModel checking: Posterior predictive checks validate our model assumptions\n\nNow let’s apply these same principles to a more complex problem: predicting timber volume with multiple predictors and making economic decisions under uncertainty."
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#part-a-volume-prediction-model",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#part-a-volume-prediction-model",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "Part A: Volume Prediction Model",
    "text": "Part A: Volume Prediction Model\nFirst, let’s build a multiple regression model predicting volume from both DBH and height.\n\nThe Model\n\\[\n\\begin{aligned}\ny_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_1 \\cdot \\text{dbh}_i + \\beta_2 \\cdot \\text{height}_i \\\\\n\\alpha &\\sim \\text{Normal}(0, 1) \\\\\n\\beta_1 &\\sim \\text{Normal}(0, 1) \\\\\n\\beta_2 &\\sim \\text{Normal}(0, 1) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{aligned}\n\\]\nIn words:\n\nWhat we’re predicting: \\(y_i\\) is the observed volume (in cubic meters) for tree \\(i\\)\nThe model structure: Each tree’s volume comes from a Normal distribution with mean \\(\\mu_i\\) and standard deviation \\(\\sigma\\)\nThe relationship: Mean volume \\(\\mu_i\\) depends linearly on both diameter and height through \\(\\alpha + \\beta_1 \\cdot \\text{dbh}_i + \\beta_2 \\cdot \\text{height}_i\\)\nWhat the priors represent: Our uncertainty in the model parameters before seeing the data\n\n\\(\\text{Normal}(0, 1)\\) for intercept (\\(\\alpha\\)) and slopes (\\(\\beta_1\\), \\(\\beta_2\\))\n\\(\\text{Exponential}(1)\\) for residual standard deviation (\\(\\sigma\\))\n\nWhy these priors work: Since predictors are scaled (standardized to mean 0, SD 1), these priors:\n\nEncode weak prior information (expect modest-sized effects)\nAllow the data to dominate the posterior\nPrevent extreme parameter values that would produce nonsensical predictions\n\n\n\n\nFitting the Volume Model\n\n\nShow code\nlibrary(knitr)\nlibrary(tibble)\n\n# Scale predictors for prior specification\ntrees &lt;- trees %&gt;%\n  mutate(\n    height_scaled = (height - mean(height)) / sd(height),\n    dbh_scaled_vol = (dbh - mean(dbh)) / sd(dbh)\n  )\n\n# Fit Bayesian multiple regression\nfit_volume &lt;- stan_glm(\n  volume ~ dbh_scaled_vol + height_scaled,\n  data = trees,\n  family = gaussian(),\n  prior_intercept = normal(0, 1),\n  prior = normal(0, 1),\n  prior_aux = exponential(1),\n  chains = 4,\n  iter = 2000,\n  seed = 456,\n  refresh = 0\n)\n\n# Display posterior summary as formatted table\nsummary(fit_volume, probs = c(0.025, 0.975), digits = 3) %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"Parameter\") %&gt;%\n  kable(caption = \"Posterior Summary Statistics Volume Model\", digits = 3)\n\n\n\nPosterior Summary Statistics Volume Model\n\n\nParameter\nmean\nmcse\nsd\n2.5%\n97.5%\nn_eff\nRhat\n\n\n\n\n(Intercept)\n0.854\n0.000\n0.021\n0.813\n0.894\n3955\n1.001\n\n\ndbh_scaled_vol\n0.418\n0.000\n0.025\n0.369\n0.467\n2705\n1.000\n\n\nheight_scaled\n0.061\n0.001\n0.025\n0.011\n0.110\n2537\n1.001\n\n\nsigma\n0.115\n0.000\n0.016\n0.089\n0.151\n3020\n1.001\n\n\nmean_PPD\n0.854\n0.000\n0.029\n0.796\n0.910\n4149\n1.001\n\n\nlog-posterior\n18.360\n0.035\n1.444\n14.833\n20.197\n1659\n1.001\n\n\n\n\n\n\n\nMaking Probability Statements About Effects\nKey question: How certain are we that height increases volume?\nBecause we’ve scaled both predictors (standardized to mean 0, standard deviation 1), the coefficients βheight and βDBH represent the change in volume for a 1 standard deviation change in each predictor.\nFor this dataset, 1 SD of height ≈ 2 meters and 1 SD of DBH ≈ 4 cm. This scaling allows us to directly compare effect sizes: larger coefficient = stronger effect per SD change.\n\n\nShow code\n# Extract posterior samples\nposterior_volume &lt;- as_draws_df(fit_volume)\n\n# Direct probability statements about effects\nheight_prob_positive &lt;- mean(posterior_volume$height_scaled &gt; 0)\nheight_prob_gt_005 &lt;- mean(posterior_volume$height_scaled &gt; 0.05)\ndbh_prob_positive &lt;- mean(posterior_volume$dbh_scaled_vol &gt; 0)\nprob_dbh_larger &lt;- mean(posterior_volume$dbh_scaled_vol &gt; posterior_volume$height_scaled)\n\n# Create summary table\ntibble(\n  Question = c(\n    \"Does height increase volume?\",\n    \"Is height effect &gt; 0.05?\",\n    \"Does DBH increase volume?\",\n    \"Is DBH effect larger than height effect?\"\n  ),\n  Answer = c(\n    sprintf(\"P(β&lt;sub&gt;height&lt;/sub&gt; &gt; 0) = %.3f\", height_prob_positive),\n    sprintf(\"P(β&lt;sub&gt;height&lt;/sub&gt; &gt; 0.05) = %.3f\", height_prob_gt_005),\n    sprintf(\"P(β&lt;sub&gt;DBH&lt;/sub&gt; &gt; 0) = %.3f\", dbh_prob_positive),\n    sprintf(\"P(β&lt;sub&gt;DBH&lt;/sub&gt; &gt; β&lt;sub&gt;height&lt;/sub&gt;) = %.3f\", prob_dbh_larger)\n  )\n) %&gt;%\n  knitr::kable(caption = \"Direct probability statements about volume predictors\",\n               escape = FALSE)\n\n\n\nDirect probability statements about volume predictors\n\n\n\n\n\n\nQuestion\nAnswer\n\n\n\n\nDoes height increase volume?\nP(βheight &gt; 0) = 0.992\n\n\nIs height effect &gt; 0.05?\nP(βheight &gt; 0.05) = 0.674\n\n\nDoes DBH increase volume?\nP(βDBH &gt; 0) = 1.000\n\n\nIs DBH effect larger than height effect?\nP(βDBH &gt; βheight) = 1.000\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nWe can say with near certainty (\\(P &gt; 0.999\\)) that both DBH and height increase timber volume. DBH has the stronger effect per standard deviation change, which makes biological sense – volume depends on cross-sectional area (proportional to DBH²) and height.\nNote on notation: \\(P(\\beta_{\\text{height}} &gt; 0)\\) means “the probability that the height coefficient is positive.” In Bayesian inference, we can make these direct probability statements because we treat parameters as random variables with probability distributions."
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#part-b-tree-improvement-economics",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#part-b-tree-improvement-economics",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "Part B: Tree Improvement Economics",
    "text": "Part B: Tree Improvement Economics\nNow we use this understanding to evaluate a business decision: Should we invest in genetically improved seedlings?\n\nThe Scenario\nA forestry company is considering switching from wild white spruce seed ($0.50 per seedling) to improved seed from a breeding program ($2.50 per seedling). The breeding program claims 15-20% volume gains.\nEconomic question: If we plant 1 million seedlings, will the additional volume justify the $2 million extra investment?\n\n\n\n\n\n\nSimplified Economic Model\n\n\n\nFor pedagogical clarity, this example uses a simplified economic model where we assume each additional m³ of timber generates $50 in profit. This is a hypothetical number chosen to illustrate the Bayesian decision-making workflow.\nIn reality, forest economics involves complex factors including:\n\nStumpage fees paid to government\nVariable harvest and processing costs\nMarket price fluctuations\nDiscount rates over long rotations (60-80 years)\nRegulatory constraints on harvest levels\n\nSee the Alberta Forest Management box below for a realistic example of how these factors interact in practice. The key point is that the Bayesian workflow remains identical regardless of model complexity – we simply add more distributions and propagate more uncertainty.\n\n\n\n\nSimulating Realistic Data\n\n\nShow code\nset.seed(2025)\n\n# Sample size\nn &lt;- 150  # Trees measured per seedlot\n\n# Simulate wild seedlot (unimproved)\nwild_trees &lt;- tibble(\n  seedlot = \"Wild\",\n  age = 25,\n  dbh = rnorm(n, mean = 18, sd = 3.5),  # cm at 25 years\n  height = rnorm(n, mean = 14, sd = 2.2)  # meters\n) %&gt;%\n  mutate(\n    # Volume relationship with biological realism\n    log_volume = -9.5 + 2 * log(dbh) + 1 * log(height) + rnorm(n, 0, 0.15),\n    volume = exp(log_volume)   # cubic meters (m³)\n  )\n\n# Simulate improved seedlot (genetically superior)\nimproved_trees &lt;- tibble(\n  seedlot = \"Improved\",\n  age = 25,\n  dbh = rnorm(n, mean = 19.5, sd = 3.0),  # Larger and more uniform\n  height = rnorm(n, mean = 15.2, sd = 1.9)  # Taller and more uniform\n) %&gt;%\n  mutate(\n    log_volume = -9.5 + 2 * log(dbh) + 1 * log(height) + rnorm(n, 0, 0.12),\n    volume = exp(log_volume)  # cubic meters (m³)\n  )\n\n# Combine datasets\ntree_comparison &lt;- bind_rows(wild_trees, improved_trees)\n\n# Visualize the comparison\np1 &lt;- ggplot(tree_comparison, aes(x = volume, fill = seedlot)) +\n  geom_histogram(alpha = 0.6, position = \"identity\", bins = 30) +\n  scale_fill_manual(values = c(\"Wild\" = \"#d95f02\", \"Improved\" = \"#1b9e77\")) +\n  labs(\n    title = \"Volume Distribution by Seedlot Type\",\n    x = \"Volume (m³)\",\n    y = \"Count\",\n    fill = \"Seedlot\"\n  ) +\n  theme_minimal()\n\np2 &lt;- ggplot(tree_comparison, aes(x = seedlot, y = volume, fill = seedlot)) +\n  geom_boxplot(alpha = 0.7) +\n  scale_fill_manual(values = c(\"Wild\" = \"#d95f02\", \"Improved\" = \"#1b9e77\")) +\n  labs(\n    title = \"Volume Comparison\",\n    x = \"Seedlot Type\",\n    y = \"Volume (m³)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\nBayesian Models for Each Seedlot\n\n\nShow code\n# Fit model for wild seedlot\nfit_wild &lt;- stan_glm(\n  volume ~ dbh + height,\n  data = wild_trees,\n  family = gaussian(),\n  prior_intercept = normal(0, 100),\n  prior = normal(0, 50),\n  prior_aux = exponential(0.1),\n  chains = 4,\n  iter = 4000,\n  seed = 123,\n  refresh = 0\n)\n\n# Display posterior summary as formatted table\nsummary(fit_wild, probs = c(0.025, 0.975), digits = 3) %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"Parameter\") %&gt;%\n  kable(caption = \"Posterior Summary Statistics: Wild Seedlot\", digits = 3)\n\n\n\nPosterior Summary Statistics: Wild Seedlot\n\n\nParameter\nmean\nmcse\nsd\n2.5%\n97.5%\nn_eff\nRhat\n\n\n\n\n(Intercept)\n-0.704\n0.001\n0.043\n-0.790\n-0.620\n7083\n1.000\n\n\ndbh\n0.039\n0.000\n0.002\n0.036\n0.042\n8355\n1.000\n\n\nheight\n0.025\n0.000\n0.002\n0.020\n0.030\n7446\n1.000\n\n\nsigma\n0.065\n0.000\n0.004\n0.058\n0.073\n1980\n1.001\n\n\nmean_PPD\n0.350\n0.000\n0.008\n0.335\n0.365\n2962\n1.002\n\n\nlog-posterior\n185.494\n0.032\n1.452\n181.873\n187.323\n2119\n1.000\n\n\n\n\n\nShow code\n# Fit model for improved seedlot\nfit_improved &lt;- stan_glm(\n  volume ~ dbh + height,\n  data = improved_trees,\n  family = gaussian(),\n  prior_intercept = normal(0, 100),\n  prior = normal(0, 50),\n  prior_aux = exponential(0.1),\n  chains = 4,\n  iter = 4000,\n  seed = 456,\n  refresh = 0\n)\n\n# Display posterior summary as formatted table\nsummary(fit_improved, probs = c(0.025, 0.975), digits = 3) %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"Parameter\") %&gt;%\n  kable(caption = \"Posterior Summary Statistics: Improved Seedlot\", digits = 3)\n\n\n\nPosterior Summary Statistics: Improved Seedlot\n\n\nParameter\nmean\nmcse\nsd\n2.5%\n97.5%\nn_eff\nRhat\n\n\n\n\n(Intercept)\n-1.031\n0.001\n0.051\n-1.132\n-0.932\n7998\n1.000\n\n\ndbh\n0.049\n0.000\n0.002\n0.046\n0.052\n9292\n1.000\n\n\nheight\n0.035\n0.000\n0.003\n0.029\n0.040\n7530\n1.000\n\n\nsigma\n0.067\n0.000\n0.004\n0.060\n0.075\n2132\n1.001\n\n\nmean_PPD\n0.456\n0.000\n0.008\n0.441\n0.472\n3698\n1.000\n\n\nlog-posterior\n180.875\n0.032\n1.423\n177.265\n182.653\n2024\n1.001\n\n\n\n\n\n\n\nAnswering Economic Questions with Probability\nQuestion 1: What’s the expected volume gain?\n\n\nShow code\n# To properly assess genetic improvement, we compare predictions for \n# TYPICAL trees from each seedlot (using their respective mean sizes)\n# This captures both effects: (1) improved seedlings grow bigger, and \n# (2) improved seedlings may have better stem form\n\n# Typical wild seedlot tree\ntypical_wild &lt;- tibble(\n  dbh = mean(wild_trees$dbh),\n  height = mean(wild_trees$height)\n)\n\n# Typical improved seedlot tree  \ntypical_improved &lt;- tibble(\n  dbh = mean(improved_trees$dbh),\n  height = mean(improved_trees$height)\n)\n\n# Get posterior predictions - each model predicts for its own typical tree\npred_wild &lt;- posterior_predict(fit_wild, newdata = typical_wild)\npred_improved &lt;- posterior_predict(fit_improved, newdata = typical_improved)\n\n# Calculate volume difference\nvolume_gain &lt;- pred_improved - pred_wild\ngain_percent &lt;- 100 * volume_gain / pred_wild\n\n# Summarize\ntibble(\n  Metric = c(\n    \"Mean volume gain\",\n    \"Median volume gain\",\n    \"95% Credible Interval\",\n    \"Mean percent gain\",\n    \"Median percent gain\"\n  ),\n  Value = c(\n    sprintf(\"%.3f m³\", mean(volume_gain)),\n    sprintf(\"%.3f m³\", median(volume_gain)),\n    sprintf(\"[%.3f, %.3f] m³\", quantile(volume_gain, 0.025), quantile(volume_gain, 0.975)),\n    sprintf(\"%.1f%%\", mean(gain_percent)),\n    sprintf(\"%.1f%%\", median(gain_percent))\n  )\n) %&gt;%\n  knitr::kable(caption = \"Expected volume gain from tree improvement\")\n\n\n\nExpected volume gain from tree improvement\n\n\nMetric\nValue\n\n\n\n\nMean volume gain\n0.105 m³\n\n\nMedian volume gain\n0.104 m³\n\n\n95% Credible Interval\n[-0.082, 0.291] m³\n\n\nMean percent gain\n35.1%\n\n\nMedian percent gain\n30.1%\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding this comparison\n\n\n\nWe’re comparing typical trees from each seedlot (using their respective mean sizes). This captures the total genetic improvement effect, which includes:\n\nSize advantage: Improved trees grow larger (bigger DBH, taller)\nPotential form advantage: If improved trees have better stem form, taper, or wood density, this would be captured by different model coefficients (though our simulation keeps this simple)\n\nIn reality, tree breeding programs aim to improve both growth rate (get to merchantable size faster) AND stem quality (more volume per unit size). This comparison captures the combined effect, which is what matters economically: how much more volume will I harvest per tree at rotation age?\n\n\nQuestion 2: What’s the probability of achieving target gains?\n\n\nShow code\n# Probability of different gain thresholds\nprob_gain_positive &lt;- mean(volume_gain &gt; 0)\nprob_gain_10pct &lt;- mean(gain_percent &gt; 10)\nprob_gain_15pct &lt;- mean(gain_percent &gt; 15)\nprob_gain_20pct &lt;- mean(gain_percent &gt; 20)\n\n# Create summary table\ntibble(\n  Threshold = c(\n    \"Any positive gain\",\n    \"Gain &gt; 10%\",\n    \"Gain &gt; 15%\",\n    \"Gain &gt; 20%\"\n  ),\n  Probability = c(\n    sprintf(\"%.3f\", prob_gain_positive),\n    sprintf(\"%.3f\", prob_gain_10pct),\n    sprintf(\"%.3f\", prob_gain_15pct),\n    sprintf(\"%.3f\", prob_gain_20pct)\n  )\n) %&gt;%\n  knitr::kable(caption = \"Probability of achieving volume gain targets\")\n\n\n\nProbability of achieving volume gain targets\n\n\nThreshold\nProbability\n\n\n\n\nAny positive gain\n0.868\n\n\nGain &gt; 10%\n0.762\n\n\nGain &gt; 15%\n0.697\n\n\nGain &gt; 20%\n0.630\n\n\n\n\n\n\n\n\n\n\n\nBusiness interpretation\n\n\n\nThese direct probability statements answer the key question: “What’s the chance our breeding program delivers what it promises?” Decision-makers can use these probabilities to assess risk and make informed investment choices.\n\n\n\n\nGenerating Predictions with Full Uncertainty\nBefore we proceed to the economic analysis, let’s demonstrate one of Bayesian inference’s most powerful features: generating predictions that include all sources of uncertainty. This is crucial for realistic decision-making because it prevents us from being overconfident.\nWhen we predict future volumes, there are multiple sources of uncertainty:\n\nParameter uncertainty: We don’t know the exact regression coefficients (\\(\\alpha, \\beta_1, \\beta_2, \\sigma\\))\nIndividual tree variation: Even trees of the same size vary in volume due to genetics, microsite, and measurement error\nModel uncertainty: Our model is a simplification of reality\n\nThe posterior predictive distribution captures all of these simultaneously. Unlike just using the mean parameter estimates (which would give us a single “best guess”), the posterior predictive distribution gives us a full probability distribution of possible outcomes.\nWhy does this matter for tree improvement decisions?\nImagine you’re planning harvest revenues for the next rotation. If you only use the mean predicted volume gain (say, 15%), you might sign timber contracts based on that number. But what if the actual gain is only 5%? Or 25%? The posterior predictive distribution tells us the range of plausible outcomes, letting us:\n\nCalculate probability of meeting minimum revenue targets\nPlan for worst-case scenarios\nQuantify financial risk accurately\nMake conservative estimates for contracts\n\nLet’s generate posterior predictions for a future harvest of 1000 trees from each seedlot:\n\n\nShow code\n# Simulate a future harvest: 1000 trees from each seedlot\n# These will have the typical size characteristics of each seedlot\nset.seed(2025)\nn_future &lt;- 1000\n\n# Future wild seedlot trees\nfuture_wild &lt;- tibble(\n  dbh = rnorm(n_future, mean = mean(wild_trees$dbh), sd = sd(wild_trees$dbh)),\n  height = rnorm(n_future, mean = mean(wild_trees$height), sd = sd(wild_trees$height))\n)\n\n# Future improved seedlot trees  \nfuture_improved &lt;- tibble(\n  dbh = rnorm(n_future, mean = mean(improved_trees$dbh), sd = sd(improved_trees$dbh)),\n  height = rnorm(n_future, mean = mean(improved_trees$height), sd = sd(improved_trees$height))\n)\n\n# Generate posterior predictive distributions\n# Each prediction includes parameter uncertainty + residual variation\npred_future_wild &lt;- posterior_predict(fit_wild, newdata = future_wild)\npred_future_improved &lt;- posterior_predict(fit_improved, newdata = future_improved)\n\n# Calculate total volume per 1000 trees for each posterior draw\n# Each row of pred_future_* is one posterior draw, columns are individual trees\ntotal_volume_wild &lt;- rowSums(pred_future_wild)\ntotal_volume_improved &lt;- rowSums(pred_future_improved)\n\n# Volume gain per 1000 trees\ntotal_gain &lt;- total_volume_improved - total_volume_wild\ngain_per_tree &lt;- total_gain / n_future\n\n# Summarize\ntibble(\n  Metric = c(\n    \"Mean total volume (Wild)\",\n    \"Mean total volume (Improved)\",\n    \"Mean volume gain per 1000 trees\",\n    \"Mean volume gain per tree\",\n    \"95% CI for gain per tree\",\n    \"P(gain per tree &gt; 0.01 m³)\"\n  ),\n  Value = c(\n    sprintf(\"%.1f m³\", mean(total_volume_wild)),\n    sprintf(\"%.1f m³\", mean(total_volume_improved)),\n    sprintf(\"%.1f m³\", mean(total_gain)),\n    sprintf(\"%.4f m³\", mean(gain_per_tree)),\n    sprintf(\"[%.4f, %.4f] m³\", \n            quantile(gain_per_tree, 0.025), \n            quantile(gain_per_tree, 0.975)),\n    sprintf(\"%.3f\", mean(gain_per_tree &gt; 0.01))\n  )\n) %&gt;%\n  knitr::kable(caption = \"Posterior predictive distribution: Future harvest of 1000 trees per seedlot\")\n\n\n\nPosterior predictive distribution: Future harvest of 1000 trees per seedlot\n\n\nMetric\nValue\n\n\n\n\nMean total volume (Wild)\n348.3 m³\n\n\nMean total volume (Improved)\n455.9 m³\n\n\nMean volume gain per 1000 trees\n107.6 m³\n\n\nMean volume gain per tree\n0.1076 m³\n\n\n95% CI for gain per tree\n[0.0909, 0.1234] m³\n\n\nP(gain per tree &gt; 0.01 m³)\n1.000\n\n\n\n\n\n Economic Breakeven Analysis\nA critical question for decision-makers is: what volume gain do we need to justify the additional seedling cost? The figure below shows the full distribution of predicted volume gains per tree, with two key reference lines: the expected gain (blue dashed line) and the economic breakeven point (red dashed line). The breakeven point is calculated as the additional cost per seedling ($2.00) divided by the profit margin per cubic meter ($50), giving us the minimum volume gain needed to recover our investment. By comparing the distribution to this threshold, we can directly assess the probability that the tree improvement program will be economically viable.\n\n\nShow code\n# Visualize the uncertainty\nposterior_predictions &lt;- tibble(\n  total_gain = total_gain,\n  gain_per_tree = gain_per_tree\n)\n\n# Calculate the economic breakeven gain per tree\n# Additional cost per tree = $2.00\n# Hypothetical profit per m³ = $50\n# Breakeven volume gain = $2.00 / $50 = 0.04 m³ per tree\neconomic_breakeven &lt;- 2.00 / 50\n\nggplot(posterior_predictions, aes(x = gain_per_tree)) +\n  geom_histogram(bins = 50, fill = \"#1b9e77\", alpha = 0.7) +\n  geom_vline(xintercept = mean(gain_per_tree), \n             linetype = \"dashed\", color = \"blue\", linewidth = 1) +\n  geom_vline(xintercept = economic_breakeven, \n             linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Posterior Predictive Distribution: Volume Gain per Tree\",\n    subtitle = \"Full uncertainty for a future harvest of 1000 trees\",\n    x = \"Volume gain per tree (m³)\",\n    y = \"Count\",\n    caption = sprintf(\"Red line = economic breakeven (%.4f m³) | Blue line = expected gain (%.4f m³) | Distribution includes all uncertainty\",\n                      economic_breakeven, mean(gain_per_tree))\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey insight: Uncertainty matters for planning\n\n\n\nNotice the width of this distribution. While the mean gain per tree is around 0.1076 m³, the 95% credible interval spans from 0.0909 to 0.1234 m³.\nThis means that even though improved seedlings are expected to produce more volume, there’s substantial uncertainty about how much more. A harvest plan based solely on the mean prediction would miss this variability, potentially leading to:\n\nOvercommitted timber contracts if gains are at the lower end\nMissed revenue opportunities if gains are at the upper end\n\nCashflow problems from unexpected shortfalls\n\nBy using the full posterior predictive distribution, forest managers can:\n\nSet conservative harvest targets (e.g., use the 25th percentile)\nCalculate probability of meeting contract volumes\nPrice timber sales with appropriate risk premiums\nMake financially sound long-term investment decisions\n\nThis is Bayesian inference at its best: honest quantification of uncertainty that leads to better decisions.\n\n\nQuestion 3: Economic Payoff Analysis\n\n\nShow code\n# Economic parameters\ncost_per_seedling_wild &lt;- 0.50\ncost_per_seedling_improved &lt;- 2.50\nadditional_cost &lt;- cost_per_seedling_improved - cost_per_seedling_wild\n\ntrees_planted &lt;- 1e6  # 1 million trees\ntotal_investment &lt;- additional_cost * trees_planted\n\n# Hypothetical profit per m³ (simplified for illustration)\n# In reality, this would be: Product revenue - Stumpage - Harvest costs - Processing\nprofit_per_m3 &lt;- 50  # dollars per cubic meter\n\n# Calculate economic return for each posterior sample\neconomic_return &lt;- volume_gain * profit_per_m3 * trees_planted\nnet_benefit &lt;- economic_return - total_investment\nroi &lt;- 100 * net_benefit / total_investment\n\n# Summarize economic outcomes\ntibble(\n  Metric = c(\n    \"Total investment\",\n    \"Expected additional revenue\",\n    \"Expected net benefit\",\n    \"Expected ROI\",\n    \"95% CI for net benefit\",\n    \"Probability of positive ROI\"\n  ),\n  Value = c(\n    sprintf(\"$%.2f million\", total_investment / 1e6),\n    sprintf(\"$%.2f million\", mean(economic_return) / 1e6),\n    sprintf(\"$%.2f million\", mean(net_benefit) / 1e6),\n    sprintf(\"%.1f%%\", mean(roi)),\n    sprintf(\"[$%.2f, $%.2f] million\", \n            quantile(net_benefit, 0.025) / 1e6, \n            quantile(net_benefit, 0.975) / 1e6),\n    sprintf(\"%.3f\", mean(net_benefit &gt; 0))\n  )\n) %&gt;%\n  knitr::kable(caption = \"Economic analysis: Tree improvement investment (simplified model)\",\n             format = \"html\",\n             escape = FALSE,\n             row.names = FALSE)\n\n\n\nEconomic analysis: Tree improvement investment (simplified model)\n\n\nMetric\nValue\n\n\n\n\nTotal investment\n$2.00 million\n\n\nExpected additional revenue\n$5.25 million\n\n\nExpected net benefit\n$3.25 million\n\n\nExpected ROI\n162.6%\n\n\n95% CI for net benefit\n[$-6.09, $12.54] million\n\n\nProbability of positive ROI\n0.755\n\n\n\n\n\n\n\nVisualizing the Economic Decision\n\n\nShow code\n# Create dataframe for plotting\neconomic_df &lt;- tibble(\n  volume_gain = as.vector(volume_gain),\n  net_benefit = as.vector(net_benefit),\n  roi = as.vector(roi)\n)\n\n# Plot net benefit distribution\np1 &lt;- ggplot(economic_df, aes(x = net_benefit / 1e6)) +\n  geom_histogram(bins = 50, fill = \"#1b9e77\", alpha = 0.7) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(net_benefit) / 1e6, \n             linetype = \"dashed\", color = \"blue\", linewidth = 1) +\n  labs(\n    title = \"Posterior Distribution of Net Benefit\",\n    subtitle = \"Red line = break-even | Blue line = expected value\",\n    x = \"Net Benefit ($ millions)\",\n    y = \"Count\",\n    caption = sprintf(\"P(positive ROI) = %.3f\", mean(net_benefit &gt; 0))\n  ) +\n  theme_minimal()\n\n# Plot ROI distribution\np2 &lt;- ggplot(economic_df, aes(x = roi)) +\n  geom_histogram(bins = 50, fill = \"#7570b3\", alpha = 0.7) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(roi), \n             linetype = \"dashed\", color = \"blue\", linewidth = 1) +\n  labs(\n    title = \"Posterior Distribution of ROI\",\n    subtitle = \"Return on $2 million investment\",\n    x = \"ROI (%)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\nWhat’s the Minimum Required Gain?\n\n\nShow code\n# What volume gain do we need to break even?\nbreakeven_gain &lt;- total_investment / (profit_per_m3 * trees_planted)\nbreakeven_percent &lt;- 100 * breakeven_gain / mean(pred_wild)\n\n# Probability we exceed breakeven\nprob_exceed_breakeven &lt;- mean(volume_gain &gt; breakeven_gain)\n\n# Create summary table\ntibble(\n  Metric = c(\n    \"Breakeven volume gain\",\n    \"Breakeven percent gain\",\n    \"P(exceeding breakeven)\"\n  ),\n  Value = c(\n    sprintf(\"%.3f m³ per tree\", breakeven_gain),\n    sprintf(\"%.1f%%\", breakeven_percent),\n    sprintf(\"%.3f\", prob_exceed_breakeven)\n  )\n) %&gt;%\n  knitr::kable(caption = \"Breakeven analysis\")\n\n\n\nBreakeven analysis\n\n\nMetric\nValue\n\n\n\n\nBreakeven volume gain\n0.040 m³ per tree\n\n\nBreakeven percent gain\n11.4%\n\n\nP(exceeding breakeven)\n0.755\n\n\n\n\n\n\n\n\n\n\n\nBusiness decision: Investment NOT recommended\n\n\n\nWith the estimated volume gains and current profit margins, there is only a 75.5% probability that the tree improvement investment will be profitable.\nThis means there’s a 24.5% chance of LOSING money on this investment.\n\nWhy this is a poor investment:\n\nThe expected volume gains are not sufficient to offset the $2 million additional cost\nThe risk is asymmetric: small upside potential vs. substantial downside risk\nWith only a 1-in-3 chance of breaking even, this investment fails basic financial due diligence\n\n\n\nWhat would need to change?\nFor this investment to make sense, one or more of the following would need to improve:\n\nLarger volume gains: Breeding program would need to deliver &gt;20% gains consistently\nHigher profit margins: Product prices would need to increase or costs decrease\n\nLower seedling costs: Improved seed costs would need to decrease\nOther benefits not captured: Improved form, disease resistance, or faster growth to rotation\n\nDecision-makers should reject this investment unless substantial improvements in these factors can be demonstrated through additional trials or market analysis.\n\n\n\n\n\nSensitivity to Profit Margins\nHow sensitive is our investment decision to changes in profit margins? Let’s examine scenarios ranging from depressed markets (lower margins) to boom conditions (higher margins):\n\n\nShow code\n# Explore different profit margin scenarios\nprofit_scenarios &lt;- tibble(\n  scenario = c(\"Low (depressed market)\", \"Current (baseline)\", \"High (favorable market)\"),\n  profit_per_m3 = c(30, 50, 70)  # dollars per cubic meter\n)\n\n# Calculate ROI for each scenario\nroi_by_scenario &lt;- profit_scenarios %&gt;%\n  mutate(\n    economic_return = map_dbl(profit_per_m3, \n                              ~mean(volume_gain) * .x * trees_planted),\n    net_benefit = economic_return - total_investment,\n    roi = 100 * net_benefit / total_investment,\n    prob_positive = map_dbl(profit_per_m3,\n                           ~mean(volume_gain * .x * trees_planted &gt; total_investment))\n  )\n\nroi_by_scenario %&gt;%\n  select(scenario, profit_per_m3, roi, prob_positive) %&gt;%\n  knitr::kable(\n    digits = 1,\n    caption = \"ROI sensitivity to profit margins\",\n    col.names = c(\"Market Scenario\", \"Profit Margin ($/m³)\", \"Expected ROI (%)\", \"P(positive ROI)\")\n  )\n\n\n\nROI sensitivity to profit margins\n\n\n\n\n\n\n\n\nMarket Scenario\nProfit Margin ($/m³)\nExpected ROI (%)\nP(positive ROI)\n\n\n\n\nLow (depressed market)\n30\n57.6\n0.7\n\n\nCurrent (baseline)\n50\n162.6\n0.8\n\n\nHigh (favorable market)\n70\n267.7\n0.8\n\n\n\n\n\n\n\n\n\n\n\nKey insight\n\n\n\nEven in a depressed market ($30/m³ profit margin), the investment still shows reasonable returns. In favorable markets ($70/m³), returns are substantial. This sensitivity analysis demonstrates that the investment decision is robust to margin fluctuations – a critical consideration for long-term forestry investments that span decades and multiple market cycles.\nThe high probability of positive ROI across all margin scenarios (assuming our volume gain estimates are accurate) provides confidence for decision-making even under market uncertainty."
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#real-world-application-the-alberta-tree-improvement-investment-problem",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#real-world-application-the-alberta-tree-improvement-investment-problem",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "Real-World Application: The Alberta Tree Improvement Investment Problem",
    "text": "Real-World Application: The Alberta Tree Improvement Investment Problem\nHow would this analysis work in practice for an Alberta forest company? The simplified example above illustrates the Bayesian workflow clearly, but real forestry decisions involve additional layers of complexity – particularly around regulatory approval. Does this interest you? Just follow the rabbit down the hole 🐰\n\n\n\n\n\n\nThe Business Problem 🐰\n\n\n\n\n\nAlberta forest companies face a multi-million dollar question: Should we invest in tree improvement programs? The challenge isn’t biological – field trials consistently show 10-25% volume gains from improved seedlings. The challenge is that government recognizes only a fraction of demonstrated gains for regulatory purposes.\nIn Alberta’s Forest Management Agreement (FMA) system, companies lease harvesting rights on Crown land. To access additional timber volume from tree improvement, they need government approval to adjust their Annual Allowable Cut (AAC) – a mechanism called the Allowable Cut Effect (ACE).\nThe reality: As of 2016, seven Controlled Parentage Programs (tree improvement programs) existed in Alberta with government-approved height gains averaging 2.3% – far below the 10-25% demonstrated in field trials. Of these seven programs, only three companies found the economics compelling enough to incorporate the gains into their Forest Management Plans.\nWhy did the other four companies decline? With only 2.3% recognized gains, the return on investment becomes marginal. When you’re investing millions in breeding programs, seed orchards, and improved seedlings at $2.50 each (vs. $0.50 for wild seed), the economics only work if enough factors align favorably: high deployment rates, manageable costs, good survival rates, favorable markets.\nFrom industry’s perspective (Schreiber & Thomas, 2017): “The low levels of genetic gain and its translation into the desired ACE, the long timelines for TI program development (i.e., decades), and the lack of certainty and risk aversion with Government approvals do not justify the expenses involved.”\n\nWhy Standard Cost-Benefit Analysis Falls Short\nSchreiber & Thomas (2017) developed an economic model (TIIFA) for Alberta tree improvement programs. Using sensitivity analysis across different scenarios, they showed that investment can be profitable even with modest gains – but profitability depends on multiple uncertain factors aligning favorably.\nTheir key findings:\n\nWith 2% volume gain per decade and deployment reaching 15%+, investment is profitable at 8% discount rate\nDeployment area matters as much as genetic gain: Increasing deployment from 5% to 20% per decade has enormous impact\nProgram costs are manageable: Even at $5 million per decade, programs can be profitable\nDiscount rate is critical: NPV strongly positive at 4%, moderately positive at 8%, barely positive at 12%\n\nBut their model analyzed scenarios independently. In reality, companies face compound uncertainty:\n\nWill we achieve 15% deployment rates across our FMA area? (Operational)\nWill program costs stay under $5M per decade? (Financial)\nWill the 2.3% recognized gains actually translate to field performance? (Biological)\nWhat will stumpage fees and lumber prices be over the 20-year FMP cycle? (Market)\nWill climate change affect realization of genetic gains? (Environmental)\n\nTheir conclusion: “Investment in TI still remains a profitable enterprise... if the area planted is maximized on which improved seed is deployed.”\nThat “if” is doing a lot of work. When four out of seven companies look at the same analysis and decide not to incorporate gains into their plans, they’re implicitly saying: “We’re not confident enough that the conditions will align favorably.”\nStandard economic models give point estimates under assumed scenarios. What companies actually need to know: “What’s the probability our investment will be profitable given simultaneous uncertainty across all these factors?”\nGovernment faces similar challenges. Regulators must balance economic benefits (forest industry jobs, community sustainability) against conservation of public forests. When field trials show 10-25% gains but regulators approve only 2.3%, this conservatism reflects genuine uncertainty: Will gains materialize at scale across diverse sites? Will deployment rates be sufficient? A Bayesian framework could help regulators make more transparent, defensible decisions by quantifying: “At 80% confidence, we expect gains between 1.8-2.8%; approving 2.3% AAC increases keeps risk of over-harvest below 10%.” This provides accountable stewardship of public resources while supporting industry investment.\n\n\nThe Bayesian Advantage\nA Bayesian framework naturally handles compound uncertainty by treating each uncertain factor as a probability distribution rather than a fixed assumption:\nInstead of: “Assume we achieve 15% deployment”\nBayesian says: “Deployment rate ~ Beta(\\(\\alpha\\), \\(\\beta\\)) based on historical rates across our FMA areas”\nInstead of: “Assume costs stay at $3M per decade”\nBayesian says: “Program costs ~ LogNormal(\\(\\mu\\), \\(\\sigma\\)) based on past breeding program expenses”\nInstead of: “Assume 2.3% gains translate to field performance”\nBayesian says: “Realized gains ~ Normal(0.023, \\(\\sigma\\)) accounting for site-to-site variation”\nThen propagate all these uncertainties simultaneously through the economic model:\nA Bayesian framework handles this the same way we analyzed tree height-diameter relationships earlier in this post: treat each uncertain input as a probability distribution, sample from all distributions simultaneously, and propagate uncertainty through the economic calculation. Instead of running the analysis once with fixed assumptions, you run it thousands of times, each time drawing different combinations of deployment rates, costs, gains, and market conditions from their respective distributions.\nThe result isn’t a single number but a full probability distribution of outcomes. Instead of “Expected NPV = $7.4M,” you get probability statements that directly answer the business question: “There’s a 73% chance of positive returns, a 45% chance NPV exceeds $5M, and an 8% chance of losses over $2M.” This directly answers the business question: “What’s the probability this investment pays off given everything we’re uncertain about?”\nSchreiber & Thomas (2017) key insight makes it even more important: Deployment area matters as much as genetic gain. In Bayesian terms: If you can control deployment (reduce its uncertainty), you dramatically improve the probability distribution of returns. A company that commits to 20% deployment with high confidence has much better odds than one hoping for 15% but uncertain about achieving it.\n\n\nWhy This Matters\nForest companies face compound business uncertainty when evaluating long-term investments:\nOperational uncertainties:\n\nCan we realistically achieve 15-20% deployment across diverse FMA areas?\nWill field crews consistently plant improved stock rather than wild seed?\nHow do deployment logistics vary by terrain and access?\n\nFinancial uncertainties:\n\nWill breeding program costs stay manageable over decades?\nWhat will improved seedling costs be as production scales?\nHow sensitive are returns to discount rate assumptions?\n\nBiological uncertainties:\n\nWill 2.3% recognized gains translate to actual field performance?\nHow much site-to-site variation should we expect?\nWill climate change affect genetic gain realization?\n\nMarket uncertainties:\n\nWhat will stumpage fees and lumber prices be over 20-year FMP cycles?\nHow do mill closures or expansions affect our timber allocation?\n\nThis is where Bayesian inference provides clarity. Standard economic models analyze scenarios independently: “If deployment = 15% and costs = $3M and gains = 2.3%, then NPV = $7.4M.”\nBayesian models integrate across all uncertainties simultaneously: “Accounting for realistic uncertainty in deployment (10-25%), costs ($2-6M), gain realization (1.8-2.8%), and markets, there’s a 73% probability of positive NPV and a 45% probability NPV exceeds $5M.”\nThe critical insight: When only 3 of 7 companies proceed, it suggests the others looked at compound uncertainty and concluded the probability of success was too low. A Bayesian framework makes that reasoning explicit and quantifiable.\nMoreover, it identifies which uncertainties matter most. If deployment rate is the dominant factor (as Schreiber & Thomas (2017) suggest), companies can focus on operational commitments that reduce deployment uncertainty rather than waiting for higher genetic gains. A company that can guarantee 20% deployment with high confidence faces much better odds than one hoping for 25% genetic gains that regulators may not recognize.\nThe Bayesian workflow we’ve demonstrated – specify priors, update with data, propagate uncertainty, make probability statements – applies directly to these multi-million dollar, multi-decade investment decisions where numerous uncertainties compound.\nThe gap between 2.3% recognized gains and 10-25% demonstrated gains creates a difficult business problem:\n\nsmall recognized gains mean economics are marginal, and\ncompound uncertainties make outcomes unpredictable.\n\nOnly 3 of 7 Alberta programs found the risk-reward balance favorable. Quantifying probability of success under compound uncertainty – rather than analyzing scenarios independently – is exactly where Bayesian methods add value for complex, long-term business decisions."
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#the-journey-weve-taken",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#the-journey-weve-taken",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "The Journey We’ve Taken",
    "text": "The Journey We’ve Taken\nOver this three-post series, we’ve built a complete understanding of statistical inference:\nPost 1 (Frequentist Foundations): We learned that frequentist inference is about procedures with long-run error rates. Confidence intervals describe what happens across hypothetical repetitions. P-values measure compatibility with null models. These tools are powerful when you want known error rates for repeated decisions.\nPost 2 (Bayesian Foundations): We shifted to thinking about parameters as random variables representing our uncertainty. We learned that Bayesian inference naturally handles sequential learning, allows direct probability statements, and makes assumptions explicit through priors. The seed germination example showed these principles in action for a single parameter.\nPost 3 (This Post): We scaled those principles to multiple parameters without changing the fundamental logic. Tree height regression used the same workflow: specify priors, check if they’re sensible, update with data, validate the model, make probabilistic predictions. The tree improvement example showed how this framework naturally handles real business decisions under uncertainty."
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#core-principles-that-carry-forward",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#core-principles-that-carry-forward",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "Core Principles That Carry Forward",
    "text": "Core Principles That Carry Forward\n1. Prior choice is critical but not mysterious\n\nDon’t use flat priors – they allow nonsense\nUse weakly informative priors that encode basic domain knowledge\nAlways conduct prior predictive checks to validate your choices\nYour priors are assumptions made explicit, not a source of bias\n\n2. Probability statements answer real questions\n\n“P(β &gt; 0) = 0.98” directly answers “Is the effect positive?”\n“P(ROI &gt; 0) = 0.85” directly answers “Will this investment pay off?”\nNo need to translate between p-values and practical meaning\n\n3. Uncertainty propagates through all calculations\n\nFrom parameter posteriors → to predictions → to economic outcomes\nThis prevents false precision and enables realistic planning\nPoint estimates hide critical information that full distributions reveal\n\n4. The Bayesian workflow is consistent\n\nSpecify priors (encode domain knowledge)\nCheck priors (prior predictive checks)\nFit model (MCMC sampling)\nCheck convergence (Rhat, ESS, trace plots)\nValidate model (posterior predictive checks)\nMake inferences (probability statements)\nMake predictions (full uncertainty)\n\n5. Model checking is essential\n\nPrior predictive checks prevent nonsense inputs\nPosterior predictive checks catch model failures\nDiagnostic plots reveal computational problems\nBad models give unreliable answers – always validate"
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#what-changes-and-what-doesnt",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#what-changes-and-what-doesnt",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "What Changes and What Doesn’t",
    "text": "What Changes and What Doesn’t\nFrom 1 parameter to many:\n\n✓ Same conceptual framework\n✓ Same workflow steps\n✓ Same interpretation of credible intervals\n✗ More complex prior specification (but same principles)\n✗ More computationally demanding (but MCMC handles it)\n✗ More difficult visualization (but 2D plots of key relationships work)\n\nFrom simple models to complex ones:\n\nThe principles in this series extend to hierarchical models, time series, spatial statistics, and more\nComplexity grows in implementation, not in conceptual foundation\nIf you understand updating beliefs about germination rates, you understand Bayesian inference"
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#practical-wisdom",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#practical-wisdom",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "Practical Wisdom",
    "text": "Practical Wisdom\nWhen Bayesian methods shine:\n\nMaking sequential decisions (examine data as it arrives)\nQuantifying probability of specific outcomes (P(effect &gt; threshold))\nIncorporating prior knowledge from previous studies\nCommunicating uncertainty to non-statisticians\nSmall sample sizes where prior information helps\n\nWhen to be cautious:\n\nPrior specification requires thought – no autopilot\nComputational demands can be substantial\nDifferent analysts with different priors may disagree (but so will frequentist analysts with different analysis choices)\nSkeptical audiences may question prior choices (though the same audiences accept arbitrary \\(\\alpha\\)-levels)\n\nThe bottom line:\nBoth frequentist and Bayesian frameworks are legitimate approaches to statistical inference. They answer different questions and suit different contexts. The frequentist approach in Post 1 remains widely used, well-understood, and appropriate for many scientific questions. The Bayesian approach offers advantages for decision-making under uncertainty, sequential learning, and direct quantification of what we want to know.\nThe shift from thinking about procedures to thinking about beliefs is the essence of Bayesian inference. Once you internalize this shift – as we’ve done across three posts from seeds to trees – you can apply the same reasoning to arbitrarily complex models.\nThe real power isn’t in the fancy mathematics or the MCMC algorithms. It’s in the conceptual clarity of saying “Here’s what I believed before, here’s what I observed, here’s what I believe now, and here’s the probability of what I care about.”\nThat’s Bayesian inference. You already know how to think this way – we’ve just given you the mathematical tools to do it rigorously."
  },
  {
    "objectID": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#what-happens-with-flat-priors",
    "href": "posts/2025-10-27-understanding-bayesian-inference-linear-regression/index.html#what-happens-with-flat-priors",
    "title": "Understanding Bayesian Inference: Linear Regression",
    "section": "What Happens with Flat Priors?",
    "text": "What Happens with Flat Priors?\nLet’s use the same philosophy as Beta(1,1) for germination – expressing “complete ignorance” through very vague priors:\nSuppose we use very vague priors that express “ignorance”. This seems “objective” – we’re not favoring any particular values. But watch what happens when we use prior predictive checks (the same technique from the seed germination post) to see what kinds of predictions these priors produce before seeing any real data.\nFor this example, we’ll assume these vague “flat” priors:\n\n\\(\\alpha \\sim \\text{Normal}(0, 100)\\)\n\\(\\beta \\sim \\text{Normal}(0, 10)\\)\n\\(\\sigma \\sim \\text{Exponential}(0.1)\\)\n\n\n\nShow code\nset.seed(20250823)\n\n# Very vague priors—often called \"flat\" or \"uninformative\"\nn_draws &lt;- 1000\nalpha_prior_flat &lt;- rnorm(n_draws, mean = 0, sd = 100)  # Intercept: anywhere from -200 to +200 m\nbeta_prior_flat  &lt;- rnorm(n_draws, mean = 0, sd = 10)   # Slope: anywhere from -20 to +20 m per 10 cm\n\n# Grid for predictions across observed DBH range\ndbh_grid &lt;- seq(min(trees$dbh), max(trees$dbh), length.out = 100)\ndbh_grid_scaled &lt;- (dbh_grid - mean(trees$dbh)) / scale_factor\n\n# Generate prior predictive lines\nprior_pred_flat &lt;- map2_dfr(\n  alpha_prior_flat, beta_prior_flat,\n  ~ tibble(\n    dbh = dbh_grid,\n    height = .x + .y * dbh_grid_scaled\n  ),\n  .id = \"draw\"\n)\n\n# Sample 100 lines for visualization\nselected_draws &lt;- sample(unique(prior_pred_flat$draw), 100)\nprior_pred_subset &lt;- prior_pred_flat %&gt;% filter(draw %in% selected_draws)\n\nggplot(prior_pred_subset, aes(x = dbh, y = height, group = draw)) +\n  geom_line(alpha = 0.3, color = \"steelblue\") +\n  geom_vline(xintercept = mean(trees$dbh), linetype = \"dashed\", color = \"grey60\") +\n  ylim(-50, 100) +\n  labs(\n    title = \"Prior Predictive Check: Flat Priors\",\n    subtitle = \"100 randomly drawn regression lines from vague priors\",\n    x = \"Diameter (cm)\", \n    y = \"Predicted Height (m)\",\n    caption = \"Problem: Many lines predict negative heights or impossibly tall trees!\\nDashed line shows mean DBH where intercept is interpretable.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nProblems with flat priors:\n\nAllows negative tree heights (biologically impossible)\nAllows negative slopes (taller trees have larger diameters)\nAllows impossibly tall trees (100+ meters for black cherry)\nWastes probability mass on parameter combinations that could never occur\n\nFlat priors are not “objective” – they encode very strong (and wrong) assumptions. They say “a 50-meter tall sapling is just as plausible as a 20-meter mature tree” and “trees that shrink as they grow are reasonable.”\nThe lesson from seed germination still applies: priors encode assumptions whether we acknowledge them or not. The difference is that in regression, flat priors have worse consequences:\n\nBeta(1,1) for germination: Allows any rate from 0 to 1, which are all biologically possible\nNormal(0, 100) for tree height intercept: Allows negative heights and 100+ meter saplings, which are impossible\n\nThe key insight from our series: just as we used prior knowledge (published germination rates, historical supplier data) to inform the seed germination prior, we should use domain knowledge (trees don’t have negative height, trees grow up not down) to inform regression priors.\nYou’re not “cheating” by using informative priors – you’re incorporating knowledge that everyone already has. The alternative (flat priors) pretends you don’t know basic facts about the world, which is both dishonest and statistically harmful.\n\nWeakly Informative Priors\nNow let’s use priors that encode minimal biological knowledge:\n\n\nShow code\nset.seed(20250823)\n\n# Weakly informative priors based on minimal biological knowledge\nalpha_prior_inf &lt;- rnorm(n_draws, mean = 20, sd = 3)   # Height at mean DBH: around 15-25 m\nbeta_prior_inf &lt;- rnorm(n_draws, mean = 1, sd = 1)     # Slope: around 0-3 m per 10 cm, centered at 1\n\n# Generate prior predictive lines\nprior_pred_inf &lt;- map2_dfr(\n  alpha_prior_inf, beta_prior_inf,\n  ~ tibble(\n    dbh = dbh_grid,\n    height = .x + .y * dbh_grid_scaled\n  ),\n  .id = \"draw\"\n)\n\nselected_draws_inf &lt;- sample(unique(prior_pred_inf$draw), 100)\nprior_pred_subset_inf &lt;- prior_pred_inf %&gt;% filter(draw %in% selected_draws_inf)\n\nggplot(prior_pred_subset_inf, aes(x = dbh, y = height, group = draw)) +\n  geom_line(alpha = 0.3, color = \"steelblue\") +\n  geom_vline(xintercept = mean(trees$dbh), linetype = \"dashed\", color = \"grey60\") +\n  labs(\n    title = \"Prior Predictive Check: Weakly Informative Priors\",\n    subtitle = \"100 randomly drawn regression lines from biologically informed priors\",\n    x = \"Diameter (cm)\", \n    y = \"Predicted Height (m)\",\n    caption = \"All predictions are biologically plausible: positive heights (15-30 m), positive slopes.\\nDashed line shows mean DBH.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nMuch better! These priors:\n\nKeep heights positive and reasonable (15-30 m range)\nSupport positive relationships (taller trees have larger diameters)\nStill allow data to substantially update beliefs (wide enough to not be dogmatic)\nReflect minimal biological constraints (you don’t need a PhD in forestry)\n\nThis is the key insight: You’re not claiming to know the answer ahead of time; you’re just ruling out nonsense. It’s like saying “I don’t know exactly how tall that building is, but I know it’s not negative height and it’s not 1000 km tall.”"
  }
]