[
  {
    "objectID": "about.html#stefan-schreiber-ph.d.-p.biol.",
    "href": "about.html#stefan-schreiber-ph.d.-p.biol.",
    "title": "Who we are",
    "section": "Stefan Schreiber, Ph.D., P.Biol.",
    "text": "Stefan Schreiber, Ph.D., P.Biol.\nStefan holds a Ph.D. in Forest Biology and Management from the University of Alberta and is a certified Professional Biologist with the Alberta Society of Professional Biologists. Prior to his time in Canada, Stefan pursued his Bachelor’s and Master’s degree in Biology at Ruhr-University Bochum, specializing in plant ecology and molecular phylogeny. His passion are trees, specifically understanding their adaptive mechanisms to diverse environments. In addition to his academic pursuits, Stefan sharpened his skills in quantitative methods and statistics, establishing a robust foundation in R programming throughout his career. Beyond his work life, Stefan cherishes every moment with his wife and three children. He also has a passion for ultra-endurance sports, embodying a resilient spirit both in his academic and work-related endeavors, as well as personal pursuits."
  },
  {
    "objectID": "about.html#sanja-schreiber-ph.d.-m.sc.-b.sc",
    "href": "about.html#sanja-schreiber-ph.d.-m.sc.-b.sc",
    "title": "Who we are",
    "section": "Sanja Schreiber, Ph.D., M.Sc., B.Sc",
    "text": "Sanja Schreiber, Ph.D., M.Sc., B.Sc\nSanja is a clinician with a Ph.D. in Rehabilitation Science and a Postdoctorate in Evidence Synthesis, both from University of Alberta. She has extensive expertise in clinical trial design and methodology, conduct, and reporting, with an interest in bridging gaps between research and practice. She is an expert in evidence synthesis methodologies, including scoping reviews, systematic reviews, Cochrane systematic reviews, overviews of reviews and living systematic reviews, where she uses software technology to expedite review production and minimize human error. Employing the same rigorous methodology utilized in medical research, she directs environmental evidence syntheses production at EnviroStats Solutions, facilitating evidence-based policymaking and addressing complex environmental decisions. Outside of work, Sanja loves travel adventures with her husband and three children."
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Quantitative Environmental Services",
    "section": "",
    "text": "At EnviroStats Solutions, our focus centers on applying the most suitable and current analyses and visualizations to environmental data. We are well-versed in both traditional statistical methods and Bayesian statistics. Our day-to-day modeling involves a range of techniques:\n\nLinear models\nGeneralized linear models\nLinear mixed effects models\nGeneralized linear mixed effects models\nModels accommodating zero and one inflated data\nClassification and regression tree analyses\nMultivariate analyses, inclusive principal component analysis, linear discriminant analysis, and non-linear multidimensional scaling techniques\nGenerating Shiny Web Apps for easy visualization of data\nEvidence syntheses through scoping and systematic reviews using narrative or meta-analyses\n\nIn addition to our analytical work, we have ample experience in offering tailored R teaching workshops. These workshops cater to your specific requirements, providing participants with fundamental and advanced training in R, statistics, and data visualization."
  },
  {
    "objectID": "services.html#our-services",
    "href": "services.html#our-services",
    "title": "Quantitative Environmental Services",
    "section": "",
    "text": "At EnviroStats Solutions, our focus centers on applying the most suitable and current analyses and visualizations to environmental data. We are well-versed in both traditional statistical methods and Bayesian statistics. Our day-to-day modeling involves a range of techniques:\n\nLinear models\nGeneralized linear models\nLinear mixed effects models\nGeneralized linear mixed effects models\nModels accommodating zero and one inflated data\nClassification and regression tree analyses\nMultivariate analyses, inclusive principal component analysis, linear discriminant analysis, and non-linear multidimensional scaling techniques\nGenerating Shiny Web Apps for easy visualization of data\nEvidence syntheses through scoping and systematic reviews using narrative or meta-analyses\n\nIn addition to our analytical work, we have ample experience in offering tailored R teaching workshops. These workshops cater to your specific requirements, providing participants with fundamental and advanced training in R, statistics, and data visualization."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#why-the-normal-distribution-matters",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#why-the-normal-distribution-matters",
    "title": "Understanding Frequentist Inference",
    "section": "Why the Normal Distribution Matters",
    "text": "Why the Normal Distribution Matters\nNotice how closely the empirical sampling distribution above matches the theoretical normal curve (red line). This is no coincidence – it’s a consequence of the Central Limit Theorem (CLT), which states that the sampling distribution of the mean tends toward a normal distribution as sample size increases, even if the underlying data aren’t normally distributed, as long as the observations are independent and identically distributed (Casella & Berger, 2002).\nThis distribution, also known as the Gaussian distribution after Carl Friedrich Gauss who studied it extensively in the early 19th century (though it was discovered earlier by de Moivre (De Moivre, 1738; Stigler, 1986)), has remarkable properties that make it central to statistical inference.\nThis normality of the sampling distribution is fundamental because:\n\nIt’s mathematically well-defined: The normal distribution is symmetric with known properties, allowing us to calculate precise probabilities.\nIt enables p-value calculations: We can determine exactly how extreme our observed statistic is under the null hypothesis.\nIt justifies confidence intervals: We know what multiplier to use (from the t-distribution) to achieve desired coverage probabilities.\nIt provides known error rates: Our Type I error rate (\\(\\alpha\\)) is controlled at exactly the level we specify.\n\nWhen assumptions fail: If the \\(i.i.d.\\) assumptions are violated – such as with strong dependence between observations, severe skewness combined with small sample sizes, or non-identical distributions – the sampling distribution may not follow this normal shape. In such cases, our p-values, confidence intervals, and error rates become unreliable. This is why checking assumptions and considering robust or alternative methods is crucial when working with real data.\nNow that we understand that the sampling distribution is approximately normal (thanks to the CLT), we can leverage this mathematical property to make two types of inferential statements. First, we can test specific hypotheses about parameter values. Second, we can construct intervals that quantify our uncertainty. Let’s examine hypothesis testing first."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#how-confidence-intervals-are-constructed",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#how-confidence-intervals-are-constructed",
    "title": "Understanding Frequentist Inference",
    "section": "How Confidence Intervals Are Constructed",
    "text": "How Confidence Intervals Are Constructed\nRecall that the standard error (SE) measures the expected variability of the sample mean across repeated samples. We calculate it as \\(\\text{SE} = s/\\sqrt{n}\\), where \\(s\\) is the sample standard deviation and \\(n\\) is the sample size.\nTo build a confidence interval, we take our sample mean and add/subtract a margin of error:\n\\[\\text{CI} = \\bar{x} \\pm (\\text{critical value} \\times \\text{SE})\\]\nThe critical value is a multiplier that determines how wide our interval should be. Think of it as answering the question: “How many standard errors away from our estimate should we go to capture the true parameter with 95% confidence?”\nBut here’s where a subtle complication arises…"
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#from-theory-to-practice-why-we-need-the-t-distribution",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#from-theory-to-practice-why-we-need-the-t-distribution",
    "title": "Understanding Frequentist Inference",
    "section": "From Theory to Practice: Why We Need the t-Distribution",
    "text": "From Theory to Practice: Why We Need the t-Distribution\nSo far, we’ve been using the formula \\(\\text{SE} = s/\\sqrt{n}\\) as if it were straightforward. But there’s a subtle issue here: we’re using the sample standard deviation \\(s\\) to estimate the population standard deviation \\(\\sigma\\). This creates a problem.\nWhen we simulate the sampling distribution (as we did earlier), we know the true population parameters. But in real data analysis, we never know \\(\\sigma\\) – we can only estimate it from our sample. This estimation adds an extra source of uncertainty that the normal distribution doesn’t account for.\nEnter the t-distribution. William Sealy Gosset (writing under the pseudonym “Student”) discovered that when we replace \\(\\sigma\\) with \\(s\\), the sampling distribution is no longer exactly normal – it follows what we now call Student’s t-distribution (Student, 1908). The key features are:\n\nIt has heavier tails than the normal distribution (more probability in the extremes).\nIt’s wider than the normal distribution, reflecting our additional uncertainty.\nAs sample size increases, it converges to the normal distribution.\nIt has a parameter called “degrees of freedom” (\\(df = n - 1\\)), which controls how wide it is.\n\nFor small samples, using the normal distribution would give us confidence intervals that are too narrow – they’d capture the true parameter less than 95% of the time. The t-distribution corrects for this, giving us appropriately wider intervals that maintain the correct coverage probability.\nLet’s see this difference visually:\n\n\nShow code\nlibrary(tidyverse)\n\n# Generate data comparing normal and t-distributions\nx &lt;- seq(-4, 4, length.out = 500)\ndf_compare &lt;- tibble(\n  x = x,\n  Normal = dnorm(x),\n  `t (df=5)` = dt(x, df = 5),      # Small sample: heavier tails\n  `t (df=30)` = dt(x, df = 30)     # Larger sample: approaches normal\n) |&gt; \n  pivot_longer(cols = -x, names_to = \"distribution\", values_to = \"density\")\n\n# Plot comparison\nggplot(df_compare, aes(x = x, y = density, color = distribution)) +\n  geom_line(linewidth = 1) +\n  labs(\n    title = \"Comparing Normal and t-Distributions\",\n    subtitle = \"The t-distribution has heavier tails, especially with small sample sizes\",\n    x = \"Value\",\n    y = \"Density\",\n    color = \"Distribution\"\n  ) +\n  scale_color_manual(values = c(\"Normal\" = \"black\", \"t (df=5)\" = \"#d95f02\", \"t (df=30)\" = \"#1b9e77\")) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nNotice how the t-distribution with 5 degrees of freedom (orange) has much heavier tails than the normal distribution (black). With 30 degrees of freedom (green), it’s already quite close to normal. This is why the t-distribution matters most for small samples."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#what-a-confidence-interval-really-means",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#what-a-confidence-interval-really-means",
    "title": "Understanding Frequentist Inference",
    "section": "What a Confidence Interval Really Means",
    "text": "What a Confidence Interval Really Means\nNow that we understand why we use the t-distribution instead of the normal distribution, let’s clarify what our resulting confidence intervals actually mean – and more importantly, what they don’t mean.\nA 95% confidence interval does not mean there is a 95% probability that the true mean lies within a particular observed interval. In the frequentist framework, the parameter is fixed (though unknown), so it either is or isn’t in any particular interval – there’s no probability involved once we’ve observed our data.\nInstead, the 95% refers to the procedure: if we repeated the entire sampling and interval-construction process infinitely many times, about 95% of those intervals would contain the true population mean. Our single observed interval is just one realization from this process.\nNow let’s see this in action with a simulation:\n\n\nShow code\nlibrary(tidyverse)\n\n# Function to simulate one sample and compute its 95% CI\nsimulation &lt;- function(n, mu, stdev) {\n  s &lt;- rnorm(n, mu, stdev)\n  sample_mean &lt;- mean(s)\n  sample_sd &lt;- sd(s)\n  sample_se &lt;- sample_sd / sqrt(n)\n  confint_95 &lt;- qt(0.975, df = n - 1) * sample_se  # t-critical value × SE\n  tibble(\n    n = n,\n    sample_mean = sample_mean,\n    sample_sd = sample_sd,\n    sample_se = sample_se,\n    confint_95 = confint_95,\n    lower_bound = sample_mean - confint_95,\n    upper_bound = sample_mean + confint_95,\n    contains_mu = mu &gt;= sample_mean - confint_95 & mu &lt;= sample_mean + confint_95\n  )\n}\n\n# Set parameters and run simulation\nset.seed(777)\nn &lt;- 10; mu &lt;- 0; stdev &lt;- 5\nsim &lt;- 100\n\n# Generate 100 confidence intervals\ndraws &lt;- map(1:sim, ~ simulation(n, mu, stdev)) |&gt;  \n  bind_rows() |&gt;  \n  mutate(experiment_id = 1:sim)\n\n# Calculate observed coverage rate\ncoverage &lt;- mean(draws$contains_mu)\n\n# Visualize confidence intervals\nggplot(draws, aes(x = sample_mean, y = experiment_id, color = contains_mu)) + \n  geom_point(size = 3) + \n  geom_errorbarh(aes(xmin = lower_bound, xmax = upper_bound), linewidth = 1, alpha = 0.5) + \n  geom_vline(xintercept = mu, linetype = \"dashed\", color = \"grey40\") +\n  scale_color_manual(\n    name = \"95% Confidence Interval\",\n    values = c(\"TRUE\" = \"#1b9e77\", \"FALSE\" = \"#d95f02\"),\n    labels = c(\"Captures true mean\", \"Misses true mean\")\n  ) +\n  labs(\n    title = \"95% Confidence Intervals Across 100 Simulated Experiments\",\n    subtitle = sprintf(\"Teal = captures μ, Orange = misses μ | Observed coverage: %.0f%%\", \n                      coverage * 100),\n    x = \"Sample mean\",\n    y = \"Experiment ID\",\n    caption = \"Dashed line marks the true population mean (μ = 0)\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nThe practical challenge: While this frequentist interpretation is logically sound, it doesn’t directly answer the question most researchers want answered: “What can I say about my single observed interval?” This tension between the procedure’s long-run properties and what we can infer from one dataset is an inherent limitation of the frequentist approach."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#interpreting-the-plots",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#interpreting-the-plots",
    "title": "Understanding Frequentist Inference",
    "section": "Interpreting the Plots",
    "text": "Interpreting the Plots\nTop panel (\\(n = 30\\) - Underpowered):\n\nThe orange curve shows the sampling distribution if \\(H_0: \\mu = 100\\) is true.\nThe green curve shows the sampling distribution if \\(H_1: \\mu = 103\\) is true.\nStandard error is \\(\\text{SE} \\approx 1.83\\), creating relatively wide, overlapping distributions.\nThe orange shaded regions (in the tails under \\(H_0\\)) show Type I error rate \\(\\alpha = 0.05\\).\nThe teal shaded region (under \\(H_1\\), within the non-rejection region) shows Type II error rate \\(\\beta \\approx 0.63\\) - meaning if the true mean is 103, we’d fail to detect this effect 63% of the time.\nThe purple shaded regions (under \\(H_1\\), in the rejection regions) show power \\(\\approx 0.37\\) - only a 37% chance of correctly detecting the true effect.\n\nBottom panel (\\(n = 200\\) - Overpowered):\n\nSame hypotheses, but now \\(\\text{SE} \\approx 0.71\\) (much smaller due to very large sample size).\nThe distributions are extremely narrow with almost no overlap.\nThe teal region (\\(\\beta\\)) is now virtually invisible, with \\(\\beta \\approx 0.01\\) – we’d almost never miss the true effect.\nThe purple regions (power) dominate the \\(H_1\\) distribution, with power \\(\\approx 0.99\\) – nearly 100% chance of correctly detecting the true effect.\n\nKey observations:\nThe comparison reveals an important trade-off in study design. This tension between statistical and practical significance is particularly problematic in fields where underpowered studies are common (Button et al., 2013), leading to inflated effect size estimates and poor replication rates:\n\nToo small (\\(n = 30\\)): High risk of missing real effects, wasting research effort and potentially leading to false conclusions.\nToo large (\\(n = 200\\)): Excessive power means collecting far more data than necessary, wasting time, money, and resources.\n\nWhile the underpowered study clearly has problems, the overpowered study also raises concerns. Beyond a certain point, additional data provides diminishing returns – power increases very little while costs continue to mount. Moreover, with very large samples, even trivially small effects become “statistically significant,” which doesn’t necessarily mean they’re practically meaningful.\nThe Goldilocks principle of sample size: We need a sample size that’s “just right” – large enough to reliably detect effects that matter, but not so large that we waste resources or detect effects too small to care about. This is why prospective power analysis is essential for efficient research design."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#planning-sample-size-through-prospective-power-analysis",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#planning-sample-size-through-prospective-power-analysis",
    "title": "Understanding Frequentist Inference",
    "section": "Planning Sample Size Through Prospective Power Analysis",
    "text": "Planning Sample Size Through Prospective Power Analysis\nThe side-by-side comparison above revealed a critical challenge: too little power (\\(n = 30\\)) risks missing real effects, while excessive power (\\(n = 200\\)) wastes resources and may detect effects too trivial to matter. This raises a practical question: rather than guessing what constitutes an adequate sample size, how can we systematically determine the optimal n for our specific goals?\nThe answer lies in prospective power analysis – plotting the relationship between sample size and power to find the sweet spot:\n\n\nShow code\nlibrary(tidyverse)\n\n# Define parameters (matching previous visualization)\nmu_null &lt;- 100\nmu_alt &lt;- 103\nsd_pop &lt;- 10\nalpha &lt;- 0.05\n\n# Function to calculate statistical power for a given sample size\ncalculate_power &lt;- function(n, mu_null, mu_alt, sd_pop, alpha = 0.05) {\n  se &lt;- sd_pop / sqrt(n)\n  t_crit &lt;- qt(1 - alpha / 2, df = n - 1)\n  lower_crit &lt;- mu_null - t_crit * se\n  upper_crit &lt;- mu_null + t_crit * se\n  \n  # Power = probability of rejecting H0 when H1 is true\n  power &lt;- pnorm(lower_crit, mean = mu_alt, sd = se) + \n           (1 - pnorm(upper_crit, mean = mu_alt, sd = se))\n  return(power)\n}\n\n# Calculate power across range of sample sizes\nsample_sizes &lt;- seq(10, 200, by = 5)\npower_values &lt;- map_dbl(sample_sizes, ~calculate_power(.x, mu_null, mu_alt, sd_pop))\npower_df &lt;- tibble(n = sample_sizes, power = power_values)\n\n# Find sample size needed for conventional 80% power\nn_for_80_power &lt;- sample_sizes[which.min(abs(power_values - 0.80))]\n\n# Calculate power at our example sample sizes\npower_at_n30 &lt;- calculate_power(30, mu_null, mu_alt, sd_pop)\npower_at_n200 &lt;- calculate_power(200, mu_null, mu_alt, sd_pop)\n\n# Plot power curve\nggplot(power_df, aes(x = n, y = power)) +\n  geom_line(color = \"#1b9e77\", linewidth = 1.2) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"grey40\") +\n  annotate(\"point\", x = 30, y = power_at_n30, color = \"#d95f02\", size = 4) +\n  annotate(\"point\", x = 200, y = power_at_n200, color = \"#d95f02\", size = 4) +\n  annotate(\"point\", x = n_for_80_power, y = 0.80, color = \"#7570b3\", size = 4) +\n  annotate(\"text\", x = 30, y = power_at_n30 - 0.08, \n           label = sprintf(\"n=30\\nPower≈%.0f%%\", power_at_n30*100), \n           color = \"#d95f02\", size = 3.5, fontface = \"bold\") +\n  annotate(\"text\", x = 200, y = power_at_n200 - 0.08, \n            label = sprintf(\"n=200\\nPower≈%.0f%%\", power_at_n200*100), \n            color = \"#d95f02\", size = 3.5, fontface = \"bold\") +\n  annotate(\"text\", x = n_for_80_power, y = 0.88, \n           label = sprintf(\"n≈%d needed\\nfor 80%% power\", n_for_80_power), \n           color = \"#7570b3\", size = 3.5, fontface = \"bold\") +\n  scale_x_continuous(limits = c(0, 210)) +\n  labs(\n    title = \"Statistical Power as a Function of Sample Size\",\n    subtitle = expression(paste(\"Effect size: \", mu[1], \" - \", mu[0], \" = 3 (Cohen's d ≈ 0.3)\")),\n    x = \"Sample size (n)\",\n    y = expression(paste(\"Power (1 - \", beta, \")\")),\n    caption = \"Orange points = our examples; Purple point = sample size for 80% power\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\nKey insights:\n\nAt \\(n = 30\\), power is only about 38% – inadequate for reliable detection.\nAt \\(n = 200\\), power is nearly 100% – far more than needed, representing wasted resources.\nTo achieve conventional 80% power for this effect size, we’d need approximately \\(n \\approx 85\\).\nLarger sample sizes increase power by decreasing the standard error (\\(\\text{SE} = s/\\sqrt{n}\\)). Smaller SE means narrower. sampling distributions, which creates better separation between the null and alternative distributions, reducing overlap and making true effects easier to detect.\nThis is why prospective power analysis is crucial for study design – it helps us determine the sample size needed to detect effects of meaningful magnitude with adequate probability (Cohen, 1988).\n\nPractical lesson: The error and power visualization showed us the problem (low power with \\(n = 30\\)), and this plot shows us the solution (increase sample size to approximately 85). This demonstrates why researchers should conduct power analyses before collecting data, not after!\nNow that we understand how to plan studies and interpret individual tests, we need to confront a more subtle challenge: what happens when we conduct not just one test, but many? And beyond statistical significance, how do we assess whether effects actually matter in practice?"
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#multiple-testing-inflates-false-positives",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#multiple-testing-inflates-false-positives",
    "title": "Understanding Frequentist Inference",
    "section": "Multiple Testing Inflates False Positives",
    "text": "Multiple Testing Inflates False Positives\nImportant distinction: This problem applies to multiple tests on the same data, not to independent replication studies. If different researchers independently test the same hypothesis on different datasets, those p-values shouldn’t be corrected for multiple testing – that’s replication, which is exactly what science needs!\nWhen conducting multiple hypothesis tests within the same study or dataset, a critical issue emerges: false positives accumulate. If you conduct 100 tests on the same dataset where all null hypotheses are true and use a significance level of \\(\\alpha = 0.05\\), you expect about 5 false positives, as predicted by the Type I error rate.\nExamples where correction is needed:\n\nTesting 20 different biomarkers to see which predict disease.\nComparing 5 treatment groups (resulting in 10 pairwise comparisons).\nTesting the same intervention across 15 different outcomes.\nExploratory analysis where you examine many potential relationships.\n\nWhy this matters:\n\nWithout correction, the probability of getting at least one false positive increases dramatically.\nFor example, with 20 tests on the same dataset at \\(\\alpha = 0.05\\), there’s a 64% chance of at least one false positive even if all null hypotheses are true (\\(1 - (1-0.05)^{20} = 1 - 0.95^{20} \\approx 0.64\\)).\nThis is why multiple testing corrections (Bonferroni, FDR control (Benjamini & Hochberg, 1995), Holm-Bonferroni) are essential.\n\nPractical implication: When you conduct multiple tests, either:\n\nApply appropriate corrections (and accept reduced power).\nClearly distinguish exploratory vs. confirmatory analyses.\nPre-register your primary hypothesis to avoid the “multiple testing” critique.\n\nThe “replication crisis” in many sciences is partly attributable to (Ioannidis, 2005):\n\nRunning many tests but only reporting the significant ones.\nNot correcting for multiple comparisons.\nTreating exploratory findings as if they were confirmatory.\n\nA single \\(p &lt; 0.05\\) result should never be considered definitive evidence. Replication, effect sizes, and consideration of prior plausibility are all crucial."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#statistical-vs.-practical-significance",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#statistical-vs.-practical-significance",
    "title": "Understanding Frequentist Inference",
    "section": "Statistical vs. Practical Significance",
    "text": "Statistical vs. Practical Significance\nAnother crucial distinction: statistical significance is not the same as practical importance. With a large enough sample size, even tiny, meaningless differences will yield \\(p &lt; 0.05\\). For example:\n\n\nShow code\nlibrary(tidyverse)\nlibrary(knitr)\n\n# Simulate a statistically significant but practically meaningless result\nset.seed(999)\nn_large &lt;- 10000\nsmall_effect &lt;- 0.05  # Negligible effect: 0.05 SD difference\n\n# Generate two groups with trivial difference\ngroup1 &lt;- rnorm(n_large, mean = 0, sd = 1)\ngroup2 &lt;- rnorm(n_large, mean = small_effect, sd = 1)\n\n# Conduct t-test\nt_result &lt;- t.test(group1, group2)\n\n# Calculate effect size\ncohens_d &lt;- small_effect / 1\n\n# Summarize results\nresults_df &lt;- tibble(\n  Metric = c(\"Sample size per group\", \"Cohen's d (effect size)\", \"p-value\", \"Mean difference\"),\n  Value = c(\n    format(n_large, big.mark = \",\"),\n    sprintf(\"%.3f\", cohens_d),\n    sprintf(\"%.6f\", t_result$p.value),\n    sprintf(\"%.3f\", diff(t_result$estimate))\n  )\n)\n\nkable(results_df, align = c(\"l\", \"r\"))\n\n\n\n\n\nMetric\nValue\n\n\n\n\nSample size per group\n10,000\n\n\nCohen’s d (effect size)\n0.050\n\n\np-value\n0.000049\n\n\nMean difference\n0.058\n\n\n\n\n\nThis hypothetical example shows that with 10,000 observations per group, we can detect an effect size of only 0.05 standard deviations with high confidence – but such a small effect is likely meaningless in most practical contexts.\nKey principle: Always report and interpret effect sizes alongside p-values (Cohen, 1994). Ask not just “Is there an effect?” but “How large is the effect, and does that matter?”\nThis tension between statistical and practical significance naturally raises another question: under what conditions can we trust these inferential procedures? Throughout this post, we’ve repeatedly invoked assumptions like independence and normality. It’s time to examine what happens when these assumptions break down."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#when-is-frequentist-inference-robust",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#when-is-frequentist-inference-robust",
    "title": "Understanding Frequentist Inference",
    "section": "When is frequentist inference robust?",
    "text": "When is frequentist inference robust?\n\nModerate departures from normality are often acceptable for means (due to Central Limit Theorem), especially with larger samples (\\(n &gt; 30\\) as a rough guideline).\nUnequal variances can be addressed with Welch’s \\(t\\)-test instead of Student’s \\(t\\)-test.\nMild dependence might inflate Type I error rates but not completely invalidate inference."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#when-do-we-need-alternative-approaches",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#when-do-we-need-alternative-approaches",
    "title": "Understanding Frequentist Inference",
    "section": "When do we need alternative approaches?",
    "text": "When do we need alternative approaches?\n\nStrong dependence (time series, clustered data) requires specialized methods like mixed models (Pinheiro & Bates, 2000) or generalized estimating equations (Liang & Zeger, 1986).\nSevere non-normality with small samples might require non-parametric tests, or better yet, generalized linear models (GLMs) which can directly model non-normal response distributions without transformation (Bolker et al., 2009).\nHeteroscedasticity (unequal variances) with unequal group sizes can seriously distort inferences, but can be modeled explicitly using generalized least squares or variance structures in mixed models (Pinheiro & Bates, 2000).\n\nPractical advice: Always examine your data visually, check diagnostic plots, and consider whether your assumptions are reasonable. Statistical methods are tools that work best when their assumptions are at least approximately met."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#key-takeaways",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#key-takeaways",
    "title": "Understanding Frequentist Inference",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nFrequentist inference is about procedures, not single-case probabilities.\nThe sampling distribution and standard error are the foundation – they quantify how much our estimates would vary across repeated samples.\nThe t-distribution accounts for the additional uncertainty when we estimate the population standard deviation from sample data.\nConfidence intervals and hypothesis tests are mathematically linked procedures using the same sampling distribution machinery.\nP-values measure compatibility with a specific null model, not truth or importance.\nEffect sizes matter as much as (or more than) p-values for scientific interpretation.\nSample size planning via power analysis is essential for efficient research – too little power wastes effort, too much power wastes resources.\nMultiple testing inflates false positive rates – correct for this when appropriate.\nAssumptions matter, but many methods are reasonably robust to moderate violations.\nReplication and cumulative evidence are more reliable than single p-values.\n\nAll these tools rely on the framework of hypothetical repeated sampling and assumptions about the data-generating process. They give us a powerful and well-understood framework for reasoning about populations from samples – but they must be interpreted carefully and in proper context.\nA natural question emerges: Is there an alternative framework that addresses some of these interpretational challenges – particularly the indirect nature of probability statements in the frequentist approach? This brings us to Bayesian inference."
  },
  {
    "objectID": "mission.html",
    "href": "mission.html",
    "title": "Quantitative Environmental Services",
    "section": "",
    "text": "Solid scientific inference and the foundations of evidence-based decision-making are rooted in data that precisely address the core questions and objectives at hand. This underscores the critical importance of meticulously planned study designs and targeted data collection. As the stakes rise, so does the demand for confidence. Building this confidence hinges on:\n\nThoughtful Experimental Design:\nCrafting the right experimental framework tailored to your inquiry.\nAccurate Data Analysis:\nApplying sound analytical techniques that draw out reliable insights.\nTransparent Visualization:\nConveying findings truthfully and effectively.\n\nEnviroStats Solutions stands ready to provide expert guidance and consultation in data analysis, visualization, and interpretation, catering to those navigating the landscape of environmental challenges. Backed by an extensive background in both applied and foundational research, we offer comprehensive support for evidence-driven decision-making — guiding you from initial experimental design to conclusive recommendations.\nOur commitment extends beyond consultation. We’ll share our proficiency in data analysis and environmental statistics through tailor-made workshops and training sessions that align with your specific requirements. With over a decade of hands-on experience as dedicated users of the R Environment for Statistical Computing, we’re dedicated to equipping you for success."
  },
  {
    "objectID": "mission.html#our-mission",
    "href": "mission.html#our-mission",
    "title": "Quantitative Environmental Services",
    "section": "",
    "text": "Solid scientific inference and the foundations of evidence-based decision-making are rooted in data that precisely address the core questions and objectives at hand. This underscores the critical importance of meticulously planned study designs and targeted data collection. As the stakes rise, so does the demand for confidence. Building this confidence hinges on:\n\nThoughtful Experimental Design:\nCrafting the right experimental framework tailored to your inquiry.\nAccurate Data Analysis:\nApplying sound analytical techniques that draw out reliable insights.\nTransparent Visualization:\nConveying findings truthfully and effectively.\n\nEnviroStats Solutions stands ready to provide expert guidance and consultation in data analysis, visualization, and interpretation, catering to those navigating the landscape of environmental challenges. Backed by an extensive background in both applied and foundational research, we offer comprehensive support for evidence-driven decision-making — guiding you from initial experimental design to conclusive recommendations.\nOur commitment extends beyond consultation. We’ll share our proficiency in data analysis and environmental statistics through tailor-made workshops and training sessions that align with your specific requirements. With over a decade of hands-on experience as dedicated users of the R Environment for Statistical Computing, we’re dedicated to equipping you for success."
  },
  {
    "objectID": "posts/2024-04-10-evidence-synthesis/index.html",
    "href": "posts/2024-04-10-evidence-synthesis/index.html",
    "title": "The Power of Evidence Synthesis",
    "section": "",
    "text": "Implementation of research findings into practice is generally slow-paced. It is estimated that that it takes 17–20 years to get clinical innovations into practice (Morris, Wooding, and Grant 2011), whereas fewer than 50% of them ever make it into general usage (Bauer and Kirchner 2020). While primary studies are an important first step in generating evidence, knowledge synthesis (evidence synthesis) serve as a bridge that expedite this process, ensuring that valuable insights from primary research reach those who can benefit most from them.\nTake the medical field, for example. Here, evidence synthesis is not just a helpful tool; it’s a cornerstone of clinical decision-making. By systematically accessing, evaluating, and consolidating scientific information, evidence synthesis provides a recognized standard of rigor, objectivity, and transparency. It’s the compass guiding healthcare professionals toward the most effective treatments and interventions.\nInterestingly, while evidence synthesis has long been a staple in medical (https://www.cochrane.org/) and social sciences (https://www.campbellcollaboration.org), its adoption in other fields, like environmental sciences, has been surprisingly slow. Yet, recent discussions in journals such as Trends in Ecology & Evolution (Dicks, Walsh, and Sutherland 2014) and Conservation Biology (Kadykalo et al. 2021) highlight its potential in addressing complex questions surrounding policy-making and controversial decisions. By grounding such decisions in the most reliable evidence available, evidence synthesis becomes a vital tool in navigating the intricate landscape of environmental challenges.\nMaintaining the integrity of evidence synthesis requires clear standards and meticulous attention to detail. Every step of the process, from article search strategy and their selection criteria, to identifying potential biases in the evidence and scrutinizing the synthesis itself, must be conducted with precision. This meticulous process ensures trust and confidence in the findings, allowing end-users to rely on the synthesized evidence with certainty.\nIn the environmental sciences, two common forms of evidence synthesis exist:\n\nScoping reviews (evidence maps) and\nSystematic reviews\n\nScoping reviews are ideal for assessing the breadth of knowledge on a specific topic and identifying gaps in the literature. On the other hand, systematic reviews compile all existing evidence on a particular research question, offering a comprehensive overview of the available literature. Systematic reviews are considered at the top of hierarchy of evidence because they offer higher-quality evidence compared to expert opinions, observational or experimental studies (Figure 1).\n\n\n\n\n\n\nFigure 1: Hierarchy of evidence pyramid\n\n\n\nWhen primary studies exhibit considerable heterogeneity, a narrative synthesis of evidence provides a cohesive description that draws connections between findings. Conversely, when primary studies share similar design and conduct, a quantitative data synthesis like meta-analysis serves as a powerful tool for drawing robust conclusions.\nIn conclusion, evidence synthesis provides clarity in ever-growing research. Whether guiding medical practitioners toward optimal patient care or informing policymakers on environmental matters, its role in bridging the gap between research and practice cannot be overstated. By embracing evidence synthesis, researchers can avoid redundant studies, make better use of existing data, and ensure that decisions are based on the most reliable and comprehensive evidence available. This can help minimize waste in research by directing resources towards studies that are more likely to yield meaningful results and contribute significantly to the advancement of knowledge.\n\nPlease do not hesitate to reach out (%20info at envirostats dot ca) if you have any questions!\n\n\n\n\nReferences\n\nBauer, Mark S., and JoAnn Kirchner. 2020. “Implementation Science: What Is It and Why Should I Care?” Psychiatry Research, VSI:Implementation Science, 283 (January): 112376. https://doi.org/10.1016/j.psychres.2019.04.025.\n\n\nDicks, Lynn V., Jessica C. Walsh, and William J. Sutherland. 2014. “Organising Evidence for Environmental Management Decisions: A ‘4S’ Hierarchy.” Trends in Ecology & Evolution 29 (11): 607–13. https://doi.org/10.1016/j.tree.2014.09.004.\n\n\nKadykalo, Andrew N., Rachel T. Buxton, Peter Morrison, Christine M. Anderson, Holly Bickerton, Charles M. Francis, Adam C. Smith, and Lenore Fahrig. 2021. “Bridging Research and Practice in Conservation.” Conservation Biology 35 (6): 1725–37. https://doi.org/10.1111/cobi.13732.\n\n\nMorris, Zoë Slote, Steven Wooding, and Jonathan Grant. 2011. “The Answer Is 17 Years, What Is the Question: Understanding Time Lags in Translational Research.” Journal of the Royal Society of Medicine 104 (12): 510–20. https://doi.org/10.1258/jrsm.2011.110180."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Quantitative Environmental Services",
    "section": "Welcome!",
    "text": "Welcome!\nEnviroStats Solutions is a dynamic Edmonton-based consulting firm dedicated to delivering efficient and rigorous environmental statistics, data analysis, visualization, and interpretation services. Our focus revolves around the environmental domain, providing practical insights that drive meaningful decisions.\nBeyond our analytical services, we also offer professional training in statistics, data management, and visualization, all centered within the versatile R programming environment. These workshops are designed to empower you to confidently navigate the world of data.\nEager to know more about our journey and the impact we’re making? Don’t hesitate to reach out (%20info at envirostats dot ca)!"
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "",
    "text": "When in doubt, just update your beliefs. ;P"
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#the-fishing-story-bayesian-reasoning-in-daily-life",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#the-fishing-story-bayesian-reasoning-in-daily-life",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "The Fishing Story: Bayesian Reasoning in Daily Life",
    "text": "The Fishing Story: Bayesian Reasoning in Daily Life\nMy oldest son recently got into fishing and asked where we could try along the North Saskatchewan River. We didn’t just pick a random spot – we built a prior. We looked at fishing forums, asked in tackle shops, and thought about where fish like to hide (around structure like fallen trees). We also preferred places near parking so the walk wasn’t too long. That’s all prior information we used to narrow down our choices.\nThen we went out and tested those beliefs. After a few hours, my son had caught three small perch near a spot with a big willow tree and nothing at a location under the bridge. We updated our beliefs: the willow tree location moved up in our mental ranking, while the bridge spot moved down. Next weekend, we’ll use this updated knowledge as our new prior – starting at the willow tree instead of exploring randomly.\nThis process – combining what you know ahead of time with what you observe – is exactly how Bayesian inference works.\nA frequentist approach would be different. You’d go to a spot with no prior assumptions and fish for a fixed period – say 2 hours. If you caught 3 fish, you might construct a confidence interval: “If I fished here many times for 2 hours each, 95% of my confidence intervals would contain the true catch rate.” Notice this doesn’t tell you “there’s a 95% chance the true rate is between X and Y” – it’s a statement about the procedure, not the parameter.\nBayesian reasoning is always conditional on the data actually observed – we update beliefs based on what really happened. Frequentist inference requires imagining repeated experiments under identical conditions, which is straightforward in controlled experiments but often artificial in observational studies or unique situations."
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#types-of-priors",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#types-of-priors",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "Types of Priors",
    "text": "Types of Priors\nThis fishing example used what we’d call a weakly informative prior – we had some knowledge but weren’t certain. Let’s formalize the different types of priors you might use:\nNon-informative (flat) priors:\n\nDeliberately vague, letting data dominate\nExample: Uniform(0, 1) for a coin’s probability of heads when you have no prior information\nCan seem “objective” but often encode strong assumptions on transformed scales\nMay lead to improper posteriors or numerical instability\n\nWeakly informative priors:\n\nStill wide, but rule out absurd values\nHelp stabilize estimation, especially with small samples\nExample: Normal(0, 10) for a standardized regression slope – expecting effects around zero but allowing substantial deviations. In original units, this might translate to “a 1-year increase in education increases log-income by somewhere between -20 and +20, with values near 0 most plausible.”\nOften the best default choice (Gelman, 2006; Gelman et al., 2017)\n\nInformative priors:\n\nBased on previous research or expert knowledge\nCan substantially improve inference when justified\nExample: Beta(30, 70) for disease prevalence when meta-analysis of previous studies found approximately 30% prevalence rates\nRequire careful justification and sensitivity analysis\n\n\n\n\n\n\n\n🐰 Wait, What’s a Normal(0, 10)? A Quick Field Guide Through The Burrows 🗺\n\n\n\n\n\n\nCommon Distribution Families\nWhen we specify priors, we’re choosing probability distributions that describe our beliefs about parameter values before seeing data. Here are the key concepts:\nUniform(a, b) or \\(\\theta \\sim \\text{Uniform}(a, b)\\): Every value between \\(a\\) and \\(b\\) is equally likely. Example: \\(\\text{Uniform}(0, 1)\\) means we believe a probability parameter could be anywhere from 0 to 1 with equal plausibility. This is “flat” – no value is preferred over another.\nNormal(μ, σ) or \\(\\theta \\sim \\mathcal{N}(\\mu, \\sigma^2)\\): The familiar bell curve, centered at mean \\(\\mu\\) with standard deviation \\(\\sigma\\). Note that in mathematical notation, the second parameter is often the variance \\(\\sigma^2\\), not the standard deviation. Example: \\(\\theta \\sim \\mathcal{N}(0, 100)\\) for a regression slope says we expect the effect to be around 0 with standard deviation \\(\\sigma = 10\\), but values within roughly \\(-20\\) to \\(+20\\) are plausible.\nBeta(α, β) or \\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\): Restricted to values between 0 and 1, making it perfect for probabilities or proportions. The shape depends on \\(\\alpha\\) and \\(\\beta\\): \\(\\text{Beta}(1, 1)\\) is uniform, \\(\\text{Beta}(2, 2)\\) is slightly peaked in the middle, \\(\\text{Beta}(10, 10)\\) is strongly concentrated around 0.5.\nGamma(α, β) or \\(\\theta \\sim \\text{Gamma}(\\alpha, \\beta)\\): Restricted to positive values, often used for scale parameters or rates. Warning: different software uses different parameterizations (shape/rate vs. shape/scale), so always check documentation!\nBinomial(n, p) or \\(Y \\sim \\text{Binomial}(n, p)\\): Counts the number of successes in \\(n\\) independent trials, each with probability \\(p\\) of success. This is a discrete distribution with possible values 0, 1, 2, …, \\(n\\). Example: \\(\\text{Binomial}(10, 0.3)\\) represents flipping a weighted coin 10 times where each flip has a 30% chance of heads – you might observe anywhere from 0 to 10 heads, with 3 being most likely.\nPoisson(λ) or \\(\\theta \\sim \\text{Poisson}(\\lambda)\\): For count data (non-negative integers: 0, 1, 2, 3, …) with no upper limit. The parameter \\(\\lambda\\) (lambda) is both the mean and variance. Example: \\(\\text{Poisson}(5)\\) is centered around 5 counts, with most probability on values between 1 and 10. Commonly used for rare events or occurrences over time/space.\nWant to see what these distributions look like (and much more)? Check out this interactive distribution explorer.\n\n\nProbability Density vs. Probability Mass\nFor continuous parameters (like temperature, height, or regression slopes), we use probability density functions (PDFs). The density at a point doesn’t give you a probability directly – instead, probability comes from integrating the density over an interval. For example, with \\(\\text{Normal}(0, 1)\\), we can’t say “the probability \\(\\theta = 0\\) is \\(X\\)”, but we can say “the probability \\(\\theta\\) is between \\(-0.1\\) and \\(0.1\\) is \\(Y\\)”.\nFor discrete parameters (like counts or categories), we use probability mass functions (PMFs). Here, each specific value does have a probability. For example, with a \\(\\text{Poisson}(5)\\) distribution, we can say “the probability of observing exactly 3 events is 0.14”.\nIn Bayesian inference, most parameters are continuous, so we work with probability densities. When we say “the prior is \\(\\text{Normal}(0, 10)\\)”, we mean the prior density follows that normal distribution.\n\n\nWhat Makes a Prior “Informative”?\nThe key is how much the prior constrains possible parameter values:\n\nFlat/non-informative: \\(\\text{Normal}(0, 1000)\\) for a regression slope barely constrains anything – it says slopes from \\(-2000\\) to \\(+2000\\) are all plausible.\nWeakly informative: \\(\\text{Normal}(0, 10)\\) gently suggests the effect is moderate, ruling out absurdly large values while remaining open-minded.\nInformative: \\(\\text{Normal}(5, 1)\\) strongly expects the parameter to be near 5, with most probability mass between 3 and 7.\n\nThe “right” prior depends on your actual knowledge and the scale of your problem. A slope of 1000 might be absurd for predicting human height from weight, but perfectly reasonable for predicting income from education years."
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#implicit-assumptions-in-frequentist-methods",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#implicit-assumptions-in-frequentist-methods",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "Implicit Assumptions in Frequentist Methods",
    "text": "Implicit Assumptions in Frequentist Methods\nAn important realization: Frequentist methods also make implicit assumptions, they’re just less visible (Robert, 2007):\n\nChoosing which test to use (t-test vs. Mann-Whitney)\nSelecting significance levels (\\(\\alpha = 0.05\\) vs. \\(0.01\\))\nDeciding when to stop collecting data\nWhich variables to include in a model\nHow to handle outliers or missing data\n\nFor example, when you choose to use a t-test instead of a nonparametric test, you’re implicitly assuming normality – that’s an assumption about your data, just never stated as a probability distribution. When you select \\(\\alpha = 0.05\\) rather than \\(0.01\\), you’re making a judgment about the relative costs of Type I and Type II errors.\nBy making priors explicit, Bayesian analysis gives you control and transparency. You can examine whether your assumptions are reasonable, test sensitivity to different priors, and clearly communicate what you’re assuming. As Berger (2006) argues, explicit modeling of prior information often leads to more honest and reproducible science than pretending we can analyze data without any prior assumptions.\n\n\n\n\n\n\n🐰 But before we dive deeper, let’s address some frequent misunderstandings 🧭\n\n\n\n\n\nCommon Misconceptions About Bayesian Inference\n“Priors are subjective, so Bayesian inference is unscientific.”\nAll statistical methods involve choices (which test to use, what \\(\\alpha\\)-level, when to stop collecting data). Bayesian methods make these assumptions explicit and transparent. Moreover, with sufficient data, different reasonable priors converge to similar posteriors (Berger, 2006).\n“You need strong prior beliefs to use Bayesian methods.”\nNot at all. Weakly informative priors that gently constrain parameters to reasonable ranges often work best (Gelman et al., 2017). You can be quite uncertain in your prior while still gaining the benefits of the Bayesian framework.\n“Bayesian credible intervals are the same as confidence intervals.”\nThey often give numerically similar results but have fundamentally different interpretations. A credible interval directly states the probability the parameter lies within it; a confidence interval describes properties of the procedure across repeated samples (Morey et al., 2016).\n“The prior ‘overwhelms’ the data.”\nFor reasonable priors and moderate sample sizes, the data dominate. The prior matters most when data are sparse – which is exactly when incorporating external knowledge is most valuable."
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#the-scenario-testing-a-new-seed-batch",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#the-scenario-testing-a-new-seed-batch",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "The Scenario: Testing a New Seed Batch",
    "text": "The Scenario: Testing a New Seed Batch\nImagine you’re a seed supplier evaluating a new batch of Echinacea purpurea (Purple Coneflower) seeds from a different grower. You plant 20 seeds under controlled conditions and observe that 12 germinate successfully. What can you conclude about the true germination rate of this batch?\nUnlike flipping a coin, you’re not starting from complete ignorance. You have relevant prior knowledge:\n\nPublished studies report 70-85% germination for fresh Echinacea seeds under optimal conditions\nYour company’s historical data from other suppliers shows similar rates\nHowever, germination can vary by seed source, storage conditions, and growing season\nSeeds from new suppliers sometimes underperform until growing practices are optimized\n\nThis is a perfect scenario for Bayesian inference – you have genuine prior information to incorporate, but also meaningful uncertainty to resolve with data."
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#the-beta-binomial-model",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#the-beta-binomial-model",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "The Beta-Binomial Model",
    "text": "The Beta-Binomial Model\nThe natural Bayesian model for germination data is:\n\nPrior: \\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\) (germination probability)\nLikelihood: Number germinating \\(\\sim \\text{Binomial}(n, \\theta)\\)\nPosterior: \\(\\theta \\sim \\text{Beta}(\\alpha + \\text{germinated}, \\beta + \\text{failed})\\)\n\nThe Beta distribution is perfect here because:\n\nIt’s defined on \\([0,1]\\), matching the range of probabilities\nIt’s the conjugate prior for the Binomial – the posterior is also Beta, making calculations simple\nIt’s flexible, able to represent various beliefs from uniform (no information) to highly concentrated\n\nUnderstanding Beta parameters intuitively: You can think of the Beta prior \\(\\text{Beta}(\\alpha, \\beta)\\) as if you’d already observed pseudo-data from previous experiments. Specifically:\n\n\\(\\alpha - 1\\) = number of prior “germinations” you’ve seen\n\\(\\beta - 1\\) = number of prior “failures” you’ve seen\n\nFor example, \\(\\text{Beta}(15, 5)\\) is like having previously tested \\(14 + 4 = 18\\) seeds and observed 14 germinate (with a 78% success rate). The prior is just data you saw before. \\(\\text{Beta}(1, 1)\\) is like having tested 0 seeds – complete ignorance.\nThe beautiful part: When you observe new data, you simply add your actual observations to these pseudo-observations:\n\\[\n\\text{Beta}(\\alpha, \\beta) + \\text{Data}(k, n-k) = \\text{Beta}(\\alpha + k, \\beta + n - k)\n\\]\nwhere \\(n\\) = total trials, \\(k\\) = germinations, and \\(n-k\\) = failures\n\n\n\n\n\n\n🐰 Another Burrow to Explore: Conjugate Priors Explained 🔦\n\n\n\n\n\n\nWhat’s a Conjugate Prior?\nA conjugate prior is a prior distribution that, when combined with a particular likelihood, produces a posterior in the same family. For the Binomial likelihood, the Beta distribution is conjugate – meaning:\n\nBeta prior + Binomial data = Beta posterior (still a Beta!)\n\nThis is special because most combinations don’t work this way. Usually, prior × likelihood gives you some complicated function that’s hard to work with. But with conjugate pairs, the math stays clean.\n\n\nThe Simple Updating Rule\nHere’s the beautiful part. If your prior is \\(\\text{Beta}(\\alpha, \\beta)\\) and you observe \\(k\\) successes in \\(n\\) trials, your posterior is:\n\\[\n\\text{Posterior} = \\text{Beta}(\\alpha_{\\text{new}}, \\beta_{\\text{new}})\n\\]\nwhere:\n\\[\n\\begin{align}\n\\alpha_{\\text{new}} &= \\alpha + k \\quad \\text{(old } \\alpha \\text{ + successes observed)} \\\\\n\\beta_{\\text{new}} &= \\beta + (n-k) \\quad \\text{(old } \\beta \\text{ + failures observed)}\n\\end{align}\n\\]\nIn plain English: Take your starting \\(\\alpha\\) and add the number of seeds that germinated. Take your starting \\(\\beta\\) and add the number that failed. Done!\n\n\nWhy Does This Work? The Prior as Pseudo-Data\nHere’s the key insight: You can think of the Beta prior as if you’d already conducted some germination trials before your actual experiment.\nThe parameters \\(\\alpha\\) and \\(\\beta\\) translate to pseudo-observations like this:\n\n\\(\\alpha - 1\\) = number of “prior germinations”\n\\(\\beta - 1\\) = number of “prior failures”\n\nWhy the “minus 1”? It’s a mathematical quirk of how the Beta distribution is defined. Just subtract 1 from each parameter to convert to counts.\nExamples:\n\n\\(\\text{Beta}(1, 1)\\): That’s \\(1-1=0\\) prior germinations and \\(1-1=0\\) prior failures. You’re starting with a blank slate – no prior information.\n\\(\\text{Beta}(15, 5)\\): That’s \\(15-1=14\\) prior germinations and \\(5-1=4\\) prior failures. It’s as if you’d already tested 18 seeds and seen 14 germinate. This encodes a belief that germination rate is probably around 75-80%.\n\\(\\text{Beta}(40, 10)\\): That’s \\(40-1=39\\) prior germinations and \\(10-1=9\\) prior failures. This represents stronger prior knowledge (48 pseudo-observations) with similar 80% germination expectation, but with more certainty.\n\n\n\nBayesian Updating is Just Adding New Data to Old Data\nWhen you observe real data, you’re literally adding your new observations to your pseudo-observations.\nFull example:\nYou start with prior \\(\\text{Beta}(15, 5)\\) based on historical data. Converting to counts:\n\nPrior pseudo-germinations: \\(15 - 1 = 14\\)\nPrior pseudo-failures: \\(5 - 1 = 4\\)\n\nThen you actually test seeds and observe 12 germinations and 8 failures. Add them together:\n\nTotal germinations: \\(14 + 12 = 26\\)\nTotal failures: \\(4 + 8 = 12\\)\n\nNow convert back to Beta parameters (add 1 to each count):\n\nNew \\(\\alpha = 26 + 1 = 27\\)\nNew \\(\\beta = 12 + 1 = 13\\)\nPosterior: \\(\\text{Beta}(27, 13)\\)\n\nOr use the shortcut: Just add observed counts directly to the old parameters:\n\n\\(\\alpha_{\\text{new}} = 15 + 12 = 27\\)\n\\(\\beta_{\\text{new}} = 5 + 8 = 13\\)\n\nSame answer, less thinking about the “-1” and “+1”!\n\n\n\n\nLet’s visualize different prior beliefs:\n\n\nShow code\nlibrary(tidyverse)\n\n# Create sequence of probability values\ntheta &lt;- seq(0, 1, length.out = 200)\n\n# Define three different priors representing different states of knowledge\nprior_df &lt;- bind_rows(\n  # Uniform prior: no prior knowledge\n  tibble(\n    theta = theta,\n    density = dbeta(theta, 1, 1),\n    prior_type = \"Beta(1,1): Uniform\\n(No prior knowledge)\"\n  ),\n  # Weakly informative: general knowledge about Echinacea\n  tibble(\n    theta = theta,\n    density = dbeta(theta, 15, 5),\n    prior_type = \"Beta(15,5): Centered at 0.75\\n(Literature suggests 70-85%)\"\n  ),\n  # Informative prior: strong historical data from your company\n  tibble(\n    theta = theta,\n    density = dbeta(theta, 40, 10),\n    prior_type = \"Beta(40,10): Strong belief at 0.80\\n(Extensive company records)\"\n  )\n)\n\n# Plot the three priors\nggplot(prior_df, aes(x = theta, y = density, color = prior_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"#1b9e77\", \"#d95f02\", \"#7570b3\")) +\n  labs(\n    title = \"Different Prior Beliefs About Echinacea Germination Rate\",\n    subtitle = \"Beta distributions representing various states of prior knowledge\",\n    caption = \"Three different prior beliefs about germination rates for Purple Coneflower seeds.\",\n    x = \"Germination rate (θ)\",\n    y = \"Density\",\n    color = \"Prior Distribution\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"top\",\n        legend.text = element_text(size = 8),\n        legend.title = element_text(size = 9)\n        )"
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#bayesian-updating-in-action",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#bayesian-updating-in-action",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "Bayesian Updating in Action",
    "text": "Bayesian Updating in Action\nNow let’s see how these priors update when we observe 12 germinations out of 20 seeds:\n\n\nShow code\nlibrary(tidyverse)\n\n# Create sequence of probability values\ntheta &lt;- seq(0, 1, length.out = 200)\n\n# Our observed data\nn_germinated &lt;- 12\nn_failed &lt;- 8\n\n# Calculate prior and posterior densities for each prior\n# Prior 1: Uniform (Beta(1,1))\nprior1_prior &lt;- dbeta(theta, 1, 1)\nprior1_post &lt;- dbeta(theta, 1 + n_germinated, 1 + n_failed)\n\n# Prior 2: Literature-based (Beta(15,5))\nprior2_prior &lt;- dbeta(theta, 15, 5)\nprior2_post &lt;- dbeta(theta, 15 + n_germinated, 5 + n_failed)\n\n# Prior 3: Strong company data (Beta(40,10))\nprior3_prior &lt;- dbeta(theta, 40, 10)\nprior3_post &lt;- dbeta(theta, 40 + n_germinated, 10 + n_failed)\n\n# Combine all priors and posteriors into one dataframe\nupdating_df &lt;- bind_rows(\n  tibble(theta = theta, density = prior1_prior, distribution = \"Prior\", prior_type = \"No Prior Knowledge\"),\n  tibble(theta = theta, density = prior1_post, distribution = \"Posterior\", prior_type = \"No Prior Knowledge\"),\n  tibble(theta = theta, density = prior2_prior, distribution = \"Prior\", prior_type = \"Literature-Based Prior\"),\n  tibble(theta = theta, density = prior2_post, distribution = \"Posterior\", prior_type = \"Literature-Based Prior\"),\n  tibble(theta = theta, density = prior3_prior, distribution = \"Prior\", prior_type = \"Strong Company Data\"),\n  tibble(theta = theta, density = prior3_post, distribution = \"Posterior\", prior_type = \"Strong Company Data\")\n)\n\n# Plot priors and posteriors for comparison\nggplot(updating_df, aes(x = theta, y = density, \n                        color = distribution, linetype = distribution)) +\n  geom_line(linewidth = 1) +\n  facet_wrap(~ prior_type, ncol = 1) +\n  # Add vertical line at observed proportion (12/20 = 0.60)\n  geom_vline(xintercept = 12/20, linetype = \"dashed\", color = \"grey40\") +\n  scale_color_manual(values = c(\"Prior\" = \"#7570b3\", \"Posterior\" = \"#d95f02\")) +\n  labs(\n    title = \"Bayesian Updating: How Germination Data Changes Our Beliefs\",\n    subtitle = \"Data: 12 seeds germinated out of 20 | Dashed line shows observed rate (0.60)\",\n    caption = \"How different priors update with the same data.\",\n    color = \"Distribution\", \n    linetype = \"Distribution\",\n    x = \"Germination rate (θ)\",\n    y = \"Density\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nKey insights:\n\nAll posteriors shift toward the observed data (\\(12/20 = 0.60\\)), regardless of starting beliefs\nDifferent priors lead to different posteriors – your starting beliefs matter, especially with limited data\nStronger priors (more peaked) require more data to substantially shift, while weak priors let the data dominate quickly\nWith enough data, all reasonable priors eventually converge to similar posteriors\nThis illustrates a fundamental principle: Bayesian inference represents a compromise between prior beliefs and observed data\nThe weight given to each depends on their relative certainty – weak priors defer to data, strong data overwhelms priors"
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#credible-intervals-direct-probability-statements",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#credible-intervals-direct-probability-statements",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "Credible Intervals: Direct Probability Statements",
    "text": "Credible Intervals: Direct Probability Statements\nNow let’s quantify our uncertainty about the germination rate using the posterior distribution.\nUnlike frequentist confidence intervals, Bayesian credible intervals have a direct probability interpretation (Kruschke, 2014; Morey et al., 2016).\nA 95% credible interval is simply the range containing 95% of the posterior probability. We’ll calculate an equal-tailed interval, which places 2.5% of probability in each tail (there’s also a “highest posterior density” interval that finds the narrowest 95% region, but for symmetric posteriors like ours, they’re nearly identical).\nIn Bayesian inference, the posterior distribution is the final product – it fully describes our updated beliefs about the parameter after seeing the data. Any interval you report (e.g., 80%, 89%, 90%, or 95%) is just a way of summarizing that posterior. The chosen level is not dictated by the method; it’s a communication choice, not a fixed error-control convention.\nIn contrast, frequentist confidence intervals are tied to a pre-specified error rate (the \\(\\alpha\\)-level, such as 0.05 for 95% coverage). The level determines the long-run frequency properties of the procedure: if repeated infinitely, 95% of intervals constructed this way would contain the true value of \\(\\theta\\). Changing the level changes the frequentist procedure itself.\nTo see this difference concretely:\n\nBayesian 89%: “I’m reporting 89% of my posterior. I could have reported 80% or 95 – all are valid summaries of the same posterior distribution.”\nFrequentist 95%: “I chose \\(\\alpha = 0.05\\) to control long-run error rates. This determines the procedure’s coverage properties. But the coverage is about the procedure in general, not about this specific interval capturing this specific parameter.”\n\nAs McElreath (2020, p. 58) notes, “It is not easy to defend the choice of 95% (5%), outside of pleas of convention.” Gelman & Carlin (2014) and Kruschke (2014) make the same point: since the posterior fully represents uncertainty, the interval percentage is a matter of reporting style, not of statistical principle. So why not go with 89% throughout this series? It’s such a beautiful number :)\n\n\nShow code\nlibrary(tidyverse)\n\n# Create sequence of probability values\ntheta &lt;- seq(0, 1, length.out = 200)\n\n# Calculate posterior (using uniform prior)\nalpha_post &lt;- 1 + n_germinated  # 13\nbeta_post &lt;- 1 + n_failed       # 9\n\n# Calculate 89% credible interval (equal-tailed)\nlower &lt;- qbeta(0.055, alpha_post, beta_post)\nupper &lt;- qbeta(0.945, alpha_post, beta_post)\npost_mean &lt;- alpha_post / (alpha_post + beta_post)\n\n# Create data for plotting\nposterior_data &lt;- tibble(theta = theta, density = dbeta(theta, alpha_post, beta_post))\n# Extract only the data within the credible interval for shading\nci_data &lt;- posterior_data %&gt;% filter(theta &gt;= lower & theta &lt;= upper)\n\n# Plot posterior with shaded credible interval\nggplot(posterior_data, aes(x = theta, y = density)) +\n  geom_line(linewidth = 1.2, color = \"#d95f02\") +\n  # Shade the 89% credible interval\n  geom_area(data = ci_data, aes(x = theta, y = density), \n            fill = \"#d95f02\", alpha = 0.3) +\n  # Add posterior mean line\n  geom_vline(xintercept = post_mean, linetype = \"dashed\", color = \"grey40\") +\n  # Annotate with posterior mean\n  annotate(\"text\", x = post_mean, y = max(posterior_data$density) * 1.05,\n           label = sprintf(\"Posterior mean = %.3f\", post_mean),\n           hjust = 0.5, size = 4) +\n  # Annotate with credible interval bounds\n  annotate(\"text\", x = post_mean, y = max(posterior_data$density) * 0.3,\n           label = sprintf(\"89%% Credible Interval:\\n[%.3f, %.3f]\", lower, upper),\n           hjust = 0.5, size = 4, color = \"#d95f02\", fontface = \"bold\") +\n  labs(\n    title = \"89% Bayesian Credible Interval for Germination Rate\",\n    subtitle = \"The shaded region contains 89% of the posterior probability\",\n    x = \"Germination rate (θ)\",\n    y = \"Posterior density\",\n    caption = \"This is the interpretation most people intuitively expect from any interval estimate.\\nAnd there is no reason why it should be 95% other than convention.\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\nKey insights:\n\nThe crucial difference between confidence and credible intervals lies in their interpretation\nA confidence interval (frequentist) means “If we repeated this procedure infinitely, 95% of intervals would capture \\(\\theta\\)” – a statement about the procedure, not the parameter\nA credible interval (Bayesian) means “There is a 95% probability that \\(\\theta\\) lies in this interval” – a direct statement about the parameter itself\nFor this seed batch, we can say with 89% confidence: “The true germination rate is between 42% and 75%”\nThis is the statement seed suppliers and customers actually care about – it directly quantifies our uncertainty about this specific batch’s quality\nThe Bayesian statement is what most people think a confidence interval means, and in Bayesian inference, it actually does mean that\n\n\n\n\n\n\n\n🎉 Progress Check!\n\n\n\nYou’ve learned: - ✅ How priors encode beliefs - ✅ How Bayes’ theorem updates those beliefs\n- ✅ How to construct credible intervals - ✅ What makes 89% such a beautiful number\nStill to come: - Making predictions with uncertainty - Sequential learning without penalties\nYou’re over halfway there! The hard conceptual work is done—now we get to see the payoff.\n\nQuick breather - Bad Statistics Joke: Why did the Bayesian seed always germinate?\nBecause it had prior experience! 🌱\n(I’ll see myself out… but first, let’s talk about posterior predictions!) 😅"
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#posterior-predictions-what-happens-next",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#posterior-predictions-what-happens-next",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "Posterior Predictions: What Happens Next?",
    "text": "Posterior Predictions: What Happens Next?\nOne of Bayesian inference’s most powerful features is that we can use our posterior distribution to predict future observations, accounting for all our uncertainty (Gabry et al., 2019; Gelman et al., 1996). This is huge – instead of just estimating “the germination rate is probably around 59%,” we can directly answer questions like “If I plant 10 seeds from this batch in a customer’s garden, how many will germinate?”\nFor a seed supplier, this is far more useful than a point estimate. You need to:\n\nSet realistic customer expectations\nDecide whether to accept or reject the batch\nDetermine appropriate pricing\nEstimate how many seeds to include per packet\n\n\nThe Question\nWe tested 20 seeds and observed 12 germinations, giving us a posterior \\(\\text{Beta}(13, 9)\\) for the germination rate \\(\\theta\\). Now suppose a customer plants 10 seeds from this batch in their garden. How many should we expect to germinate?\n\n\nTwo Sources of Uncertainty\nA good prediction must account for:\n\nParameter uncertainty: We’re not completely sure what \\(\\theta\\) is (we have a posterior distribution, not a single value)\nSampling variability: Even if we knew \\(\\theta\\) exactly, germination is inherently variable – weather, soil conditions, planting depth, and random chance all matter. We wouldn’t get exactly \\(10 \\times \\theta\\) germinations even with perfect knowledge of \\(\\theta\\).\n\nThe posterior predictive distribution accounts for both by:\n\nDrawing a plausible \\(\\theta\\) value from our posterior\nSimulating seed germinations using that \\(\\theta\\)\nRepeating this many times to build up a distribution of predictions\n\nThink of it this way: Imagine asking “How many Echinacea seedlings will emerge in my garden?” rather than “What is the true germination rate of this batch?” The first question (prediction) naturally incorporates both your uncertainty about the batch quality and the randomness inherent in any particular planting.\nLet’s see this in action:\n\n\nShow code\nlibrary(tidyverse)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Calculate posterior (using uniform prior)\nalpha_post &lt;- 1 + n_germinated  # 13\nbeta_post &lt;- 1 + n_failed       # 9\n\n# Number of seeds customer will plant\nn_future &lt;- 10\nn_sims &lt;- 10000\n\n# Simulate from posterior predictive distribution\n# Step 1: Draw theta values from posterior Beta(13, 9)\ntheta_samples &lt;- rbeta(n_sims, alpha_post, beta_post)\n\n# Step 2: For each theta, simulate seed germinations\nfuture_germinations &lt;- rbinom(n_sims, size = n_future, prob = theta_samples)\n\n# Create dataframe for plotting\npredictive_df &lt;- tibble(future_germinations = future_germinations)\n\n# Calculate posterior mean prediction (for comparison)\nposterior_mean_theta &lt;- alpha_post / (alpha_post + beta_post)\npoint_prediction &lt;- n_future * posterior_mean_theta\n\n# Plot the posterior predictive distribution\nggplot(predictive_df, aes(x = future_germinations)) +\n  geom_bar(aes(y = after_stat(count) / sum(after_stat(count))),\n           fill = \"#d95f02\", alpha = 0.7) +\n  geom_vline(xintercept = point_prediction, \n             linetype = \"dashed\", color = \"#1b9e77\", linewidth = 1) +\n  annotate(\"text\", x = point_prediction + 1.6, y = 0.21,\n           label = sprintf(\"Point estimate:\\n%.1f germinations\", point_prediction),\n           color = \"#1b9e77\", size = 3.5) +\n  scale_x_continuous(breaks = 0:10) +\n  labs(\n    title = \"Predicting Germinations for a Customer's Planting\",\n    subtitle = sprintf(\"Based on posterior from 12 germinations in 20 seeds | Mean prediction: %.1f germinations\", \n                      mean(future_germinations)),\n    x = \"Number of germinations out of 10 seeds planted\",\n    y = \"Probability\",\n    caption = \"The distribution captures both parameter uncertainty and natural variation in germination.\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(plot.subtitle = element_text(size = 10))\n\n\n\n\n\n\n\n\n\n\n\nShow code\nlibrary(tidyverse)\nlibrary(knitr)\n\n# Summary statistics for the posterior predictive distribution\nsummary_stats &lt;- tibble(\n  Metric = c(\"Mean prediction\", \n             \"Most likely outcome\", \n             \"89% Prediction Interval\"),\n  Value = c(\n    sprintf(\"%.1f germinations\", mean(future_germinations)),\n    sprintf(\"%d germinations (%.1f%% probability)\",\n            as.numeric(names(sort(table(future_germinations), decreasing = TRUE)[1])),\n            100 * max(table(future_germinations)) / n_sims),\n    sprintf(\"[%d, %d] germinations\",\n            quantile(future_germinations, 0.055),\n            quantile(future_germinations, 0.945))\n  )\n)\n\nkable(summary_stats, \n      caption = \"Posterior predictive summary for 10 seeds from this batch.\",\n      align = c(\"l\", \"r\"))\n\n\n\nPosterior predictive summary for 10 seeds from this batch.\n\n\nMetric\nValue\n\n\n\n\nMean prediction\n5.9 germinations\n\n\nMost likely outcome\n6 germinations (20.5% probability)\n\n\n89% Prediction Interval\n[3, 9] germinations\n\n\n\n\n\n Key insights:\n\nThe distribution is wider than you might expect – this honest uncertainty reflects our uncertainty about \\(\\theta\\) plus natural variation in germination\nIt’s naturally discrete (whole numbers only), matching the reality that you can’t have fractional germinations\nWhile the mean is near our point estimate (\\(10 \\times 0.59 \\approx 5.9\\) germinations), the full distribution shows all plausible outcomes\nWe can say: “There’s an 89% chance a customer planting 10 seeds will see between 3 and 8 germinate”\n\nFor practical decision-making, we can say: “There’s an 89% chance a customer planting 10 seeds will see between 3 and 8 germinate.” This helps you:\n\nSet customer expectations: Don’t promise 80% germination when the data suggest 60%\nMake batch acceptance decisions: Is 3-8 germinations per 10 seeds acceptable?\nAdjust seed packet counts: Maybe include 15 seeds instead of 10 to ensure customers get enough plants\nDecide on further testing: The wide uncertainty (3-8 is a big range) suggests testing more seeds would be valuable\n\nThis also enables model checking: if you test another batch and get results far outside your predictive distribution, something’s wrong with your model assumptions (Gelman et al., 1996). Perhaps germination varies by seed lot more than you thought, or environmental factors matter more than your simple model assumes.\nThis same logic – priors, likelihoods, posteriors, and predictions – extends to any statistical model, from simple proportions to complex regression and beyond.\n\n\n\n\n\n\n🐰 Emerging from the Burrow: Posterior Predictions 🌅\n\n\n\n\n\n\nPosterior and Prior Predictive Distributions\nMathematically, the posterior predictive distribution is:\n\\[\nP(\\tilde{y} \\mid y_{\\text{obs}}) = \\int P(\\tilde{y} \\mid \\theta)\\, P(\\theta \\mid y_{\\text{obs}})\\, d\\theta\n\\]\nIn words: the probability of future (or new) data \\(\\tilde{y}\\) is the average of the likelihoods, weighted by how plausible each parameter value \\(\\theta\\) is under the posterior.\nFor each possible \\(\\theta\\):\n\nCompute how likely the future data are under that parameter: \\(P(\\tilde{y} \\mid \\theta)\\)\n\nWeight that likelihood by the posterior probability: \\(P(\\theta \\mid y_{\\text{obs}})\\)\n\nIntegrate over all possible \\(\\theta\\)\n\nThis expresses the predictive uncertainty that combines both parameter uncertainty and data variability.\n\n\nWhy We Simulate Instead of Integrate\nFor simple models (like the Beta-Binomial), the integral above has a closed form. But in most real problems, it doesn’t – so we simulate instead.\nPosterior Predictive Simulation Algorithm:\n\nDraw parameter samples \\(\\theta^{(i)} \\sim P(\\theta \\mid y_{\\text{obs}})\\)\n\nFor each draw, simulate future data \\(\\tilde{y}^{(i)} \\sim P(\\tilde{y} \\mid \\theta^{(i)})\\)\n\nRepeat many times\n\nThe collection \\(\\{\\tilde{y}^{(1)}, \\tilde{y}^{(2)}, \\ldots\\}\\) approximates \\(P(\\tilde{y} \\mid y_{\\text{obs}})\\)\n\nThis process is called ancestral sampling because we first sample “ancestors” (parameters) and then simulate “descendants” (data). It works for any Bayesian model, no matter how complex.\n\n\nPrior Predictive Checks: Simulating Before Observing Data\nBefore seeing data, we can simulate from the prior predictive distribution:\n\\[\nP(\\tilde{y}) = \\int P(\\tilde{y} \\mid \\theta)\\, P(\\theta)\\, d\\theta\n\\]\nThis answers:\n\n“If my prior beliefs were true, what kinds of data would I expect to see?”\n\nIf your prior predictive simulations yield implausible outcomes (e.g., negative germination rates or rates greater than 100%), that’s a clear sign your prior needs revision (Gabry et al., 2019). We’ll use this technique extensively in the next post when working with regression models."
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#demonstration-sequential-germination-testing",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#demonstration-sequential-germination-testing",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "Demonstration: Sequential Germination Testing",
    "text": "Demonstration: Sequential Germination Testing\nImagine you’re evaluating a large shipment of Echinacea seeds. Rather than testing all at once, you test them in batches of 10 seeds as time and resources permit. Let’s watch how your beliefs evolve:\n\n\nShow code\nlibrary(tidyverse)\nlibrary(knitr)\n\nset.seed(456)\n\n# Simulate testing 100 seeds total from a batch with 60% germination rate\nn_seeds &lt;- 100\ntrue_germ_rate &lt;- 0.60\ngerminations &lt;- rbinom(n_seeds, 1, true_germ_rate)\n\n# Start with a weakly informative prior based on literature: Beta(15,5)\n# This represents belief that Echinacea typically germinates around 75%\nalpha &lt;- 15  # prior \"germinations\"\nbeta &lt;- 5    # prior \"failures\"\n\n# Create a dataframe to track how our beliefs evolve\nposterior_evolution &lt;- tibble(\n  seeds_tested = 0:n_seeds,\n  alpha_param = numeric(n_seeds + 1),\n  beta_param = numeric(n_seeds + 1),\n  mean = numeric(n_seeds + 1),\n  lower = numeric(n_seeds + 1),\n  upper = numeric(n_seeds + 1)\n)\n\n# Record initial prior\nposterior_evolution$alpha_param[1] &lt;- alpha\nposterior_evolution$beta_param[1] &lt;- beta\nposterior_evolution$mean[1] &lt;- alpha / (alpha + beta)\nposterior_evolution$lower[1] &lt;- qbeta(0.055, alpha, beta)\nposterior_evolution$upper[1] &lt;- qbeta(0.945, alpha, beta)\n\n# Update beliefs after each seed test\n# This is the sequential update formula: α_new = α_old + germinated, β_new = β_old + failed\nfor (i in 1:n_seeds) {\n  if (germinations[i] == 1) {\n    alpha &lt;- alpha + 1  # observed a germination\n  } else {\n    beta &lt;- beta + 1    # observed a failure\n  }\n  \n  # Record the updated posterior\n  posterior_evolution$alpha_param[i+1] &lt;- alpha\n  posterior_evolution$beta_param[i+1] &lt;- beta\n  posterior_evolution$mean[i+1] &lt;- alpha / (alpha + beta)\n  posterior_evolution$lower[i+1] &lt;- qbeta(0.055, alpha, beta)\n  posterior_evolution$upper[i+1] &lt;- qbeta(0.945, alpha, beta)\n}\n\n# Visualize the evolution of our beliefs\nggplot(posterior_evolution, aes(x = seeds_tested)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper), fill = \"#d95f02\", alpha = 0.3) +\n  geom_line(aes(y = mean), color = \"#d95f02\", linewidth = 1.2) +\n  geom_hline(yintercept = true_germ_rate, linetype = \"dashed\", color = \"grey40\") +\n  annotate(\"text\", x = 85, y = true_germ_rate + 0.05, \n           label = \"True germination rate (0.60)\", size = 4, color = \"grey40\") +\n  labs(\n    title = \"Sequential Bayesian Learning: No Penalty for Looking\",\n    subtitle = \"Posterior mean and 89% credible interval converge to truth as data accumulate\",\n    x = \"Number of seeds tested\",\n    y = \"Estimated germination rate (θ)\",\n    caption = \"Orange line = posterior mean | Shaded region = 89% credible interval\"\n  ) +\n  theme_minimal(base_size = 13)"
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#what-this-shows",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#what-this-shows",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "What This Shows",
    "text": "What This Shows\n\nStarting point matters initially: We began with a prior centered around 75% germination (based on literature), but as data accumulate, we learn the true rate is closer to 60%\nUncertainty shrinks: The credible interval (shaded region) narrows as we test more seeds\nBeliefs converge: Our estimate approaches the true germination rate as evidence accumulates\nNo stopping penalty: We could have stopped at seed 20, examined results, then continued – our final answer would be identical\nCoherent updates: Each seed test updates our beliefs in a mathematically principled way"
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#why-this-matters-for-seed-testing",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#why-this-matters-for-seed-testing",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "Why This Matters for Seed Testing",
    "text": "Why This Matters for Seed Testing\nThis property makes Bayesian methods particularly valuable for:\n\nBatch quality control: Test small samples continuously as shipments arrive\nAdaptive testing: Stop early if germination is clearly unacceptable (save time and resources)\nSeasonal monitoring: Update beliefs about supplier quality over multiple growing seasons\nDecision-making under uncertainty: Make accept/reject decisions as soon as you have sufficient evidence\n\nYou can make decisions based on evidence accumulated so far without worrying about invalidating your statistical inference. This is exactly how quality control works in practice – you don’t wait for a predetermined sample size if the batch is obviously failing."
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#making-decisions-along-the-way",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#making-decisions-along-the-way",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "Making Decisions Along the Way",
    "text": "Making Decisions Along the Way\nAs a seed supplier, you might have decision rules like:\n\nAfter 30 seeds: If mean germination &lt; 50%, reject the batch immediately\nAfter 50 seeds: If 89% credible interval entirely below 65%, reject the batch\nAfter 100 seeds: Make final accept/reject decision\n\nWith Bayesian inference, you can implement these rules without any statistical penalties. Your posterior after 100 seeds is valid regardless of how many times you peeked at the results."
  },
  {
    "objectID": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#the-frequentist-problem-optional-stopping",
    "href": "posts/2025-10-20-understanding-bayesian-inference-foundations/index.html#the-frequentist-problem-optional-stopping",
    "title": "Understanding Bayesian Inference: Foundations",
    "section": "The Frequentist Problem: Optional Stopping",
    "text": "The Frequentist Problem: Optional Stopping\nWhat would happen in frequentist inference?\nIn a frequentist framework, the validity of p-values depends on your sampling plan. If you peek at your data and decide whether to continue based on what you see, you inflate your Type I error rate (the probability of falsely rejecting a true null hypothesis).\n\nTwo Scenarios\n\nPre-planned sequential testing (e.g., “I will check after every 20 seeds.”):\n\nYou must adjust your significance level to control overall Type I error\nUse methods like Bonferroni correction: \\(\\alpha= 0.05/5 = 0.01\\) for 5 planned looks\nOr apply formal \\(\\alpha\\)-spending functions (used in clinical trials)\nThis “spends” your \\(\\alpha\\) budget across multiple looks\n\nOptional stopping (e.g., “I’ll keep testing until I see something interesting”):\n\nEven worse: your actual Type I error rate becomes unknown and is inflated\nYou’re giving yourself unlimited chances to reject the null by accident\nThe p-value assumes a fixed sample size and stopping rule – as defined in your power analysis – and changing these invalidates it\nThis is why “p-hacking” (testing until you find \\(p &lt; 0.05\\)) is problematic\n\n\nSuppose your company’s policy requires a minimum 70% germination rate for acceptable seed batches. In the frequentist framework, we test the null hypothesis \\(H_0 : \\theta = 0.70\\) against the alternative \\(H_1 : \\theta &lt; 0.70\\). However, because we’re checking the results multiple times (after every 20 seeds) over the course of testing 100 seeds, we face the multiple testing problem. Each time we look at the data and perform a test, we increase the chance of falsely rejecting an actually acceptable batch.\n\n\nThe Frequentist Dilemma\n\nWithout correction (\\(\\alpha = 0.05\\) at each look): You inflate your Type I error rate – you’re more likely to reject good batches by chance\nWith correction (Bonferroni \\(\\alpha = 0.01\\)): You lose power – it’s harder to detect truly poor batches\nMost importantly: The p-value’s validity depends on whether you planned these looks in advance and adjusted properly\nIf you decide to test more seeds after seeing results, your p-values become invalid\n\n\n\nThe Bayesian Advantage\nYour posterior after testing 100 seeds is identical whether you:\n\nTested all 100 seeds without looking at interim results\nChecked after every single seed\n\nStopped at 50 seeds, went on vacation, then tested 50 more\nDecided to continue based on disappointing early results\n\nThe math doesn’t care about your peeking behavior – only the data you actually observed. Your inference remains valid regardless of your stopping rule."
  }
]