[
  {
    "objectID": "about.html#stefan-schreiber-ph.d.-p.biol.",
    "href": "about.html#stefan-schreiber-ph.d.-p.biol.",
    "title": "Who we are",
    "section": "Stefan Schreiber, Ph.D., P.Biol.",
    "text": "Stefan Schreiber, Ph.D., P.Biol.\nStefan holds a Ph.D. in Forest Biology and Management from the University of Alberta and is a certified Professional Biologist with the Alberta Society of Professional Biologists. Prior to his time in Canada, Stefan pursued his Bachelor’s and Master’s degree in Biology at Ruhr-University Bochum, specializing in plant ecology and molecular phylogeny. His passion are trees, specifically understanding their adaptive mechanisms to diverse environments. In addition to his academic pursuits, Stefan sharpened his skills in quantitative methods and statistics, establishing a robust foundation in R programming throughout his career. Beyond his work life, Stefan cherishes every moment with his wife and three children. He also has a passion for ultra-endurance sports, embodying a resilient spirit both in his academic and work-related endeavors, as well as personal pursuits."
  },
  {
    "objectID": "about.html#sanja-schreiber-ph.d.-m.sc.-b.sc",
    "href": "about.html#sanja-schreiber-ph.d.-m.sc.-b.sc",
    "title": "Who we are",
    "section": "Sanja Schreiber, Ph.D., M.Sc., B.Sc",
    "text": "Sanja Schreiber, Ph.D., M.Sc., B.Sc\nSanja is a clinician with a Ph.D. in Rehabilitation Science and a Postdoctorate in Evidence Synthesis, both from University of Alberta. She has extensive expertise in clinical trial design and methodology, conduct, and reporting, with an interest in bridging gaps between research and practice. She is an expert in evidence synthesis methodologies, including scoping reviews, systematic reviews, Cochrane systematic reviews, overviews of reviews and living systematic reviews, where she uses software technology to expedite review production and minimize human error. Employing the same rigorous methodology utilized in medical research, she directs environmental evidence syntheses production at EnviroStats Solutions, facilitating evidence-based policymaking and addressing complex environmental decisions. Outside of work, Sanja loves travel adventures with her husband and three children."
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Quantitative Environmental Services",
    "section": "",
    "text": "At EnviroStats Solutions, our focus centers on applying the most suitable and current analyses and visualizations to environmental data. We are well-versed in both traditional statistical methods and Bayesian statistics. Our day-to-day modeling involves a range of techniques:\n\nLinear models\nGeneralized linear models\nLinear mixed effects models\nGeneralized linear mixed effects models\nModels accommodating zero and one inflated data\nClassification and regression tree analyses\nMultivariate analyses, inclusive principal component analysis, linear discriminant analysis, and non-linear multidimensional scaling techniques\nGenerating Shiny Web Apps for easy visualization of data\nEvidence syntheses through scoping and systematic reviews using narrative or meta-analyses\n\nIn addition to our analytical work, we have ample experience in offering tailored R teaching workshops. These workshops cater to your specific requirements, providing participants with fundamental and advanced training in R, statistics, and data visualization."
  },
  {
    "objectID": "services.html#our-services",
    "href": "services.html#our-services",
    "title": "Quantitative Environmental Services",
    "section": "",
    "text": "At EnviroStats Solutions, our focus centers on applying the most suitable and current analyses and visualizations to environmental data. We are well-versed in both traditional statistical methods and Bayesian statistics. Our day-to-day modeling involves a range of techniques:\n\nLinear models\nGeneralized linear models\nLinear mixed effects models\nGeneralized linear mixed effects models\nModels accommodating zero and one inflated data\nClassification and regression tree analyses\nMultivariate analyses, inclusive principal component analysis, linear discriminant analysis, and non-linear multidimensional scaling techniques\nGenerating Shiny Web Apps for easy visualization of data\nEvidence syntheses through scoping and systematic reviews using narrative or meta-analyses\n\nIn addition to our analytical work, we have ample experience in offering tailored R teaching workshops. These workshops cater to your specific requirements, providing participants with fundamental and advanced training in R, statistics, and data visualization."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#why-the-normal-distribution-matters",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#why-the-normal-distribution-matters",
    "title": "Understanding Frequentist Inference",
    "section": "Why the Normal Distribution Matters",
    "text": "Why the Normal Distribution Matters\nNotice how closely the empirical sampling distribution above matches the theoretical normal curve (red line). This is no coincidence – it’s a consequence of the Central Limit Theorem (CLT), which states that the sampling distribution of the mean tends toward a normal distribution as sample size increases, even if the underlying data aren’t normally distributed, as long as the observations are independent and identically distributed (Casella & Berger, 2002).\nThis distribution, also known as the Gaussian distribution after Carl Friedrich Gauss who studied it extensively in the early 19th century (though it was discovered earlier by de Moivre (De Moivre, 1738; Stigler, 1986)), has remarkable properties that make it central to statistical inference.\nThis normality of the sampling distribution is fundamental because:\n\nIt’s mathematically well-defined: The normal distribution is symmetric with known properties, allowing us to calculate precise probabilities.\nIt enables p-value calculations: We can determine exactly how extreme our observed statistic is under the null hypothesis.\nIt justifies confidence intervals: We know what multiplier to use (from the t-distribution) to achieve desired coverage probabilities.\nIt provides known error rates: Our Type I error rate (\\(\\alpha\\)) is controlled at exactly the level we specify.\n\nWhen assumptions fail: If the \\(i.i.d.\\) assumptions are violated – such as with strong dependence between observations, severe skewness combined with small sample sizes, or non-identical distributions – the sampling distribution may not follow this normal shape. In such cases, our p-values, confidence intervals, and error rates become unreliable. This is why checking assumptions and considering robust or alternative methods is crucial when working with real data.\nNow that we understand that the sampling distribution is approximately normal (thanks to the CLT), we can leverage this mathematical property to make two types of inferential statements. First, we can test specific hypotheses about parameter values. Second, we can construct intervals that quantify our uncertainty. Let’s examine hypothesis testing first."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#how-confidence-intervals-are-constructed",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#how-confidence-intervals-are-constructed",
    "title": "Understanding Frequentist Inference",
    "section": "How Confidence Intervals Are Constructed",
    "text": "How Confidence Intervals Are Constructed\nRecall that the standard error (SE) measures the expected variability of the sample mean across repeated samples. We calculate it as \\(\\text{SE} = s/\\sqrt{n}\\), where \\(s\\) is the sample standard deviation and \\(n\\) is the sample size.\nTo build a confidence interval, we take our sample mean and add/subtract a margin of error:\n\\[\\text{CI} = \\bar{x} \\pm (\\text{critical value} \\times \\text{SE})\\]\nThe critical value is a multiplier that determines how wide our interval should be. Think of it as answering the question: “How many standard errors away from our estimate should we go to capture the true parameter with 95% confidence?”\nBut here’s where a subtle complication arises…"
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#from-theory-to-practice-why-we-need-the-t-distribution",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#from-theory-to-practice-why-we-need-the-t-distribution",
    "title": "Understanding Frequentist Inference",
    "section": "From Theory to Practice: Why We Need the t-Distribution",
    "text": "From Theory to Practice: Why We Need the t-Distribution\nSo far, we’ve been using the formula \\(\\text{SE} = s/\\sqrt{n}\\) as if it were straightforward. But there’s a subtle issue here: we’re using the sample standard deviation \\(s\\) to estimate the population standard deviation \\(\\sigma\\). This creates a problem.\nWhen we simulate the sampling distribution (as we did earlier), we know the true population parameters. But in real data analysis, we never know \\(\\sigma\\) – we can only estimate it from our sample. This estimation adds an extra source of uncertainty that the normal distribution doesn’t account for.\nEnter the t-distribution. William Sealy Gosset (writing under the pseudonym “Student”) discovered that when we replace \\(\\sigma\\) with \\(s\\), the sampling distribution is no longer exactly normal – it follows what we now call Student’s t-distribution (Student, 1908). The key features are:\n\nIt has heavier tails than the normal distribution (more probability in the extremes).\nIt’s wider than the normal distribution, reflecting our additional uncertainty.\nAs sample size increases, it converges to the normal distribution.\nIt has a parameter called “degrees of freedom” (\\(df = n - 1\\)), which controls how wide it is.\n\nFor small samples, using the normal distribution would give us confidence intervals that are too narrow – they’d capture the true parameter less than 95% of the time. The t-distribution corrects for this, giving us appropriately wider intervals that maintain the correct coverage probability.\nLet’s see this difference visually:\n\n\nShow code\nlibrary(tidyverse)\n\n# Generate data comparing normal and t-distributions\nx &lt;- seq(-4, 4, length.out = 500)\ndf_compare &lt;- tibble(\n  x = x,\n  Normal = dnorm(x),\n  `t (df=5)` = dt(x, df = 5),      # Small sample: heavier tails\n  `t (df=30)` = dt(x, df = 30)     # Larger sample: approaches normal\n) |&gt; \n  pivot_longer(cols = -x, names_to = \"distribution\", values_to = \"density\")\n\n# Plot comparison\nggplot(df_compare, aes(x = x, y = density, color = distribution)) +\n  geom_line(linewidth = 1) +\n  labs(\n    title = \"Comparing Normal and t-Distributions\",\n    subtitle = \"The t-distribution has heavier tails, especially with small sample sizes\",\n    x = \"Value\",\n    y = \"Density\",\n    color = \"Distribution\"\n  ) +\n  scale_color_manual(values = c(\"Normal\" = \"black\", \"t (df=5)\" = \"#d95f02\", \"t (df=30)\" = \"#1b9e77\")) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nNotice how the t-distribution with 5 degrees of freedom (orange) has much heavier tails than the normal distribution (black). With 30 degrees of freedom (green), it’s already quite close to normal. This is why the t-distribution matters most for small samples."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#what-a-confidence-interval-really-means",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#what-a-confidence-interval-really-means",
    "title": "Understanding Frequentist Inference",
    "section": "What a Confidence Interval Really Means",
    "text": "What a Confidence Interval Really Means\nNow that we understand why we use the t-distribution instead of the normal distribution, let’s clarify what our resulting confidence intervals actually mean – and more importantly, what they don’t mean.\nA 95% confidence interval does not mean there is a 95% probability that the true mean lies within a particular observed interval. In the frequentist framework, the parameter is fixed (though unknown), so it either is or isn’t in any particular interval – there’s no probability involved once we’ve observed our data.\nInstead, the 95% refers to the procedure: if we repeated the entire sampling and interval-construction process infinitely many times, about 95% of those intervals would contain the true population mean. Our single observed interval is just one realization from this process.\nNow let’s see this in action with a simulation:\n\n\nShow code\nlibrary(tidyverse)\n\n# Function to simulate one sample and compute its 95% CI\nsimulation &lt;- function(n, mu, stdev) {\n  s &lt;- rnorm(n, mu, stdev)\n  sample_mean &lt;- mean(s)\n  sample_sd &lt;- sd(s)\n  sample_se &lt;- sample_sd / sqrt(n)\n  confint_95 &lt;- qt(0.975, df = n - 1) * sample_se  # t-critical value × SE\n  tibble(\n    n = n,\n    sample_mean = sample_mean,\n    sample_sd = sample_sd,\n    sample_se = sample_se,\n    confint_95 = confint_95,\n    lower_bound = sample_mean - confint_95,\n    upper_bound = sample_mean + confint_95,\n    contains_mu = mu &gt;= sample_mean - confint_95 & mu &lt;= sample_mean + confint_95\n  )\n}\n\n# Set parameters and run simulation\nset.seed(777)\nn &lt;- 10; mu &lt;- 0; stdev &lt;- 5\nsim &lt;- 100\n\n# Generate 100 confidence intervals\ndraws &lt;- map(1:sim, ~ simulation(n, mu, stdev)) |&gt;  \n  bind_rows() |&gt;  \n  mutate(experiment_id = 1:sim)\n\n# Calculate observed coverage rate\ncoverage &lt;- mean(draws$contains_mu)\n\n# Visualize confidence intervals\nggplot(draws, aes(x = sample_mean, y = experiment_id, color = contains_mu)) + \n  geom_point(size = 3) + \n  geom_errorbarh(aes(xmin = lower_bound, xmax = upper_bound), linewidth = 1, alpha = 0.5) + \n  geom_vline(xintercept = mu, linetype = \"dashed\", color = \"grey40\") +\n  scale_color_manual(\n    name = \"95% Confidence Interval\",\n    values = c(\"TRUE\" = \"#1b9e77\", \"FALSE\" = \"#d95f02\"),\n    labels = c(\"Captures true mean\", \"Misses true mean\")\n  ) +\n  labs(\n    title = \"95% Confidence Intervals Across 100 Simulated Experiments\",\n    subtitle = sprintf(\"Teal = captures μ, Orange = misses μ | Observed coverage: %.0f%%\", \n                      coverage * 100),\n    x = \"Sample mean\",\n    y = \"Experiment ID\",\n    caption = \"Dashed line marks the true population mean (μ = 0)\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nThe practical challenge: While this frequentist interpretation is logically sound, it doesn’t directly answer the question most researchers want answered: “What can I say about my single observed interval?” This tension between the procedure’s long-run properties and what we can infer from one dataset is an inherent limitation of the frequentist approach."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#interpreting-the-plots",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#interpreting-the-plots",
    "title": "Understanding Frequentist Inference",
    "section": "Interpreting the Plots",
    "text": "Interpreting the Plots\nTop panel (\\(n = 30\\) - Underpowered):\n\nThe orange curve shows the sampling distribution if \\(H_0: \\mu = 100\\) is true.\nThe green curve shows the sampling distribution if \\(H_1: \\mu = 103\\) is true.\nStandard error is \\(\\text{SE} \\approx 1.83\\), creating relatively wide, overlapping distributions.\nThe orange shaded regions (in the tails under \\(H_0\\)) show Type I error rate \\(\\alpha = 0.05\\).\nThe teal shaded region (under \\(H_1\\), within the non-rejection region) shows Type II error rate \\(\\beta \\approx 0.63\\) - meaning if the true mean is 103, we’d fail to detect this effect 63% of the time.\nThe purple shaded regions (under \\(H_1\\), in the rejection regions) show power \\(\\approx 0.37\\) - only a 37% chance of correctly detecting the true effect.\n\nBottom panel (\\(n = 200\\) - Overpowered):\n\nSame hypotheses, but now \\(\\text{SE} \\approx 0.71\\) (much smaller due to very large sample size).\nThe distributions are extremely narrow with almost no overlap.\nThe teal region (\\(\\beta\\)) is now virtually invisible, with \\(\\beta \\approx 0.01\\) – we’d almost never miss the true effect.\nThe purple regions (power) dominate the \\(H_1\\) distribution, with power \\(\\approx 0.99\\) – nearly 100% chance of correctly detecting the true effect.\n\nKey observations:\nThe comparison reveals an important trade-off in study design. This tension between statistical and practical significance is particularly problematic in fields where underpowered studies are common (Button et al., 2013), leading to inflated effect size estimates and poor replication rates:\n\nToo small (\\(n = 30\\)): High risk of missing real effects, wasting research effort and potentially leading to false conclusions.\nToo large (\\(n = 200\\)): Excessive power means collecting far more data than necessary, wasting time, money, and resources.\n\nWhile the underpowered study clearly has problems, the overpowered study also raises concerns. Beyond a certain point, additional data provides diminishing returns – power increases very little while costs continue to mount. Moreover, with very large samples, even trivially small effects become “statistically significant,” which doesn’t necessarily mean they’re practically meaningful.\nThe Goldilocks principle of sample size: We need a sample size that’s “just right” – large enough to reliably detect effects that matter, but not so large that we waste resources or detect effects too small to care about. This is why prospective power analysis is essential for efficient research design."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#planning-sample-size-through-prospective-power-analysis",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#planning-sample-size-through-prospective-power-analysis",
    "title": "Understanding Frequentist Inference",
    "section": "Planning Sample Size Through Prospective Power Analysis",
    "text": "Planning Sample Size Through Prospective Power Analysis\nThe side-by-side comparison above revealed a critical challenge: too little power (\\(n = 30\\)) risks missing real effects, while excessive power (\\(n = 200\\)) wastes resources and may detect effects too trivial to matter. This raises a practical question: rather than guessing what constitutes an adequate sample size, how can we systematically determine the optimal n for our specific goals?\nThe answer lies in prospective power analysis – plotting the relationship between sample size and power to find the sweet spot:\n\n\nShow code\nlibrary(tidyverse)\n\n# Define parameters (matching previous visualization)\nmu_null &lt;- 100\nmu_alt &lt;- 103\nsd_pop &lt;- 10\nalpha &lt;- 0.05\n\n# Function to calculate statistical power for a given sample size\ncalculate_power &lt;- function(n, mu_null, mu_alt, sd_pop, alpha = 0.05) {\n  se &lt;- sd_pop / sqrt(n)\n  t_crit &lt;- qt(1 - alpha / 2, df = n - 1)\n  lower_crit &lt;- mu_null - t_crit * se\n  upper_crit &lt;- mu_null + t_crit * se\n  \n  # Power = probability of rejecting H0 when H1 is true\n  power &lt;- pnorm(lower_crit, mean = mu_alt, sd = se) + \n           (1 - pnorm(upper_crit, mean = mu_alt, sd = se))\n  return(power)\n}\n\n# Calculate power across range of sample sizes\nsample_sizes &lt;- seq(10, 200, by = 5)\npower_values &lt;- map_dbl(sample_sizes, ~calculate_power(.x, mu_null, mu_alt, sd_pop))\npower_df &lt;- tibble(n = sample_sizes, power = power_values)\n\n# Find sample size needed for conventional 80% power\nn_for_80_power &lt;- sample_sizes[which.min(abs(power_values - 0.80))]\n\n# Calculate power at our example sample sizes\npower_at_n30 &lt;- calculate_power(30, mu_null, mu_alt, sd_pop)\npower_at_n200 &lt;- calculate_power(200, mu_null, mu_alt, sd_pop)\n\n# Plot power curve\nggplot(power_df, aes(x = n, y = power)) +\n  geom_line(color = \"#1b9e77\", linewidth = 1.2) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"grey40\") +\n  annotate(\"point\", x = 30, y = power_at_n30, color = \"#d95f02\", size = 4) +\n  annotate(\"point\", x = 200, y = power_at_n200, color = \"#d95f02\", size = 4) +\n  annotate(\"point\", x = n_for_80_power, y = 0.80, color = \"#7570b3\", size = 4) +\n  annotate(\"text\", x = 30, y = power_at_n30 - 0.08, \n           label = sprintf(\"n=30\\nPower≈%.0f%%\", power_at_n30*100), \n           color = \"#d95f02\", size = 3.5, fontface = \"bold\") +\n  annotate(\"text\", x = 200, y = power_at_n200 - 0.08, \n            label = sprintf(\"n=200\\nPower≈%.0f%%\", power_at_n200*100), \n            color = \"#d95f02\", size = 3.5, fontface = \"bold\") +\n  annotate(\"text\", x = n_for_80_power, y = 0.88, \n           label = sprintf(\"n≈%d needed\\nfor 80%% power\", n_for_80_power), \n           color = \"#7570b3\", size = 3.5, fontface = \"bold\") +\n  scale_x_continuous(limits = c(0, 210)) +\n  labs(\n    title = \"Statistical Power as a Function of Sample Size\",\n    subtitle = expression(paste(\"Effect size: \", mu[1], \" - \", mu[0], \" = 3 (Cohen's d ≈ 0.3)\")),\n    x = \"Sample size (n)\",\n    y = expression(paste(\"Power (1 - \", beta, \")\")),\n    caption = \"Orange points = our examples; Purple point = sample size for 80% power\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\nKey insights:\n\nAt \\(n = 30\\), power is only about 38% – inadequate for reliable detection.\nAt \\(n = 200\\), power is nearly 100% – far more than needed, representing wasted resources.\nTo achieve conventional 80% power for this effect size, we’d need approximately \\(n \\approx 85\\).\nLarger sample sizes increase power by decreasing the standard error (\\(\\text{SE} = s/\\sqrt{n}\\)). Smaller SE means narrower. sampling distributions, which creates better separation between the null and alternative distributions, reducing overlap and making true effects easier to detect.\nThis is why prospective power analysis is crucial for study design – it helps us determine the sample size needed to detect effects of meaningful magnitude with adequate probability (Cohen, 1988).\n\nPractical lesson: The error and power visualization showed us the problem (low power with \\(n = 30\\)), and this plot shows us the solution (increase sample size to approximately 85). This demonstrates why researchers should conduct power analyses before collecting data, not after!\nNow that we understand how to plan studies and interpret individual tests, we need to confront a more subtle challenge: what happens when we conduct not just one test, but many? And beyond statistical significance, how do we assess whether effects actually matter in practice?"
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#multiple-testing-inflates-false-positives",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#multiple-testing-inflates-false-positives",
    "title": "Understanding Frequentist Inference",
    "section": "Multiple Testing Inflates False Positives",
    "text": "Multiple Testing Inflates False Positives\nImportant distinction: This problem applies to multiple tests on the same data, not to independent replication studies. If different researchers independently test the same hypothesis on different datasets, those p-values shouldn’t be corrected for multiple testing – that’s replication, which is exactly what science needs!\nWhen conducting multiple hypothesis tests within the same study or dataset, a critical issue emerges: false positives accumulate. If you conduct 100 tests on the same dataset where all null hypotheses are true at \\(\\alpha = 0.05\\), you expect about 5 false positives purely by chance.\nExamples where correction is needed:\n\nTesting 20 different biomarkers to see which predict disease.\nComparing 5 treatment groups (resulting in 10 pairwise comparisons).\nTesting the same intervention across 15 different outcomes.\nExploratory analysis where you examine many potential relationships.\n\nWhy this matters:\n\nWithout correction, the probability of getting at least one false positive increases dramatically.\nWith 20 independent tests at \\(\\alpha = 0.05\\), there’s a 64% chance of at least one false positive even if all nulls are true (\\(1 - (1-0.05)^{20} = 1 - 0.95^{20} \\approx 0.64\\)).\nThis is why multiple testing corrections (Bonferroni, FDR control (Benjamini & Hochberg, 1995), Holm-Bonferroni) are essential.\n\nPractical implication: When you conduct multiple tests, either:\n\nApply appropriate corrections (and accept reduced power).\nClearly distinguish exploratory vs. confirmatory analyses.\nPre-register your primary hypothesis to avoid the “multiple testing” critique.\n\nThe “replication crisis” in many sciences is partly attributable to (Ioannidis, 2005):\n\nRunning many tests but only reporting the significant ones.\nNot correcting for multiple comparisons.\nTreating exploratory findings as if they were confirmatory.\n\nA single \\(p &lt; 0.05\\) result should never be considered definitive evidence. Replication, effect sizes, and consideration of prior plausibility are all crucial."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#statistical-vs.-practical-significance",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#statistical-vs.-practical-significance",
    "title": "Understanding Frequentist Inference",
    "section": "Statistical vs. Practical Significance",
    "text": "Statistical vs. Practical Significance\nAnother crucial distinction: statistical significance is not the same as practical importance. With a large enough sample size, even tiny, meaningless differences will yield \\(p &lt; 0.05\\). For example:\n\n\nShow code\nlibrary(tidyverse)\nlibrary(knitr)\n\n# Simulate a statistically significant but practically meaningless result\nset.seed(999)\nn_large &lt;- 10000\nsmall_effect &lt;- 0.05  # Negligible effect: 0.05 SD difference\n\n# Generate two groups with trivial difference\ngroup1 &lt;- rnorm(n_large, mean = 0, sd = 1)\ngroup2 &lt;- rnorm(n_large, mean = small_effect, sd = 1)\n\n# Conduct t-test\nt_result &lt;- t.test(group1, group2)\n\n# Calculate effect size\ncohens_d &lt;- small_effect / 1\n\n# Summarize results\nresults_df &lt;- tibble(\n  Metric = c(\"Sample size per group\", \"Cohen's d (effect size)\", \"p-value\", \"Mean difference\"),\n  Value = c(\n    format(n_large, big.mark = \",\"),\n    sprintf(\"%.3f\", cohens_d),\n    sprintf(\"%.6f\", t_result$p.value),\n    sprintf(\"%.3f\", diff(t_result$estimate))\n  )\n)\n\nkable(results_df, align = c(\"l\", \"r\"))\n\n\n\n\n\nMetric\nValue\n\n\n\n\nSample size per group\n10,000\n\n\nCohen’s d (effect size)\n0.050\n\n\np-value\n0.000049\n\n\nMean difference\n0.058\n\n\n\n\n\nThis hypothetical example shows that with 10,000 observations per group, we can detect an effect size of only 0.05 standard deviations with high confidence – but such a small effect is likely meaningless in most practical contexts.\nKey principle: Always report and interpret effect sizes alongside p-values (Cohen, 1994). Ask not just “Is there an effect?” but “How large is the effect, and does that matter?”\nThis tension between statistical and practical significance naturally raises another question: under what conditions can we trust these inferential procedures? Throughout this post, we’ve repeatedly invoked assumptions like independence and normality. It’s time to examine what happens when these assumptions break down."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#when-is-frequentist-inference-robust",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#when-is-frequentist-inference-robust",
    "title": "Understanding Frequentist Inference",
    "section": "When is frequentist inference robust?",
    "text": "When is frequentist inference robust?\n\nModerate departures from normality are often acceptable for means (due to Central Limit Theorem), especially with larger samples (\\(n &gt; 30\\) as a rough guideline).\nUnequal variances can be addressed with Welch’s \\(t\\)-test instead of Student’s \\(t\\)-test.\nMild dependence might inflate Type I error rates but not completely invalidate inference."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#when-do-we-need-alternative-approaches",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#when-do-we-need-alternative-approaches",
    "title": "Understanding Frequentist Inference",
    "section": "When do we need alternative approaches?",
    "text": "When do we need alternative approaches?\n\nStrong dependence (time series, clustered data) requires specialized methods like mixed models (Pinheiro & Bates, 2000) or generalized estimating equations (Liang & Zeger, 1986).\nSevere non-normality with small samples might require non-parametric tests, or better yet, generalized linear models (GLMs) which can directly model non-normal response distributions without transformation (Bolker et al., 2009).\nHeteroscedasticity (unequal variances) with unequal group sizes can seriously distort inferences, but can be modeled explicitly using generalized least squares or variance structures in mixed models (Pinheiro & Bates, 2000).\n\nPractical advice: Always examine your data visually, check diagnostic plots, and consider whether your assumptions are reasonable. Statistical methods are tools that work best when their assumptions are at least approximately met."
  },
  {
    "objectID": "posts/2025-10-12-understanding-frequentist-inference/index.html#key-takeaways",
    "href": "posts/2025-10-12-understanding-frequentist-inference/index.html#key-takeaways",
    "title": "Understanding Frequentist Inference",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nFrequentist inference is about procedures, not single-case probabilities.\nThe sampling distribution and standard error are the foundation – they quantify how much our estimates would vary across repeated samples.\nThe t-distribution accounts for the additional uncertainty when we estimate the population standard deviation from sample data.\nConfidence intervals and hypothesis tests are mathematically linked procedures using the same sampling distribution machinery.\nP-values measure compatibility with a specific null model, not truth or importance.\nEffect sizes matter as much as (or more than) p-values for scientific interpretation.\nSample size planning via power analysis is essential for efficient research – too little power wastes effort, too much power wastes resources.\nMultiple testing inflates false positive rates – correct for this when appropriate.\nAssumptions matter, but many methods are reasonably robust to moderate violations.\nReplication and cumulative evidence are more reliable than single p-values.\n\nAll these tools rely on the framework of hypothetical repeated sampling and assumptions about the data-generating process. They give us a powerful and well-understood framework for reasoning about populations from samples – but they must be interpreted carefully and in proper context.\nA natural question emerges: Is there an alternative framework that addresses some of these interpretational challenges – particularly the indirect nature of probability statements in the frequentist approach? This brings us to Bayesian inference."
  },
  {
    "objectID": "mission.html",
    "href": "mission.html",
    "title": "Quantitative Environmental Services",
    "section": "",
    "text": "Solid scientific inference and the foundations of evidence-based decision-making are rooted in data that precisely address the core questions and objectives at hand. This underscores the critical importance of meticulously planned study designs and targeted data collection. As the stakes rise, so does the demand for confidence. Building this confidence hinges on:\n\nThoughtful Experimental Design:\nCrafting the right experimental framework tailored to your inquiry.\nAccurate Data Analysis:\nApplying sound analytical techniques that draw out reliable insights.\nTransparent Visualization:\nConveying findings truthfully and effectively.\n\nEnviroStats Solutions stands ready to provide expert guidance and consultation in data analysis, visualization, and interpretation, catering to those navigating the landscape of environmental challenges. Backed by an extensive background in both applied and foundational research, we offer comprehensive support for evidence-driven decision-making — guiding you from initial experimental design to conclusive recommendations.\nOur commitment extends beyond consultation. We’ll share our proficiency in data analysis and environmental statistics through tailor-made workshops and training sessions that align with your specific requirements. With over a decade of hands-on experience as dedicated users of the R Environment for Statistical Computing, we’re dedicated to equipping you for success."
  },
  {
    "objectID": "mission.html#our-mission",
    "href": "mission.html#our-mission",
    "title": "Quantitative Environmental Services",
    "section": "",
    "text": "Solid scientific inference and the foundations of evidence-based decision-making are rooted in data that precisely address the core questions and objectives at hand. This underscores the critical importance of meticulously planned study designs and targeted data collection. As the stakes rise, so does the demand for confidence. Building this confidence hinges on:\n\nThoughtful Experimental Design:\nCrafting the right experimental framework tailored to your inquiry.\nAccurate Data Analysis:\nApplying sound analytical techniques that draw out reliable insights.\nTransparent Visualization:\nConveying findings truthfully and effectively.\n\nEnviroStats Solutions stands ready to provide expert guidance and consultation in data analysis, visualization, and interpretation, catering to those navigating the landscape of environmental challenges. Backed by an extensive background in both applied and foundational research, we offer comprehensive support for evidence-driven decision-making — guiding you from initial experimental design to conclusive recommendations.\nOur commitment extends beyond consultation. We’ll share our proficiency in data analysis and environmental statistics through tailor-made workshops and training sessions that align with your specific requirements. With over a decade of hands-on experience as dedicated users of the R Environment for Statistical Computing, we’re dedicated to equipping you for success."
  },
  {
    "objectID": "posts/2024-04-10-evidence-synthesis/index.html",
    "href": "posts/2024-04-10-evidence-synthesis/index.html",
    "title": "The Power of Evidence Synthesis",
    "section": "",
    "text": "Implementation of research findings into practice is generally slow-paced. It is estimated that that it takes 17–20 years to get clinical innovations into practice (Morris, Wooding, and Grant 2011), whereas fewer than 50% of them ever make it into general usage (Bauer and Kirchner 2020). While primary studies are an important first step in generating evidence, knowledge synthesis (evidence synthesis) serve as a bridge that expedite this process, ensuring that valuable insights from primary research reach those who can benefit most from them.\nTake the medical field, for example. Here, evidence synthesis is not just a helpful tool; it’s a cornerstone of clinical decision-making. By systematically accessing, evaluating, and consolidating scientific information, evidence synthesis provides a recognized standard of rigor, objectivity, and transparency. It’s the compass guiding healthcare professionals toward the most effective treatments and interventions.\nInterestingly, while evidence synthesis has long been a staple in medical (https://www.cochrane.org/) and social sciences (https://www.campbellcollaboration.org), its adoption in other fields, like environmental sciences, has been surprisingly slow. Yet, recent discussions in journals such as Trends in Ecology & Evolution (Dicks, Walsh, and Sutherland 2014) and Conservation Biology (Kadykalo et al. 2021) highlight its potential in addressing complex questions surrounding policy-making and controversial decisions. By grounding such decisions in the most reliable evidence available, evidence synthesis becomes a vital tool in navigating the intricate landscape of environmental challenges.\nMaintaining the integrity of evidence synthesis requires clear standards and meticulous attention to detail. Every step of the process, from article search strategy and their selection criteria, to identifying potential biases in the evidence and scrutinizing the synthesis itself, must be conducted with precision. This meticulous process ensures trust and confidence in the findings, allowing end-users to rely on the synthesized evidence with certainty.\nIn the environmental sciences, two common forms of evidence synthesis exist:\n\nScoping reviews (evidence maps) and\nSystematic reviews\n\nScoping reviews are ideal for assessing the breadth of knowledge on a specific topic and identifying gaps in the literature. On the other hand, systematic reviews compile all existing evidence on a particular research question, offering a comprehensive overview of the available literature. Systematic reviews are considered at the top of hierarchy of evidence because they offer higher-quality evidence compared to expert opinions, observational or experimental studies (Figure 1).\n\n\n\n\n\n\nFigure 1: Hierarchy of evidence pyramid\n\n\n\nWhen primary studies exhibit considerable heterogeneity, a narrative synthesis of evidence provides a cohesive description that draws connections between findings. Conversely, when primary studies share similar design and conduct, a quantitative data synthesis like meta-analysis serves as a powerful tool for drawing robust conclusions.\nIn conclusion, evidence synthesis provides clarity in ever-growing research. Whether guiding medical practitioners toward optimal patient care or informing policymakers on environmental matters, its role in bridging the gap between research and practice cannot be overstated. By embracing evidence synthesis, researchers can avoid redundant studies, make better use of existing data, and ensure that decisions are based on the most reliable and comprehensive evidence available. This can help minimize waste in research by directing resources towards studies that are more likely to yield meaningful results and contribute significantly to the advancement of knowledge.\n\nPlease do not hesitate to reach out (%20info at envirostats dot ca) if you have any questions!\n\n\n\n\nReferences\n\nBauer, Mark S., and JoAnn Kirchner. 2020. “Implementation Science: What Is It and Why Should I Care?” Psychiatry Research, VSI:Implementation Science, 283 (January): 112376. https://doi.org/10.1016/j.psychres.2019.04.025.\n\n\nDicks, Lynn V., Jessica C. Walsh, and William J. Sutherland. 2014. “Organising Evidence for Environmental Management Decisions: A ‘4S’ Hierarchy.” Trends in Ecology & Evolution 29 (11): 607–13. https://doi.org/10.1016/j.tree.2014.09.004.\n\n\nKadykalo, Andrew N., Rachel T. Buxton, Peter Morrison, Christine M. Anderson, Holly Bickerton, Charles M. Francis, Adam C. Smith, and Lenore Fahrig. 2021. “Bridging Research and Practice in Conservation.” Conservation Biology 35 (6): 1725–37. https://doi.org/10.1111/cobi.13732.\n\n\nMorris, Zoë Slote, Steven Wooding, and Jonathan Grant. 2011. “The Answer Is 17 Years, What Is the Question: Understanding Time Lags in Translational Research.” Journal of the Royal Society of Medicine 104 (12): 510–20. https://doi.org/10.1258/jrsm.2011.110180."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Quantitative Environmental Services",
    "section": "Welcome!",
    "text": "Welcome!\nEnviroStats Solutions is a dynamic Edmonton-based consulting firm dedicated to delivering efficient and rigorous environmental statistics, data analysis, visualization, and interpretation services. Our focus revolves around the environmental domain, providing practical insights that drive meaningful decisions.\nBeyond our analytical services, we also offer professional training in statistics, data management, and visualization, all centered within the versatile R programming environment. These workshops are designed to empower you to confidently navigate the world of data.\nEager to know more about our journey and the impact we’re making? Don’t hesitate to reach out (%20info at envirostats dot ca)!"
  }
]