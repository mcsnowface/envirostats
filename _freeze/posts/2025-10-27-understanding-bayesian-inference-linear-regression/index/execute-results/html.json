{
  "hash": "90b232c50a2138f57583e010fb41f240",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Understanding Bayesian Inference: Linear Regression\"\nsubtitle: \"From Seed Germination to Tree Improvement to Business Decisions\"\nauthor: \"Stefan Schreiber\"\ndate: 10-27-2025\ncategories: [bayesian, inference, regression]\n\n# # Local image for the page\nimage: francis_galton.png\nimage-alt: \"Francis Galton\"\n\ncitation:\n  url: https://envirostats.ca/posts/2025-10-27-understanding-bayesian-inference-linear-regression/\n\nbibliography: references.bib\ncsl: apa.csl\n\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: true\n    code-summary: \"Show code\"\n    df-print: paged\n\neditor: source\n\nexecute:\n  echo: true\n  warning: false\n  message: false\n  cache: true\n\ndraft: false  \n---\n\n![When in doubt, just fit straight lines. ;P](francis_galton.png){width=\"50%\" fig-align=\"left\"}\n\n# Introduction\n\nIn our [previous post](https://www.envirostats.ca/posts/2025-10-20-understanding-bayesian-inference-foundations/), we explored the foundations of Bayesian inference through a simple example: estimating germination rates from seed trials. We learned how to specify priors, update beliefs with data through Bayes' theorem, and make probabilistic predictions. The key insight was treating parameters as random variables with probability distributions, allowing us to make direct statements like \"there's a 95% probability the germination rate is between 0.42 and 0.75.\"\n\nThat example involved estimating a single parameter -- a probability $\\theta$. But most real-world problems involve relationships between variables: How does tree height relate to diameter? How does drug dosage affect blood pressure? How do temperature and rainfall predict crop yield? These questions require regression models with **multiple parameters**: slopes, intercepts, and error terms.\n\nIn that first post on seed germination, we worked with a single parameter $\\theta$ (the germination rate). The Beta-Binomial model was simple: one probability to estimate, one prior to specify. You might be wondering: does Bayesian inference scale to more complex problems? What happens when we have multiple parameters that interact?\n\nThe answer is both simple and profound: **the logic doesn't change at all**. Whether you have 1 parameter or 100, the Bayesian workflow remains identical. You specify priors for each parameter, observe data, apply Bayes' theorem, and make predictions. The mathematics gets more complex, but the conceptual framework is exactly the same.\n\nThis post demonstrates that claim by extending everything you learned about seed germination to linear regression with multiple predictors. If you understood how to update beliefs about germination rates, you already understand Bayesian regression -- we're just applying the same principles to familiar statistical models.\n\nThe beautiful truth about Bayesian inference is that moving from one parameter to many changes nothing fundamental. The logic remains identical:\n\n1. **Specify priors** for all parameters\n2. **Observe data** and compute the likelihood\n3. **Apply Bayes' theorem** to get the posterior distribution\n4. **Make predictions** by sampling from the posterior\n\nIn this post, we'll apply Bayesian inference to linear regression using real forestry data. We'll work through two examples:\n\n- **Example 1**: Predicting tree height from diameter -- learning why prior choice matters\n- **Example 2**: Evaluating tree improvement programs -- making economic decisions with uncertainty\n\nBy the end, you'll see that Bayesian regression is simply Bayesian inference applied to familiar statistical models. The principles you learned from seed germination transfer directly -- we're just working with more parameters simultaneously.\n\n# A Brief Historical Note: Francis Galton and the Origins of Regression\n\nBefore diving into our analysis, it's worth appreciating where linear regression came from. The method we're about to use has a fascinating origin story rooted in Victorian-era genetics and an obsession with heredity.\n\n**Sir Francis Galton** (1822-1911), a polymath and Charles Darwin's half-cousin, pioneered regression while studying inheritance patterns. In the 1880s, Galton collected data on the heights of parents and their adult children. He discovered something curious: tall parents tended to have tall children (as expected), but those children were, on average, shorter than their parents. Conversely, short parents tended to have children who were taller than them.\n\nGalton called this phenomenon \"regression toward mediocrity\" -- later shortened to simply \"regression.\" The children's heights \"regressed\" toward the population mean. This observation led him to develop the mathematical tools for describing the linear relationship between two variables [@galton1886regression].\n\n**The method's name is now somewhat misleading**: we rarely use \"regression\" to study this \"regression to the mean\" phenomenon anymore. Instead, the term stuck as the general name for modeling relationships between variables using straight lines (or curves). What Galton discovered was that children's heights could be predicted from parental heights using a linear equation -- the same kind of equation we'll use to predict tree heights from diameters.\n\n**Karl Pearson** (1857-1936), Galton's protégé, formalized these ideas into the regression methods we use today, developing the mathematics of correlation and least squares estimation [@pearson1896mathematical]. Interestingly, while Galton and Pearson developed their methods in a frequentist framework -- focusing on best-fit lines through data -- the Bayesian approach we'll use asks slightly different questions: not just \"what line best fits this data?\" but \"given this data, what can we believe about the relationship?\"\n\nThe tree data we'll analyze follows Galton's original spirit: we're exploring biological relationships where larger individuals tend to have other larger characteristics, though with considerable variation around the trend.\n\n# Example 1: A Simple Start - Predicting Tree Height from Diameter\n\nWe'll begin with a straightforward example: predicting tree height from diameter using the classic `trees` dataset. This single-predictor model will help us understand the Bayesian regression workflow and -- crucially -- see why prior choice matters.\n\n## The Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstanarm)\nlibrary(tidybayes)\nlibrary(bayesplot)\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Load and prepare data\nscale_factor <- 10  # DBH in units of 10 cm\n\n# Convert to metric and scale for better interpretation\ntrees <- datasets::trees %>%\n  mutate(\n    dbh = Girth * 2.54,                           # inches -> cm\n    height = Height * 0.3048,                     # feet -> meters\n    volume = Volume * 0.0283168,                  # cubic feet -> cubic meters\n    dbh_scaled = (dbh - mean(dbh)) / scale_factor # center and scale by 10 cm\n  )\n\n# Quick look at the relationship\nggplot(trees, aes(x = dbh, y = height)) +\n  geom_point(size = 3, alpha = 0.7) +\n  labs(\n    title = \"Tree Height vs. Diameter\", \n    subtitle = \"Black Cherry Trees (n = 31)\",\n    x = \"Diameter at Breast Height (cm)\", \n    y = \"Height (m)\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nThe relationship looks roughly linear and positive -- we expect taller trees to have larger diameters.\n\n## The Model\n\nWe'll predict tree height from diameter:\n\n$$\n\\begin{aligned}\ny_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot x_i \\\\\n\\alpha &\\sim \\text{Normal}(20, 3) \\\\\n\\beta &\\sim \\text{Normal}(1, 1) \\\\\n\\sigma &\\sim \\text{Exponential}(0.5)\n\\end{aligned}\n$$\n\n**In words**: \n\n- **What we're predicting**: $y_i$ is the observed height (in meters) for tree $i$\n- **The model structure**: Each tree's height comes from a Normal distribution with mean $\\mu_i$ and standard deviation $\\sigma$\n- **The relationship**: Mean height $\\mu_i$ depends linearly on diameter through $\\alpha + \\beta \\cdot x_i$, where $x_i$ is the scaled diameter (DBH) for tree $i$\n- **What the priors represent**: Our uncertainty about the model parameters before seeing the data\n  - $\\text{Normal}(20, 3)$ for intercept ($\\alpha$): expected height at mean DBH is around 20 meters\n  - $\\text{Normal}(1, 1)$ for slope ($\\beta$): height increases by roughly 1 meter per 10 cm increase in DBH\n  - $\\text{Exponential}(0.5)$ for residual standard deviation ($\\sigma$): constrains variation to be positive\n- **Why these priors work**: They encode basic domain knowledge (trees are positive heights, larger diameter → taller) while remaining weak enough that the data can easily override them if reality differs\n\n## Why Prior Choice Matters: Revisiting a Key Lesson\n\nIn our seed germination example, we compared different priors (uniform, weakly informative, and strongly informative) and saw how they updated with data. That example involved a single parameter bounded between 0 and 1, making it relatively straightforward to specify sensible priors.\n\nBut regression introduces new challenges: **multiple parameters on different scales that interact**. This is where the \"flat priors are objective\" misconception becomes especially dangerous.\n\nRecall from the frequentist post that all statistical methods involve choices -- which test to use, what $\\alpha$-level, when to stop data collection. The Bayesian framework makes these choices explicit through prior specification. But what happens when we try to be \"objective\" by using very vague priors in regression?\n\n## What Happens with Flat Priors?\n\nLet's use the same philosophy as Beta(1,1) for germination -- expressing \"complete ignorance\" through very vague priors:\n\nSuppose we use very vague priors that express \"ignorance\". This seems \"objective\" --  we're not favoring any particular values. But watch what happens when we use prior predictive checks (the same technique from the seed germination post) to see what kinds of data these priors expect. For this example, we'll assume these vague priors:\n\n- $\\alpha \\sim \\text{Normal}(0, 100)$\n- $\\beta \\sim \\text{Normal}(0, 10)$\n- $\\sigma \\sim \\text{Exponential}(0.1)$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20250823)\n\n# Very vague priors—often called \"flat\" or \"uninformative\"\nn_draws <- 1000\nalpha_prior_flat <- rnorm(n_draws, mean = 0, sd = 100)  # Intercept: anywhere from -200 to +200 m\nbeta_prior_flat  <- rnorm(n_draws, mean = 0, sd = 10)   # Slope: anywhere from -20 to +20 m per 10 cm\n\n# Grid for predictions across observed DBH range\ndbh_grid <- seq(min(trees$dbh), max(trees$dbh), length.out = 100)\ndbh_grid_scaled <- (dbh_grid - mean(trees$dbh)) / scale_factor\n\n# Generate prior predictive lines\nprior_pred_flat <- map2_dfr(\n  alpha_prior_flat, beta_prior_flat,\n  ~ tibble(\n    dbh = dbh_grid,\n    height = .x + .y * dbh_grid_scaled\n  ),\n  .id = \"draw\"\n)\n\n# Sample 100 lines for visualization\nselected_draws <- sample(unique(prior_pred_flat$draw), 100)\nprior_pred_subset <- prior_pred_flat %>% filter(draw %in% selected_draws)\n\nggplot(prior_pred_subset, aes(x = dbh, y = height, group = draw)) +\n  geom_line(alpha = 0.3, color = \"steelblue\") +\n  geom_vline(xintercept = mean(trees$dbh), linetype = \"dashed\", color = \"grey60\") +\n  ylim(-50, 100) +\n  labs(\n    title = \"Prior Predictive Check: Flat Priors\",\n    subtitle = \"100 randomly drawn regression lines from vague priors\",\n    x = \"Diameter (cm)\", \n    y = \"Predicted Height (m)\",\n    caption = \"Problem: Many lines predict negative heights or impossibly tall trees!\\nDashed line shows mean DBH where intercept is interpretable.\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n**Problems with flat priors:**\n\n- **Allows negative tree heights** (biologically impossible)\n- **Allows negative slopes** (taller trees have larger diameters)\n- **Allows impossibly tall trees** (100+ meters for black cherry)\n- **Wastes probability mass** on parameter combinations that could never occur\n\n**Flat priors are not \"objective\"** -- they encode very strong (and wrong) assumptions. They say \"a 50-meter tall sapling is just as plausible as a 20-meter mature tree\" and \"trees that shrink as they grow are reasonable.\"\n\n**The lesson from seed germination still applies:** priors encode assumptions whether we acknowledge them or not. The difference is that in regression, **flat priors have worse consequences**:\n\n- **Beta(1,1) for germination:** Allows any rate from 0 to 1, which are all biologically possible\n- **Normal(0, 100) for tree height intercept:** Allows negative heights and 100+ meter saplings, which are **impossible**\n\nThe key insight from our series: just as we used prior knowledge (published germination rates, historical supplier data) to inform the seed germination prior, we should use **domain knowledge** (trees don't have negative height, trees grow up not down) to inform regression priors.\n\n**You're not \"cheating\" by using informative priors -- you're incorporating knowledge that everyone already has.** The alternative (flat priors) pretends you don't know basic facts about the world, which is both dishonest and statistically harmful.\n\n### Weakly Informative Priors\n\nNow let's use priors that encode minimal biological knowledge:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20250823)\n\n# Weakly informative priors based on minimal biological knowledge\nalpha_prior_inf <- rnorm(n_draws, mean = 20, sd = 3)   # Height at mean DBH: around 15-25 m\nbeta_prior_inf <- rnorm(n_draws, mean = 1, sd = 1)     # Slope: around 0-3 m per 10 cm, centered at 1\n\n# Generate prior predictive lines\nprior_pred_inf <- map2_dfr(\n  alpha_prior_inf, beta_prior_inf,\n  ~ tibble(\n    dbh = dbh_grid,\n    height = .x + .y * dbh_grid_scaled\n  ),\n  .id = \"draw\"\n)\n\nselected_draws_inf <- sample(unique(prior_pred_inf$draw), 100)\nprior_pred_subset_inf <- prior_pred_inf %>% filter(draw %in% selected_draws_inf)\n\nggplot(prior_pred_subset_inf, aes(x = dbh, y = height, group = draw)) +\n  geom_line(alpha = 0.3, color = \"steelblue\") +\n  geom_vline(xintercept = mean(trees$dbh), linetype = \"dashed\", color = \"grey60\") +\n  labs(\n    title = \"Prior Predictive Check: Weakly Informative Priors\",\n    subtitle = \"100 randomly drawn regression lines from biologically informed priors\",\n    x = \"Diameter (cm)\", \n    y = \"Predicted Height (m)\",\n    caption = \"All predictions are biologically plausible: positive heights (15-30 m), positive slopes.\\nDashed line shows mean DBH.\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n**Much better!** These priors:\n\n- **Keep heights positive and reasonable** (15-30 m range)\n- **Support positive relationships** (taller trees have larger diameters)\n- **Still allow data to substantially update beliefs** (wide enough to not be dogmatic)\n- **Reflect minimal biological constraints** (you don't need a PhD in forestry)\n\nThis is the key insight: **You're not claiming to know the answer ahead of time; you're just ruling out nonsense.** It's like saying \"I don't know exactly how tall that building is, but I know it's not negative height and it's not 1000 km tall.\"\n\n## Visualizing Our Priors as Distributions\n\nLet's also look at the prior distributions themselves:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate samples from our priors\nset.seed(789)\nn_draws <- 10000\n\nprior_samples <- tibble(\n  intercept = rnorm(n_draws, mean = 20, sd = 3),\n  slope = rnorm(n_draws, mean = 1, sd = 1),\n  sigma = rexp(n_draws, rate = 0.5)\n)\n\n# Visualize the prior distributions\np1 <- ggplot(prior_samples, aes(x = intercept)) +\n  geom_histogram(bins = 50, fill = \"#7570b3\", alpha = 0.7) +\n  labs(\n    title = \"Prior for Intercept\",\n    subtitle = \"Normal(20, 3)\",\n    x = \"Intercept α (m)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\np2 <- ggplot(prior_samples, aes(x = slope)) +\n  geom_histogram(bins = 50, fill = \"#1b9e77\", alpha = 0.7) +\n  labs(\n    title = \"Prior for Slope\",\n    subtitle = \"Normal(1, 1)\",\n    x = \"Slope β (m per 10 cm DBH)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\np3 <- ggplot(prior_samples, aes(x = sigma)) +\n  geom_histogram(bins = 50, fill = \"#d95f02\", alpha = 0.7) +\n  xlim(0, 10) +\n  labs(\n    title = \"Prior for Residual SD\",\n    subtitle = \"Exponential(0.5)\",\n    x = \"Sigma σ (m)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n(p1 | p2 | p3) +\n  plot_annotation(\n    title = \"Prior Distributions for Height Model Parameters\",\n    subtitle = \"What we believe before seeing the data\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n**What these priors say:**\n\n- **Intercept**: Centered at 20 m (typical height for trees with average DBH), allowing variation between roughly 14-26 m\n- **Slope**: Centered at 1 m per 10 cm DBH increase, allowing values between roughly -1 to 3 m\n- **Sigma**: Most mass between 0.5-6 m, weakly favoring smaller residual variation\n\nThese are **weakly informative** priors -- they gently constrain parameters to reasonable scales without being dogmatic.\n\n## Fitting the Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(knitr)\nlibrary(tibble)\n\n# Fit Bayesian linear regression\nfit_height <- stan_glm(\n  height ~ dbh_scaled,\n  data = trees,\n  family = gaussian(),\n  prior_intercept = normal(20, 3),\n  prior = normal(1, 1),\n  prior_aux = exponential(0.5),\n  chains = 4,\n  iter = 2000,\n  seed = 123,\n  refresh = 0\n)\n\n# Display posterior summary as formatted table\nsummary(fit_height, probs = c(0.025, 0.975), digits = 3) %>%\n  as.data.frame() %>%\n  rownames_to_column(\"Parameter\") %>%\n  kable(caption = \"Posterior Summary Statistics\", digits = 3)\n```\n\n::: {.cell-output-display}\n\n\nTable: Posterior Summary Statistics\n\n|Parameter     |    mean|  mcse|    sd|    2.5%|   97.5%| n_eff|  Rhat|\n|:-------------|-------:|-----:|-----:|-------:|-------:|-----:|-----:|\n|(Intercept)   |  23.136| 0.006| 0.313|  22.518|  23.754|  2965| 1.001|\n|dbh_scaled    |   1.225| 0.006| 0.382|   0.442|   1.965|  3669| 1.000|\n|sigma         |   1.737| 0.004| 0.235|   1.359|   2.252|  3094| 1.002|\n|mean_PPD      |  23.133| 0.008| 0.435|  22.274|  23.985|  2849| 1.000|\n|log-posterior | -65.312| 0.034| 1.314| -68.777| -63.826|  1514| 1.001|\n\n\n:::\n:::\n\n\n**Parameter Interpretation:**\n\n- **mean**: Average value across all posterior samples -- the central estimate of the parameter\n- **mcse**: Monte Carlo Standard Error -- uncertainty in the mean due to finite sampling (smaller is better)\n- **sd**: Standard deviation of the posterior -- measures overall uncertainty about the parameter\n- **2.5%, 97.5%**: Bounds of the 95% credible interval -- we're 95% confident the true value lies between these\n- **n_eff**: Effective sample size -- number of independent samples after accounting for autocorrelation\n- **Rhat**: Convergence diagnostic -- compares within-chain and between-chain variance\n\n**What to look for:**\n\n- **Rhat ≈ 1.00**: Chains have converged (values > 1.01 suggest problems)\n- **n_eff > 1000**: Enough independent samples for reliable inference  \n- **mcse much smaller than sd**: Monte Carlo error is negligible compared to posterior uncertainty\n- **Credible interval makes sense**: Check that the 2.5%-97.5% range is scientifically plausible\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_trace(fit_height, pars = c(\"(Intercept)\", \"dbh_scaled\", \"sigma\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n**Good trace plots** look like \"fuzzy caterpillars\" -- random noise with no trends, all chains overlapping.\n\n## Making Direct Probability Statements\n\nNow we can ask specific questions about the relationship:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract posterior samples\nposterior_height <- as_draws_df(fit_height)\n\n# Direct probability statements\nprob_slope_positive <- mean(posterior_height$dbh_scaled > 0)\nprob_slope_gt_1 <- mean(posterior_height$dbh_scaled > 1.0)\nprob_slope_gt_1.5 <- mean(posterior_height$dbh_scaled > 1.5)\n\n# Credible interval\nslope_ci <- quantile(posterior_height$dbh_scaled, c(0.025, 0.975))\n\n# Create summary table\ntibble(\n  Question = c(\n    \"Is the slope positive?\",\n    \"Is the slope greater than 1.0 m per 10 cm?\",\n    \"Is the slope greater than 1.5 m per 10 cm?\",\n    \"95% Credible Interval\"\n  ),\n  Answer = c(\n    sprintf(\"P(β > 0) = %.3f\", prob_slope_positive),\n    sprintf(\"P(β > 1.0) = %.3f\", prob_slope_gt_1),\n    sprintf(\"P(β > 1.5) = %.3f\", prob_slope_gt_1.5),\n    sprintf(\"[%.2f, %.2f] m per 10 cm\", slope_ci[1], slope_ci[2])\n  )\n) %>%\n  knitr::kable(caption = \"Direct probability statements about the height-diameter relationship\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Direct probability statements about the height-diameter relationship\n\n|Question                                   |Answer                   |\n|:------------------------------------------|:------------------------|\n|Is the slope positive?                     |P(β > 0) = 0.999         |\n|Is the slope greater than 1.0 m per 10 cm? |P(β > 1.0) = 0.738       |\n|Is the slope greater than 1.5 m per 10 cm? |P(β > 1.5) = 0.232       |\n|95% Credible Interval                      |[0.44, 1.97] m per 10 cm |\n\n\n:::\n:::\n\n\n::: {.callout-note}\n## This is what makes Bayesian inference powerful\n\nWe can make direct probability statements about parameters. Based on these results, we can say: \"There is a 73.8% probability that height increases more than 1 meter for every 10 cm increase in diameter.\"\n\nThese are the statements researchers and decision -- makers actually want—direct quantification of uncertainty about the parameter itself.\n:::\n\n\n## Visualizing the Posterior\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create prediction grid\nnewdata <- tibble(\n  dbh_scaled = seq(min(trees$dbh_scaled), max(trees$dbh_scaled), length.out = 100)\n)\n\n# Sample 100 posterior draws for visualization\nset.seed(456)\nsample_draws <- sample(1:nrow(posterior_height), size = 100)\n\nfitted_draws_height <- crossing(\n  newdata,\n  .draw = sample_draws\n) %>%\n  mutate(\n    mu = posterior_height$`(Intercept)`[.draw] + posterior_height$dbh_scaled[.draw] * dbh_scaled,\n    dbh = dbh_scaled * scale_factor + mean(trees$dbh)\n  )\n\nggplot() +\n  # Posterior mean regression lines (blue lines)\n  geom_line(\n    data = fitted_draws_height,\n    aes(x = dbh, y = mu, group = .draw),\n    alpha = 0.2,\n    color = \"blue\"\n  ) +  \n  # Observed data\n  geom_point(\n    data = trees,\n    aes(dbh, height),\n    size = 2,\n    alpha = 0.7\n  ) +\n  labs(\n    title = \"Posterior: Regression Lines Show Our Updated Beliefs\",\n    subtitle = \"Each blue line is one plausible relationship given the data\",\n    x = \"Diameter at Breast Height (cm)\",\n    y = \"Height (m)\",\n    caption = \"100 posterior draws | All lines have positive slopes -- our data confirmed trees grow up!\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n**Key insight**: Every line has a positive slope because the data overwhelmingly support a positive relationship. The variation in lines represents our remaining uncertainty about the exact slope and intercept.\n\n## Posterior Predictive Check\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(fit_height, ndraws = 100) +\n  labs(\n    title = \"Posterior Predictive Check: Does Our Model Make Sense?\",\n    subtitle = \"Comparing real data to simulated data from the model\",\n    x = \"Height (m)\",\n    y = \"Density\",\n    caption = \"Dark line = observed data | Light lines = 100 simulated datasets from posterior\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nThe observed data (dark line) falls well within the range of simulated datasets -- our model captures the essential features of tree heights.\n\n## What We've Learned from This Example\n\nThis simple example demonstrated several key principles:\n\n1. **Prior predictive checks reveal problems**: Flat priors allow nonsense predictions\n2. **Weakly informative priors encode domain knowledge**: We ruled out negative heights without being overly restrictive\n3. **Direct probability statements**: We can say \"$P(slope > 1) = 0.85$\" directly\n4. **Full uncertainty visualization**: Posterior draws show all plausible relationships\n5. **Model checking**: Posterior predictive checks validate our model assumptions\n\nNow let's apply these same principles to a more complex problem: predicting timber volume with multiple predictors and making economic decisions under uncertainty.\n\n# Example 2: Predicting Volume and Tree Improvement Economics\n\nIn this example, we'll tackle two related questions:\n\n1. How do DBH and height together predict timber volume?\n2. Should a forestry company invest in genetically improved seedlings?\n\nThis demonstrates Bayesian regression at its best: quantifying uncertainty for real business decisions.\n\n## Part A: Volume Prediction Model\n\nFirst, let's build a multiple regression model predicting volume from both DBH and height.\n\n### The Model\n\n$$\n\\begin{aligned}\ny_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta_1 \\cdot \\text{dbh}_i + \\beta_2 \\cdot \\text{height}_i \\\\\n\\alpha &\\sim \\text{Normal}(0, 1) \\\\\n\\beta_1 &\\sim \\text{Normal}(0, 1) \\\\\n\\beta_2 &\\sim \\text{Normal}(0, 1) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{aligned}\n$$\n\n**In words**: \n\n- **What we're predicting**: $y_i$ is the observed volume (in cubic meters) for tree $i$\n- **The model structure**: Each tree's volume comes from a Normal distribution with mean $\\mu_i$ and standard deviation $\\sigma$\n- **The relationship**: Mean volume $\\mu_i$ depends linearly on both diameter and height through $\\alpha + \\beta_1 \\cdot \\text{dbh}_i + \\beta_2 \\cdot \\text{height}_i$\n- **What the priors represent**: Our uncertainty in the model parameters before seeing the data\n  - $\\text{Normal}(0, 1)$ for intercept ($\\alpha$) and slopes ($\\beta_1$, $\\beta_2$)\n  - $\\text{Exponential}(1)$ for residual standard deviation ($\\sigma$)\n- **Why these priors work**: Since predictors are scaled (standardized to mean 0, SD 1), these priors:\n  - Encode weak prior information (expect modest-sized effects)\n  - Allow the data to dominate the posterior\n  - Prevent extreme parameter values that would produce nonsensical predictions\n\n\n### Fitting the Volume Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(knitr)\nlibrary(tibble)\n\n# Scale predictors for prior specification\ntrees <- trees %>%\n  mutate(\n    height_scaled = (height - mean(height)) / sd(height),\n    dbh_scaled_vol = (dbh - mean(dbh)) / sd(dbh)\n  )\n\n# Fit Bayesian multiple regression\nfit_volume <- stan_glm(\n  volume ~ dbh_scaled_vol + height_scaled,\n  data = trees,\n  family = gaussian(),\n  prior_intercept = normal(0, 1),\n  prior = normal(0, 1),\n  prior_aux = exponential(1),\n  chains = 4,\n  iter = 2000,\n  seed = 456,\n  refresh = 0\n)\n\n# Display posterior summary as formatted table\nsummary(fit_volume, probs = c(0.025, 0.975), digits = 3) %>%\n  as.data.frame() %>%\n  rownames_to_column(\"Parameter\") %>%\n  kable(caption = \"Posterior Summary Statistics Volume Model\", digits = 3)\n```\n\n::: {.cell-output-display}\n\n\nTable: Posterior Summary Statistics Volume Model\n\n|Parameter      |   mean|  mcse|    sd|   2.5%|  97.5%| n_eff|  Rhat|\n|:--------------|------:|-----:|-----:|------:|------:|-----:|-----:|\n|(Intercept)    |  0.854| 0.000| 0.021|  0.813|  0.894|  3955| 1.001|\n|dbh_scaled_vol |  0.418| 0.000| 0.025|  0.369|  0.467|  2705| 1.000|\n|height_scaled  |  0.061| 0.001| 0.025|  0.011|  0.110|  2537| 1.001|\n|sigma          |  0.115| 0.000| 0.016|  0.089|  0.151|  3020| 1.001|\n|mean_PPD       |  0.854| 0.000| 0.029|  0.796|  0.910|  4149| 1.001|\n|log-posterior  | 18.360| 0.035| 1.444| 14.833| 20.197|  1659| 1.001|\n\n\n:::\n:::\n\n\n### Making Probability Statements About Effects\n\n**Key question: How certain are we that height and diameter increase volume?**\n\nBecause we've scaled both predictors (standardized to mean 0, standard deviation 1), the coefficients β<sub>height</sub> and β<sub>DBH</sub> represent the change in volume for a **1 standard deviation change** in each predictor. For this dataset, 1 SD of height ≈ 2 meters and 1 SD of DBH ≈ 4 cm. This scaling allows us to directly compare effect sizes: larger coefficient = stronger effect per SD change.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract posterior samples\nposterior_volume <- as_draws_df(fit_volume)\n\n# Direct probability statements about effects\nheight_prob_positive <- mean(posterior_volume$height_scaled > 0)\nheight_prob_gt_005 <- mean(posterior_volume$height_scaled > 0.05)\ndbh_prob_positive <- mean(posterior_volume$dbh_scaled_vol > 0)\nprob_dbh_larger <- mean(posterior_volume$dbh_scaled_vol > posterior_volume$height_scaled)\n\nrm(posterior_volume)\nrm(fit_volume)\n\n# Create summary table\ntibble(\n  Question = c(\n    \"Does height increase volume?\",\n    \"Is height effect > 0.05?\",\n    \"Does DBH increase volume?\",\n    \"Is DBH effect larger than height effect?\"\n  ),\n  Answer = c(\nsprintf(\"P(β<sub>height</sub> > 0) = %.3f\", height_prob_positive),\nsprintf(\"P(β<sub>height</sub> > 0.05) = %.3f\", height_prob_gt_005),\nsprintf(\"P(β<sub>DBH</sub> > 0) = %.3f\", dbh_prob_positive),\nsprintf(\"P(β<sub>DBH</sub> > β<sub>height</sub>) = %.3f\", prob_dbh_larger)\n  )\n) %>%\n  knitr::kable(caption = \"Direct probability statements about volume predictors\",\n               escape = FALSE)\n```\n\n::: {.cell-output-display}\n\n\nTable: Direct probability statements about volume predictors\n\n|Question                                 |Answer                                          |\n|:----------------------------------------|:-----------------------------------------------|\n|Does height increase volume?             |P(β<sub>height</sub> > 0) = 0.992               |\n|Is height effect > 0.05?                 |P(β<sub>height</sub> > 0.05) = 0.674            |\n|Does DBH increase volume?                |P(β<sub>DBH</sub> > 0) = 1.000                  |\n|Is DBH effect larger than height effect? |P(β<sub>DBH</sub> > β<sub>height</sub>) = 1.000 |\n\n\n:::\n:::\n\n\n::: {.callout-important}\n## Interpretation\n\nWe can say with near certainty ($P > 0.999$) that both DBH and height increase timber volume. DBH has the stronger effect per standard deviation change, which makes biological sense -- volume depends on cross-sectional area (proportional to DBH²) and height. \n\n**Note on notation**: $P(\\beta_{\\text{height}} > 0)$ means \"the probability that the height coefficient is positive.\" In Bayesian inference, we can make these direct probability statements because we treat parameters as random variables with probability distributions.\n:::\n\n\n## Part B: Tree Improvement Economics\n\nNow we use this understanding to evaluate a business decision: **Should we invest in genetically improved seedlings?**\n\n### The Scenario\n\nA forestry company is considering switching from wild white spruce seed ($0.50 per seedling) to improved seed from a breeding program ($2.50 per seedling). The breeding program claims 15-20% volume gains. \n\n**Economic question**: If we plant 1 million seedlings, will the additional volume justify the $2 million extra investment?\n\n::: {.callout-note}\n## Simplified Economic Model\n\nFor pedagogical clarity, this example uses a simplified economic model where we assume **each additional m³ of timber generates $50 in profit**. This is a hypothetical number chosen to illustrate the Bayesian decision-making workflow.\n\nIn reality, forest economics involves complex factors including:\n\n- Stumpage fees paid to government\n- Variable harvest and processing costs\n- Market price fluctuations\n- Discount rates over long rotations (60-80 years)\n- Regulatory constraints on harvest levels\n\nSee the **Alberta Forest Management** box below for a realistic example of how these factors interact in practice. The key point is that the **Bayesian workflow remains identical** regardless of model complexity—we simply add more distributions and propagate more uncertainty.\n:::\n\n### Simulating Realistic Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2025)\n\n# Sample size\nn <- 150  # Trees measured per seedlot\n\n# Simulate wild seedlot (unimproved)\nwild_trees <- tibble(\n  seedlot = \"Wild\",\n  age = 25,\n  dbh = rnorm(n, mean = 18, sd = 3.5),  # cm at 25 years\n  height = rnorm(n, mean = 14, sd = 2.2)  # meters\n) %>%\n  mutate(\n    # Volume relationship with biological realism\n    log_volume = -9.5 + 2 * log(dbh) + 1 * log(height) + rnorm(n, 0, 0.15),\n    volume = exp(log_volume)   # cubic meters (m³)\n  )\n\n# Simulate improved seedlot (genetically superior)\nimproved_trees <- tibble(\n  seedlot = \"Improved\",\n  age = 25,\n  dbh = rnorm(n, mean = 19.5, sd = 3.0),  # Larger and more uniform\n  height = rnorm(n, mean = 15.2, sd = 1.9)  # Taller and more uniform\n) %>%\n  mutate(\n    log_volume = -9.5 + 2 * log(dbh) + 1 * log(height) + rnorm(n, 0, 0.12),\n    volume = exp(log_volume)  # cubic meters (m³)\n  )\n\n# Combine datasets\ntree_comparison <- bind_rows(wild_trees, improved_trees)\n\n# Visualize the comparison\np1 <- ggplot(tree_comparison, aes(x = volume, fill = seedlot)) +\n  geom_histogram(alpha = 0.6, position = \"identity\", bins = 30) +\n  scale_fill_manual(values = c(\"Wild\" = \"#d95f02\", \"Improved\" = \"#1b9e77\")) +\n  labs(\n    title = \"Volume Distribution by Seedlot Type\",\n    x = \"Volume (m³)\",\n    y = \"Count\",\n    fill = \"Seedlot\"\n  ) +\n  theme_minimal()\n\np2 <- ggplot(tree_comparison, aes(x = seedlot, y = volume, fill = seedlot)) +\n  geom_boxplot(alpha = 0.7) +\n  scale_fill_manual(values = c(\"Wild\" = \"#d95f02\", \"Improved\" = \"#1b9e77\")) +\n  labs(\n    title = \"Volume Comparison\",\n    x = \"Seedlot Type\",\n    y = \"Volume (m³)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np1 / p2\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n### Bayesian Models for Each Seedlot\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit model for wild seedlot\nfit_wild <- stan_glm(\n  volume ~ dbh + height,\n  data = wild_trees,\n  family = gaussian(),\n  prior_intercept = normal(0, 100),\n  prior = normal(0, 50),\n  prior_aux = exponential(0.1),\n  chains = 4,\n  iter = 4000,\n  seed = 123,\n  refresh = 0\n)\n\n# Display posterior summary as formatted table\nsummary(fit_wild, probs = c(0.025, 0.975), digits = 3) %>%\n  as.data.frame() %>%\n  rownames_to_column(\"Parameter\") %>%\n  kable(caption = \"Posterior Summary Statistics: Wild Seedlot\", digits = 3)\n```\n\n::: {.cell-output-display}\n\n\nTable: Posterior Summary Statistics: Wild Seedlot\n\n|Parameter     |    mean|  mcse|    sd|    2.5%|   97.5%| n_eff|  Rhat|\n|:-------------|-------:|-----:|-----:|-------:|-------:|-----:|-----:|\n|(Intercept)   |  -0.704| 0.001| 0.043|  -0.790|  -0.620|  7083| 1.000|\n|dbh           |   0.039| 0.000| 0.002|   0.036|   0.042|  8355| 1.000|\n|height        |   0.025| 0.000| 0.002|   0.020|   0.030|  7446| 1.000|\n|sigma         |   0.065| 0.000| 0.004|   0.058|   0.073|  1980| 1.001|\n|mean_PPD      |   0.350| 0.000| 0.008|   0.335|   0.365|  2962| 1.002|\n|log-posterior | 185.494| 0.032| 1.452| 181.873| 187.323|  2119| 1.000|\n\n\n:::\n\n```{.r .cell-code}\n# Fit model for improved seedlot\nfit_improved <- stan_glm(\n  volume ~ dbh + height,\n  data = improved_trees,\n  family = gaussian(),\n  prior_intercept = normal(0, 100),\n  prior = normal(0, 50),\n  prior_aux = exponential(0.1),\n  chains = 4,\n  iter = 4000,\n  seed = 456,\n  refresh = 0\n)\n\n# Display posterior summary as formatted table\nsummary(fit_improved, probs = c(0.025, 0.975), digits = 3) %>%\n  as.data.frame() %>%\n  rownames_to_column(\"Parameter\") %>%\n  kable(caption = \"Posterior Summary Statistics: Improved Seedlot\", digits = 3)\n```\n\n::: {.cell-output-display}\n\n\nTable: Posterior Summary Statistics: Improved Seedlot\n\n|Parameter     |    mean|  mcse|    sd|    2.5%|   97.5%| n_eff|  Rhat|\n|:-------------|-------:|-----:|-----:|-------:|-------:|-----:|-----:|\n|(Intercept)   |  -1.031| 0.001| 0.051|  -1.132|  -0.932|  7998| 1.000|\n|dbh           |   0.049| 0.000| 0.002|   0.046|   0.052|  9292| 1.000|\n|height        |   0.035| 0.000| 0.003|   0.029|   0.040|  7530| 1.000|\n|sigma         |   0.067| 0.000| 0.004|   0.060|   0.075|  2132| 1.001|\n|mean_PPD      |   0.456| 0.000| 0.008|   0.441|   0.472|  3698| 1.000|\n|log-posterior | 180.875| 0.032| 1.423| 177.265| 182.653|  2024| 1.001|\n\n\n:::\n:::\n\n\n### Answering Economic Questions with Probability\n\n**Question 1: What's the expected volume gain?**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# To properly assess genetic improvement, we compare predictions for \n# TYPICAL trees from each seedlot (using their respective mean sizes)\n# This captures both effects: (1) improved seedlings grow bigger, and \n# (2) improved seedlings may have better stem form\n\n# Typical wild seedlot tree\ntypical_wild <- tibble(\n  dbh = mean(wild_trees$dbh),\n  height = mean(wild_trees$height)\n)\n\n# Typical improved seedlot tree  \ntypical_improved <- tibble(\n  dbh = mean(improved_trees$dbh),\n  height = mean(improved_trees$height)\n)\n\n# Get posterior predictions - each model predicts for its own typical tree\npred_wild <- posterior_predict(fit_wild, newdata = typical_wild)\npred_improved <- posterior_predict(fit_improved, newdata = typical_improved)\n\n# Calculate volume difference\nvolume_gain <- pred_improved - pred_wild\ngain_percent <- 100 * volume_gain / pred_wild\n\n# Summarize\ntibble(\n  Metric = c(\n    \"Mean volume gain\",\n    \"Median volume gain\",\n    \"95% Credible Interval\",\n    \"Mean percent gain\",\n    \"Median percent gain\"\n  ),\n  Value = c(\n    sprintf(\"%.3f m³\", mean(volume_gain)),\n    sprintf(\"%.3f m³\", median(volume_gain)),\n    sprintf(\"[%.3f, %.3f] m³\", quantile(volume_gain, 0.025), quantile(volume_gain, 0.975)),\n    sprintf(\"%.1f%%\", mean(gain_percent)),\n    sprintf(\"%.1f%%\", median(gain_percent))\n  )\n) %>%\n  knitr::kable(caption = \"Expected volume gain from tree improvement\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Expected volume gain from tree improvement\n\n|Metric                |Value              |\n|:---------------------|:------------------|\n|Mean volume gain      |0.105 m³           |\n|Median volume gain    |0.104 m³           |\n|95% Credible Interval |[-0.082, 0.291] m³ |\n|Mean percent gain     |35.1%              |\n|Median percent gain   |30.1%              |\n\n\n:::\n:::\n\n\n::: {.callout-note}\n## Understanding this comparison\n\nWe're comparing **typical trees from each seedlot** (using their respective mean sizes). This captures the **total genetic improvement effect**, which includes:\n\n1. **Size advantage**: Improved trees grow larger (bigger DBH, taller)\n2. **Potential form advantage**: If improved trees have better stem form, taper, or wood density, this would be captured by different model coefficients (though our simulation keeps this simple)\n\nIn reality, tree breeding programs aim to improve both growth rate (get to merchantable size faster) AND stem quality (more volume per unit size). This comparison captures the combined effect, which is what matters economically: *how much more volume will I harvest per tree at rotation age?*\n:::\n\n\n**Question 2: What's the probability of achieving target gains?**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probability of different gain thresholds\nprob_gain_positive <- mean(volume_gain > 0)\nprob_gain_10pct <- mean(gain_percent > 10)\nprob_gain_15pct <- mean(gain_percent > 15)\nprob_gain_20pct <- mean(gain_percent > 20)\n\n# Create summary table\ntibble(\n  Threshold = c(\n    \"Any positive gain\",\n    \"Gain > 10%\",\n    \"Gain > 15%\",\n    \"Gain > 20%\"\n  ),\n  Probability = c(\n    sprintf(\"%.3f\", prob_gain_positive),\n    sprintf(\"%.3f\", prob_gain_10pct),\n    sprintf(\"%.3f\", prob_gain_15pct),\n    sprintf(\"%.3f\", prob_gain_20pct)\n  )\n) %>%\n  knitr::kable(caption = \"Probability of achieving volume gain targets\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Probability of achieving volume gain targets\n\n|Threshold         |Probability |\n|:-----------------|:-----------|\n|Any positive gain |0.868       |\n|Gain > 10%        |0.762       |\n|Gain > 15%        |0.697       |\n|Gain > 20%        |0.630       |\n\n\n:::\n:::\n\n\n::: {.callout-tip}\n## Business interpretation\n\nThese direct probability statements answer the key question: \"What's the chance our breeding program delivers what it promises?\" Decision-makers can use these probabilities to assess risk and make informed investment choices.\n:::\n\n### Generating Predictions with Full Uncertainty\n\nBefore we proceed to the economic analysis, let's demonstrate one of Bayesian inference's most powerful features: **generating predictions that include all sources of uncertainty**. This is crucial for realistic decision-making because it prevents us from being overconfident.\n\nWhen we predict future volumes, there are multiple sources of uncertainty:\n\n1. **Parameter uncertainty**: We don't know the *exact* regression coefficients ($\\alpha, \\beta_1, \\beta_2, \\sigma$)\n2. **Individual tree variation**: Even trees of the same size vary in volume due to genetics, microsite, and measurement error\n3. **Model uncertainty**: Our model is a simplification of reality\n\nThe **posterior predictive distribution** captures all of these simultaneously. Unlike just using the mean parameter estimates (which would give us a single \"best guess\"), the posterior predictive distribution gives us a full probability distribution of possible outcomes.\n\n**Why does this matter for tree improvement decisions?**\n\nImagine you're planning harvest revenues for the next rotation. If you only use the mean predicted volume gain (say, 15%), you might sign timber contracts based on that number. But what if the actual gain is only 5%? Or 25%? The posterior predictive distribution tells us the *range* of plausible outcomes, letting us:\n\n- Calculate probability of meeting minimum revenue targets\n- Plan for worst-case scenarios\n- Quantify financial risk accurately\n- Make conservative estimates for contracts\n\nLet's generate posterior predictions for a future harvest of 1000 trees from each seedlot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate a future harvest: 1000 trees from each seedlot\n# These will have the typical size characteristics of each seedlot\nset.seed(2025)\nn_future <- 1000\n\n# Future wild seedlot trees\nfuture_wild <- tibble(\n  dbh = rnorm(n_future, mean = mean(wild_trees$dbh), sd = sd(wild_trees$dbh)),\n  height = rnorm(n_future, mean = mean(wild_trees$height), sd = sd(wild_trees$height))\n)\n\n# Future improved seedlot trees  \nfuture_improved <- tibble(\n  dbh = rnorm(n_future, mean = mean(improved_trees$dbh), sd = sd(improved_trees$dbh)),\n  height = rnorm(n_future, mean = mean(improved_trees$height), sd = sd(improved_trees$height))\n)\n\n# Generate posterior predictive distributions\n# Each prediction includes parameter uncertainty + residual variation\npred_future_wild <- posterior_predict(fit_wild, newdata = future_wild)\npred_future_improved <- posterior_predict(fit_improved, newdata = future_improved)\n\n# Calculate total volume per 1000 trees for each posterior draw\n# Each row of pred_future_* is one posterior draw, columns are individual trees\ntotal_volume_wild <- rowSums(pred_future_wild)\ntotal_volume_improved <- rowSums(pred_future_improved)\n\n# Volume gain per 1000 trees\ntotal_gain <- total_volume_improved - total_volume_wild\ngain_per_tree <- total_gain / n_future\n\n# Summarize\ntibble(\n  Metric = c(\n    \"Mean total volume (Wild)\",\n    \"Mean total volume (Improved)\",\n    \"Mean volume gain per 1000 trees\",\n    \"Mean volume gain per tree\",\n    \"95% CI for gain per tree\",\n    \"P(gain per tree > 0.01 m³)\"\n  ),\n  Value = c(\n    sprintf(\"%.1f m³\", mean(total_volume_wild)),\n    sprintf(\"%.1f m³\", mean(total_volume_improved)),\n    sprintf(\"%.1f m³\", mean(total_gain)),\n    sprintf(\"%.4f m³\", mean(gain_per_tree)),\n    sprintf(\"[%.4f, %.4f] m³\", \n            quantile(gain_per_tree, 0.025), \n            quantile(gain_per_tree, 0.975)),\n    sprintf(\"%.3f\", mean(gain_per_tree > 0.01))\n  )\n) %>%\n  knitr::kable(caption = \"Posterior predictive distribution: Future harvest of 1000 trees per seedlot\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Posterior predictive distribution: Future harvest of 1000 trees per seedlot\n\n|Metric                          |Value               |\n|:-------------------------------|:-------------------|\n|Mean total volume (Wild)        |348.3 m³            |\n|Mean total volume (Improved)    |455.9 m³            |\n|Mean volume gain per 1000 trees |107.6 m³            |\n|Mean volume gain per tree       |0.1076 m³           |\n|95% CI for gain per tree        |[0.0909, 0.1234] m³ |\n|P(gain per tree > 0.01 m³)      |1.000               |\n\n\n:::\n:::\n\n\n<br>\n\n**Economic Breakeven Analysis**\n\nA critical question for decision-makers is: what volume gain do we need to justify the additional seedling cost? The figure below shows the full distribution of predicted volume gains per tree, with two key reference lines: the expected gain (blue dashed line) and the economic breakeven point (red dashed line). The breakeven point is calculated as the additional cost per seedling (\\$2.00) divided by the profit margin per cubic meter (\\$50), giving us the minimum volume gain needed to recover our investment. By comparing the distribution to this threshold, we can directly assess the probability that the tree improvement program will be economically viable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize the uncertainty\nposterior_predictions <- tibble(\n  total_gain = total_gain,\n  gain_per_tree = gain_per_tree\n)\n\n# Calculate the economic breakeven gain per tree\n# Additional cost per tree = $2.00\n# Hypothetical profit per m³ = $50\n# Breakeven volume gain = $2.00 / $50 = 0.04 m³ per tree\neconomic_breakeven <- 2.00 / 50\n\nggplot(posterior_predictions, aes(x = gain_per_tree)) +\n  geom_histogram(bins = 50, fill = \"#1b9e77\", alpha = 0.7) +\n  geom_vline(xintercept = mean(gain_per_tree), \n             linetype = \"dashed\", color = \"blue\", linewidth = 1) +\n  geom_vline(xintercept = economic_breakeven, \n             linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Posterior Predictive Distribution: Volume Gain per Tree\",\n    subtitle = \"Full uncertainty for a future harvest of 1000 trees\",\n    x = \"Volume gain per tree (m³)\",\n    y = \"Count\",\n    caption = sprintf(\"Red line = economic breakeven (%.4f m³) | Blue line = expected gain (%.4f m³) | Distribution includes all uncertainty\",\n                      economic_breakeven, mean(gain_per_tree))\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n::: {.callout-important}\n## Key insight: Uncertainty matters for planning\n\nNotice the **width** of this distribution. While the mean gain per tree is around 0.1076 m³, the 95% credible interval spans from 0.0909 to 0.1234 m³. \n\nThis means that even though improved seedlings are expected to produce more volume, there's substantial uncertainty about *how much* more. A harvest plan based solely on the mean prediction would miss this variability, potentially leading to:\n\n- **Overcommitted timber contracts** if gains are at the lower end\n- **Missed revenue opportunities** if gains are at the upper end  \n- **Cashflow problems** from unexpected shortfalls\n\nBy using the **full posterior predictive distribution**, forest managers can:\n- Set conservative harvest targets (e.g., use the 25th percentile)\n- Calculate probability of meeting contract volumes\n- Price timber sales with appropriate risk premiums\n- Make financially sound long-term investment decisions\n\nThis is Bayesian inference at its best: honest quantification of uncertainty that leads to better decisions.\n:::\n<br>\n**Question 3: Economic Payoff Analysis**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Economic parameters (all costs are per seedling)\ncost_per_seedling_wild <- 0.50\ncost_per_seedling_improved <- 2.50\nadditional_cost <- cost_per_seedling_improved - cost_per_seedling_wild  # $2.00 per tree\n\n# Investment scale: 1 million trees planted\ntrees_planted <- 1e6  \ntotal_investment <- additional_cost * trees_planted  # Total additional cost for 1M trees\n\n# Hypothetical profit per m³ (simplified for illustration)\n# In reality, this would be: Product revenue - Stumpage - Harvest costs - Processing\nprofit_per_m3 <- 50  # dollars per cubic meter\n\n# Calculate economic return for each posterior sample\n# volume_gain is per-tree gain (m³), so multiply by trees_planted for total\neconomic_return <- volume_gain * profit_per_m3 * trees_planted  # Total revenue gain for 1M trees\nnet_benefit <- economic_return - total_investment                # Net benefit for 1M trees\nroi <- 100 * net_benefit / total_investment                      # ROI as percentage\n\n# Summarize economic outcomes (all values now represent the full 1M tree investment)\ntibble(\n  Metric = c(\n    \"Total investment\",\n    \"Expected additional revenue\",\n    \"Expected net benefit\",\n    \"Expected ROI\",\n    \"95% CI for net benefit\",\n    \"Probability of positive ROI\"\n  ),\n  Value = c(\n    sprintf(\"$%.2f million\", total_investment / 1e6),\n    sprintf(\"$%.2f million\", mean(economic_return) / 1e6),\n    sprintf(\"$%.2f million\", mean(net_benefit) / 1e6),\n    sprintf(\"%.1f%%\", mean(roi)),\n    sprintf(\"[$%.2f, $%.2f] million\", \n            quantile(net_benefit, 0.025) / 1e6, \n            quantile(net_benefit, 0.975) / 1e6),\n    sprintf(\"%.3f\", mean(net_benefit > 0))\n  )\n) %>%\n  knitr::kable(caption = \"Economic analysis: Tree improvement investment (simplified model)\",\n             format = \"html\",\n             escape = FALSE,\n             row.names = FALSE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n<caption>Economic analysis: Tree improvement investment (simplified model)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Metric </th>\n   <th style=\"text-align:left;\"> Value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Total investment </td>\n   <td style=\"text-align:left;\"> $2.00 million </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Expected additional revenue </td>\n   <td style=\"text-align:left;\"> $5.25 million </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Expected net benefit </td>\n   <td style=\"text-align:left;\"> $3.25 million </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Expected ROI </td>\n   <td style=\"text-align:left;\"> 162.6% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 95% CI for net benefit </td>\n   <td style=\"text-align:left;\"> [$-6.09, $12.54] million </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Probability of positive ROI </td>\n   <td style=\"text-align:left;\"> 0.755 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n### Visualizing the Economic Decision\n\nThe plots below provide a complete visualization of the investment decision by showing the full distribution of possible economic outcomes. The upper panel displays the net benefit distribution (revenue minus costs), measured in millions of dollars. The red vertical line marks the breakeven point (zero net benefit), while the blue line shows the expected value. The distribution's spread reveals the range of possible outcomes given all uncertainties in our model.\n\nThe lower panel shows the return on investment (ROI) as a percentage of the $2 million investment. Together, these visualizations answer the key business question: **What's the probability this investment pays off, and by how much?** Notice that while both distributions center well above breakeven (suggesting the investment is likely profitable), there remains meaningful probability mass on both sides of zero -- reflecting honest uncertainty about whether the program will succeed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create dataframe for plotting\neconomic_df <- tibble(\n  volume_gain = as.vector(volume_gain),\n  net_benefit = as.vector(net_benefit),\n  roi = as.vector(roi)\n)\n\n# Plot net benefit distribution\np1 <- ggplot(economic_df, aes(x = net_benefit / 1e6)) +\n  geom_histogram(bins = 50, fill = \"#1b9e77\", alpha = 0.7) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(net_benefit) / 1e6, \n             linetype = \"dashed\", color = \"blue\", linewidth = 1) +\n  labs(\n    title = \"Posterior Distribution of Net Benefit\",\n    subtitle = \"Red line = break-even | Blue line = expected value\",\n    x = \"Net Benefit ($ millions)\",\n    y = \"Count\",\n    caption = sprintf(\"P(positive ROI) = %.3f\", mean(net_benefit > 0))\n  ) +\n  theme_minimal()\n\n# Plot ROI distribution\np2 <- ggplot(economic_df, aes(x = roi)) +\n  geom_histogram(bins = 50, fill = \"#7570b3\", alpha = 0.7) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(roi), \n             linetype = \"dashed\", color = \"blue\", linewidth = 1) +\n  labs(\n    title = \"Posterior Distribution of ROI\",\n    subtitle = \"Return on $2 million investment\",\n    x = \"ROI (%)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\np1 / p2\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n**Interpreting these results**: The probability of positive ROI (75.5%) indicates this investment is likely worthwhile. However, the wide distributions remind us that outcomes could range from modest losses to substantial gains. A conservative decision-maker might want to see probability > 90% before proceeding, while a more risk-tolerant organization might accept 70-80% as sufficient. The Bayesian framework makes these risk preferences explicit rather than hiding them in arbitrary assumptions.\n\n### What's the Minimum Required Gain?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# What volume gain do we need to break even?\nbreakeven_gain <- total_investment / (profit_per_m3 * trees_planted)\nbreakeven_percent <- 100 * breakeven_gain / mean(pred_wild)\n\n# Probability we exceed breakeven\nprob_exceed_breakeven <- mean(volume_gain > breakeven_gain)\n\n# Create summary table\ntibble(\n  Metric = c(\n    \"Breakeven volume gain\",\n    \"Breakeven percent gain\",\n    \"P(exceeding breakeven)\"\n  ),\n  Value = c(\n    sprintf(\"%.3f m³ per tree\", breakeven_gain),\n    sprintf(\"%.1f%%\", breakeven_percent),\n    sprintf(\"%.3f\", prob_exceed_breakeven)\n  )\n) %>%\n  knitr::kable(caption = \"Breakeven analysis\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Breakeven analysis\n\n|Metric                 |Value             |\n|:----------------------|:-----------------|\n|Breakeven volume gain  |0.040 m³ per tree |\n|Breakeven percent gain |11.4%             |\n|P(exceeding breakeven) |0.755             |\n\n\n:::\n:::\n\n\n::: {.callout-warning}\n## Business decision: Investment NOT recommended\n\nWith the estimated volume gains and current profit margins, there is only a **75.5% probability** that the tree improvement investment will be profitable. \n\n**This means there's a 24.5% chance of LOSING money on this investment.**\n\n### Why this is a poor investment:\n\n- The expected volume gains are **not sufficient** to offset the $2 million additional cost\n- The risk is **asymmetric**: small upside potential vs. substantial downside risk\n- With only a 1-in-3 chance of breaking even, this investment fails basic financial due diligence\n\n### What would need to change?\n\nFor this investment to make sense, one or more of the following would need to improve:\n\n1. **Larger volume gains**: Breeding program would need to deliver >20% gains consistently\n2. **Higher profit margins**: Product prices would need to increase or costs decrease  \n3. **Lower seedling costs**: Improved seed costs would need to decrease\n4. **Other benefits not captured**: Improved form, disease resistance, or faster growth to rotation\n\nDecision-makers should **reject this investment** unless substantial improvements in these factors can be demonstrated through additional trials or market analysis.\n:::\n\n\n### Sensitivity to Profit Margins\n\nHow sensitive is our investment decision to changes in profit margins? Let's examine scenarios ranging from depressed markets (lower margins) to boom conditions (higher margins):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Explore different profit margin scenarios\nprofit_scenarios <- tibble(\n  scenario = c(\"Low (depressed market)\", \"Current (baseline)\", \"High (favorable market)\"),\n  profit_per_m3 = c(30, 50, 70)  # dollars per cubic meter\n)\n\n# Calculate ROI for each scenario\nroi_by_scenario <- profit_scenarios %>%\n  mutate(\n    economic_return = map_dbl(profit_per_m3, \n                              ~mean(volume_gain) * .x * trees_planted),\n    net_benefit = economic_return - total_investment,\n    roi = 100 * net_benefit / total_investment,\n    prob_positive = map_dbl(profit_per_m3,\n                           ~mean(volume_gain * .x * trees_planted > total_investment))\n  )\n\nroi_by_scenario %>%\n  select(scenario, profit_per_m3, roi, prob_positive) %>%\n  knitr::kable(\n    digits = 1,\n    caption = \"ROI sensitivity to profit margins\",\n    col.names = c(\"Market Scenario\", \"Profit Margin ($/m³)\", \"Expected ROI (%)\", \"P(positive ROI)\")\n  )\n```\n\n::: {.cell-output-display}\n\n\nTable: ROI sensitivity to profit margins\n\n|Market Scenario         | Profit Margin ($/m³)| Expected ROI (%)| P(positive ROI)|\n|:-----------------------|--------------------:|----------------:|---------------:|\n|Low (depressed market)  |                   30|             57.6|             0.7|\n|Current (baseline)      |                   50|            162.6|             0.8|\n|High (favorable market) |                   70|            267.7|             0.8|\n\n\n:::\n\n```{.r .cell-code}\nrm(list = ls())\n```\n:::\n\n\n::: {.callout-tip}\n## Key insight\n\nEven in a depressed market (\\$30/m³ profit margin), the investment still shows reasonable returns. In favorable markets (\\$70/m³), returns are substantial. This sensitivity analysis demonstrates that the investment decision is **robust to margin fluctuations** -- a critical consideration for long-term forestry investments that span decades and multiple market cycles.\n\nThe high probability of positive ROI across all margin scenarios (assuming our volume gain estimates are accurate) provides confidence for decision-making even under market uncertainty.\n:::\n\n\n## Real-World Application: The Alberta Tree Improvement Investment Problem\n\n**How would this analysis work in practice for an Alberta forest company?** The simplified example above illustrates the Bayesian workflow clearly, but real forestry decisions involve additional layers of complexity -- particularly around regulatory approval. Does this interest you? Just follow the rabbit down the hole 🐰 \n\n::: {.callout-note collapse=\"true\"}\n\n### The Business Problem 🐰\n\nAlberta forest companies face a multi-million dollar question: Should we invest in tree improvement programs? The challenge isn't biological -- field trials consistently show 10-25% volume gains from improved seedlings. The challenge is that **government recognizes only a fraction of demonstrated gains for regulatory purposes**.\n\nIn Alberta's Forest Management Agreement (FMA) system, companies lease harvesting rights on Crown land. To access additional timber volume from tree improvement, they need government approval to adjust their Annual Allowable Cut (AAC) -- a mechanism called the Allowable Cut Effect (ACE). \n\n**The reality:** As of 2016, seven Controlled Parentage Programs (tree improvement programs) existed in Alberta with government-approved height gains averaging 2.3% -- far below the 10-25% demonstrated in field trials. Of these seven programs, only three companies found the economics compelling enough to incorporate the gains into their Forest Management Plans.\n\nWhy did the other four companies decline? With only 2.3% recognized gains, the return on investment becomes marginal. When you're investing millions in breeding programs, seed orchards, and improved seedlings at \\$2.50 each (vs. \\$0.50 for wild seed), the economics only work if enough factors align favorably: high deployment rates, manageable costs, good survival rates, favorable markets.\n\nFrom industry's perspective [@schreiber2017forest]: *\"The low levels of genetic gain and its translation into the desired ACE, the long timelines for TI program development (i.e., decades), and the lack of certainty and risk aversion with Government approvals do not justify the expenses involved.\"*\n\n### Why Standard Cost-Benefit Analysis Falls Short\n\n@schreiber2017forest developed an economic model (TIIFA) for Alberta tree improvement programs. Using sensitivity analysis across different scenarios, they showed that investment **can** be profitable even with modest gains -- but profitability depends on multiple uncertain factors aligning favorably.\n\n**Their key findings:**\n\n- **With 2% volume gain per decade** and deployment reaching 15%+, investment is profitable at 8% discount rate\n- **Deployment area matters as much as genetic gain**: Increasing deployment from 5% to 20% per decade has enormous impact\n- **Program costs are manageable**: Even at \\$5 million per decade, programs can be profitable\n- **Discount rate is critical**: NPV strongly positive at 4%, moderately positive at 8%, barely positive at 12%\n\nBut their model analyzed scenarios independently. In reality, companies face **compound uncertainty**:\n\n- Will we achieve 15% deployment rates across our FMA area? (Operational)\n- Will program costs stay under $5M per decade? (Financial)\n- Will the 2.3% recognized gains actually translate to field performance? (Biological)\n- What will stumpage fees and lumber prices be over the 20-year FMP cycle? (Market)\n- Will climate change affect realization of genetic gains? (Environmental)\n\n**Their conclusion:** \"*Investment in TI still remains a profitable enterprise..*. **if** *the area planted is maximized on which improved seed is deployed.*\"\n\nThat \"if\" is doing a lot of work. When four out of seven companies look at the same analysis and decide **not** to incorporate gains into their plans, they're implicitly saying: \"*We're not confident enough that the conditions will align favorably.*\"\n\nStandard economic models give point estimates under **assumed** scenarios. What companies actually need to know: *\"What's the probability our investment will be profitable given simultaneous uncertainty across all these factors?\"*\n\n**Government faces similar challenges.** Regulators must balance economic benefits (forest industry jobs, community sustainability) against conservation of public forests. When field trials show 10-25% gains but regulators approve only 2.3%, this conservatism reflects genuine uncertainty: Will gains materialize at scale across diverse sites? Will deployment rates be sufficient? A Bayesian framework could help regulators make more transparent, defensible decisions by quantifying: \"At 80% confidence, we expect gains between 1.8-2.8%; approving 2.3% AAC increases keeps risk of over-harvest below 10%.\" This provides accountable stewardship of public resources while supporting industry investment.\n\n### The Bayesian Advantage\n\nA Bayesian framework naturally handles **compound uncertainty** by treating each uncertain factor as a probability distribution rather than a fixed assumption:\n\n**Instead of:** \"Assume we achieve 15% deployment\"  \n**Bayesian says:** \"Deployment rate ~ Beta($\\alpha$, $\\beta$) based on historical rates across our FMA areas\"\n\n**Instead of:** \"Assume costs stay at \\$3M per decade\"  \n**Bayesian says:** \"Program costs ~ LogNormal($\\mu$, $\\sigma$) based on past breeding program expenses\"\n\n**Instead of:** \"Assume 2.3% gains translate to field performance\"  \n**Bayesian says:** \"Realized gains ~ Normal(0.023, $\\sigma$) accounting for site-to-site variation\"\n\nThen propagate **all** these uncertainties simultaneously through the economic model:\n\nA Bayesian framework handles this the same way we analyzed tree height-diameter relationships earlier in this post: treat each uncertain input as a probability distribution, sample from all distributions simultaneously, and propagate uncertainty through the economic calculation. Instead of running the analysis once with fixed assumptions, you run it thousands of times, each time drawing different combinations of deployment rates, costs, gains, and market conditions from their respective distributions.\n\nThe result isn't a single number but a full probability distribution of outcomes. Instead of \"Expected NPV = \\$7.4M,\" you get probability statements that directly answer the business question: \"There's a 73% chance of positive returns, a 45% chance NPV exceeds \\$5M, and an 8% chance of losses over \\$2M.\"\nThis directly answers the business question: \"**What's the probability this investment pays off given everything we're uncertain about?**\"\n\n@schreiber2017forest key insight makes it even more important: Deployment area matters as much as genetic gain. In Bayesian terms: If you can **control** deployment (reduce its uncertainty), you dramatically improve the probability distribution of returns. A company that commits to 20% deployment with high confidence has much better odds than one hoping for 15% but uncertain about achieving it.\n\n### Why This Matters\n\nForest companies face **compound business uncertainty** when evaluating long-term investments:\n\n**Operational uncertainties:**\n\n- Can we realistically achieve 15-20% deployment across diverse FMA areas?\n- Will field crews consistently plant improved stock rather than wild seed?\n- How do deployment logistics vary by terrain and access?\n\n**Financial uncertainties:**\n\n- Will breeding program costs stay manageable over decades?\n- What will improved seedling costs be as production scales?\n- How sensitive are returns to discount rate assumptions?\n\n**Biological uncertainties:**\n\n- Will 2.3% recognized gains translate to actual field performance?\n- How much site-to-site variation should we expect?\n- Will climate change affect genetic gain realization?\n\n**Market uncertainties:**\n\n- What will stumpage fees and lumber prices be over 20-year FMP cycles?\n- How do mill closures or expansions affect our timber allocation?\n\nStandard economic models analyze scenarios independently: \"If deployment = 15% and costs = \\$3M and gains = 2.3%, then NPV = \\$7.4M.\" \n\nBayesian models integrate across all uncertainties simultaneously: \"Accounting for realistic uncertainty in deployment (10-25%), costs (\\$2-6M), gain realization (1.8-2.8%), and markets, there's a **73% probability of positive NPV** and a **45% probability NPV exceeds \\$5M**.\"\n\n**The critical insight:** When only 3 of 7 companies proceed, it suggests the others looked at compound uncertainty and concluded the probability of success was too low. A Bayesian framework makes that reasoning explicit and quantifiable.\n\nMoreover, it identifies **which uncertainties matter most**. If deployment rate is the dominant factor (as @schreiber2017forest suggest), companies can focus on operational commitments that reduce deployment uncertainty rather than waiting for higher genetic gains. A company that can guarantee 20% deployment with high confidence faces much better odds than one hoping for 25% genetic gains that regulators may not recognize.\n\nThe Bayesian workflow we've demonstrated -- specify priors, update with data, propagate uncertainty, make probability statements -- applies directly to these multi-million dollar, multi-decade investment decisions where numerous uncertainties compound.\n\n\nThe gap between 2.3% recognized gains and 10-25% demonstrated gains creates a difficult business problem: small recognized gains mean economics are marginal, and compound uncertainties make outcomes unpredictable. Only 3 of 7 Alberta programs found the risk-reward balance favorable. **Quantifying probability of success under compound uncertainty** -- rather than analyzing scenarios independently -- is exactly where Bayesian methods add value for complex, long-term business decisions.\n:::\n\n\n# Key Takeaways: From Seed Germination to Tree Improvement to Business Decisions\n\n## The Journey We've Taken\n\nOver this three-post series, we've built a complete understanding of statistical inference:\n\n**Post 1 (Frequentist Foundations):** We learned that frequentist inference is about procedures with long-run error rates. Confidence intervals describe what happens across hypothetical repetitions. P-values measure compatibility with null models. These tools are powerful when you want known error rates for repeated decisions.\n\n**Post 2 (Bayesian Foundations):** We shifted to thinking about parameters as random variables representing our uncertainty. We learned that Bayesian inference naturally handles sequential learning, allows direct probability statements, and makes assumptions explicit through priors. The seed germination example showed these principles in action for a single parameter.\n\n**Post 3 (This Post):** We scaled those principles to multiple parameters without changing the fundamental logic. Tree height regression used the same workflow: specify priors, check if they're sensible, update with data, validate the model, make probabilistic predictions. The tree improvement example showed how this framework naturally handles real business decisions under uncertainty.\n\n## Core Principles That Carry Forward\n\n**1. Prior choice is critical but not mysterious**\n\n- Don't use flat priors -- they allow nonsense\n- Use weakly informative priors that encode basic domain knowledge\n- Always conduct prior predictive checks to validate your choices\n- Your priors are assumptions made explicit, not a source of bias\n\n**2. Probability statements answer real questions**\n\n- \"P(β > 0) = 0.98\" directly answers \"Is the effect positive?\"\n- \"P(ROI > 0) = 0.85\" directly answers \"Will this investment pay off?\"\n- No need to translate between p-values and practical meaning\n\n**3. Uncertainty propagates through all calculations**\n\n- From parameter posteriors → to predictions → to economic outcomes\n- This prevents false precision and enables realistic planning\n- Point estimates hide critical information that full distributions reveal\n\n**4. The Bayesian workflow is consistent**\n\n1. Specify priors (encode domain knowledge)\n2. Check priors (prior predictive checks)\n3. Fit model (MCMC sampling)\n4. Check convergence (Rhat, ESS, trace plots)\n5. Validate model (posterior predictive checks)\n6. Make inferences (probability statements)\n7. Make predictions (full uncertainty)\n\n**5. Model checking is essential**\n\n- Prior predictive checks prevent nonsense inputs\n- Posterior predictive checks catch model failures\n- Diagnostic plots reveal computational problems\n- Bad models give unreliable answers -- always validate\n\n## What Changes and What Doesn't\n\n**From 1 parameter to many:**\n\n- ✓ Same conceptual framework\n- ✓ Same workflow steps\n- ✓ Same interpretation of credible intervals\n- ✗ More complex prior specification (but same principles)\n- ✗ More computationally demanding (but MCMC handles it)\n- ✗ More difficult visualization (but 2D plots of key relationships work)\n\n**From simple models to complex ones:**\n\n- The principles in this series extend to hierarchical models, time series, spatial statistics, and more\n- Complexity grows in implementation, not in conceptual foundation\n- If you understand updating beliefs about germination rates, you understand Bayesian inference\n\n## Practical Wisdom\n\n**When Bayesian methods shine:**\n\n- Making sequential decisions (examine data as it arrives)\n- Quantifying probability of specific outcomes (P(effect > threshold))\n- Incorporating prior knowledge from previous studies\n- Communicating uncertainty to non-statisticians\n- Small sample sizes where prior information helps\n\n**When to be cautious:**\n\n- Prior specification requires thought -- no autopilot\n- Computational demands can be substantial\n- Different analysts with different priors may disagree (but so will frequentist analysts with different analysis choices)\n- Skeptical audiences may question prior choices (though the same audiences accept arbitrary $\\alpha$-levels)\n\n**The bottom line:**\n\nBoth frequentist and Bayesian frameworks are legitimate approaches to statistical inference. They answer different questions and suit different contexts. The frequentist approach in Post 1 remains widely used, well-understood, and appropriate for many scientific questions. The Bayesian approach offers advantages for decision-making under uncertainty, sequential learning, and direct quantification of what we want to know.\n\nThe shift from thinking about procedures to thinking about beliefs is the essence of Bayesian inference. Once you internalize this shift -- as we've done across three posts from seeds to trees -- you can apply the same reasoning to arbitrarily complex models.\n\nThe real power isn't in the fancy mathematics or the MCMC algorithms. It's in the conceptual clarity of saying \"*Here's what I believed before, here's what I observed, here's what I believe now, and here's the probability of what I care about.*\"\n\n**That's Bayesian inference. You already know how to think this way -- we've just given you the mathematical tools to do it rigorously.**\n\n# References\n\n::: {#refs}\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}