{
  "hash": "46c64cbd3ff87d740ffa61221e3c3bcf",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Understanding Bayesian Predictions\"\nsubtitle: \"From Complex MCMC Algorithms to Simple Matrix Arithmetic\"\nauthor: \"Stefan Schreiber\"\ndate: 2025-11-05\ncategories: [bayesian, prediction, inference]\n\n# Local image for the page\nimage: bayesian_prediction.png\nimage-alt: \"Bayesian Prediction\"\n\ncitation:\n  url: https://envirostats.ca/posts/2025-11-05-understanding-bayesian-predictions/\n\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: show\n    code-summary: \"Show code\"\n    df-print: paged\n\neditor: source\n\nexecute:\n  echo: true\n  warning: false\n  message: false\n  cache: true\n  freeze: auto\n\ndraft: false  \n---\n![From uncertainty, form arises ‚Äî the flow from belief to prediction](bayesian_prediction.png)\n<br>\n\nSo you've just fit your first Bayesian model using `brms` or `rstanarm`. Maybe you were surprised by how easy it was -- the syntax looked almost exactly like `lm()` for a simple linear regression. You hit run, waited a bit longer than usual, and got... output. Lots of output.\n\nYou look at the summary. There are means and standard deviations, credible intervals, and mysterious diagnostics like \"Rhat\" and \"ESS\". But wait -- where are the p-values? And how do you actually make predictions? People keep talking about \"posterior predictions\" and \"posterior distributions\" -- what do those actually mean, and how do you work with them?\n\nIf you've ever felt confused by the terminology around Bayesian predictions -- posterior predictive distributions, expected predictions, prediction intervals -- you're not alone. But here's the secret: once your Bayesian model calculates the posterior distribution, **everything else is just computing means, quantiles, and other summary statistics**. Let me show you what I mean.\n\n\n## The Core Insight\n\nBayesian inference has two phases:\n\n1. **The hard part**: Computing the posterior distribution $p(\\theta\\mid D)$ (this is where MCMC).\n2. **The easy part**: Summarizing that posterior and quantities derived from it -- means, standard deviations, quantiles, etc.\n\nOnce you have posterior samples from $p(\\theta\\mid D)$, you can simulate the posterior predictive distribution $p(\\tilde{y}\\mid D)$ and summarize it with its mean $E[\\tilde{y}\\mid D]$. We‚Äôll return to this trio when it becomes most helpful.\n\n## Why is the hard part hard?\n\nBefore the ‚Äúeasy part,‚Äù a quick sketch of why computing the posterior is challenging (you don't have to master this to *use* Bayesian methods, but the intuition helps).\n\n### The goal: a joint probability over all parameters\n\nBayes' theorem:\n\n$$\np(\\theta \\mid D) \\;=\\; \\frac{p(D \\mid \\theta)\\,p(\\theta)}{p(D)}\n$$\n\n- $p(\\theta \\mid D)$: **posterior** -- what we believe about parameters after seeing data  \n- $p(D \\mid \\theta)$: **likelihood** -- how well parameters explain the data  \n- $p(\\theta)$: **prior** -- what we believed before seeing data  \n- $p(D)$: **marginal likelihood / evidence** -- a normalizing constant\n\nBecause $p(D)$ is often intractable, we work with $p(\\theta \\mid D) \\propto p(D \\mid \\theta)\\, p(\\theta)$. For a simple regression with parameters $\\alpha,\\beta,\\sigma$:\n\n$$\np(\\alpha,\\beta,\\sigma \\mid D) \\propto p(D \\mid \\alpha,\\beta,\\sigma)\\, p(\\alpha,\\beta,\\sigma).\n$$\n\nThis is a joint distribution in a high-dimensional space. More parameters ‚áí higher dimension.\n\n### Starting simple: grid approximation\n\nSuppose you‚Äôre a fish biologist interested in the hatching rate of rainbow trout. You run an experiment and **4 out of 10 eggs hatched** -- what‚Äôs the hatching rate $\\theta$? With one parameter, a **grid approximation** shows the mechanics explicitly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# Define a grid of possible hatching rates from 0 to 1\ntheta_grid <- seq(from = 0, to = 1, length.out = 100)\n\n# Prior: assume all values equally plausible (flat prior)\nprior_flat <- rep(1, 100)\n\n# Prior options:\nprior_flat <- rep(1, 100)                        # Uninformative\nprior_conservative <- dbeta(theta_grid, 5, 5)    # Moderate rates likely\nprior_optimistic <- dbeta(theta_grid, 8, 2)      # High rates likely\nprior_skeptical <- dbeta(theta_grid, 2, 8)       # Low rates likely\n\n# üëâ CHANGE THIS LINE to try different priors:\nprior <- prior_flat   \n\n# Likelihood: how likely is observing 4/10 for each possible Œ∏?\n# Observed data: 4 hatched out of 10 eggs\nsuccesses <- 4\ntrials <- 10\nobserved_rate <- successes / trials\n\nlikelihood <- dbinom(successes, size = trials, prob = theta_grid)\n\n# Posterior ‚àù Likelihood √ó Prior, then normalize\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\n# Visualize the posterior distribution\ndata.frame(theta = theta_grid, posterior = posterior) |>\n  ggplot(aes(x = theta, y = posterior)) +\n  geom_line() +\n  geom_area(alpha = 0.3) +\n# Dashed red line = observed rate (from data), stays fixed\n# Shows how different priors shift the posterior relative to observed data\n  geom_vline(xintercept = observed_rate, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Hatching rate (Œ∏)\",\n       y = \"Posterior probability\",\n       title = \"Posterior distribution with 100 grid points\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/grid-approximation-1.png){width=672}\n:::\n:::\n\n\n**Quick tie-in:** From this posterior $p(\\theta\\mid D)$, we can simulate a posterior predictive distribution $p(\\tilde{y}\\mid D)$ (e.g., future hatch counts) and summarize it with the expected prediction $E[\\tilde{y}\\mid D]$.\n\n\n### The curse of dimensionality (and continuity)\n\n- 1 parameter, 100 grid points ‚áí 100 evaluations  \n- 2 parameters ‚áí $100^2 = 10{,}000$  \n- 3 parameters ‚áí $100^3 = 1{,}000{,}000$  \n- 10 parameters ‚áí $100^{10}$ (astronomical)\n\nMost parameters are also **continuous**, so inference involves integrals over infinitely many values. For instance, marginalizing $\\beta$:\n\n$$\np(\\alpha \\mid D) \\;=\\; \\int p(\\alpha,\\beta \\mid D)\\,d\\beta.\n$$\n\nAnalytical solutions exist for special conjugate cases; real-world models usually require computation.\n\n### The solution: MCMC explores the joint probability space\n\n**Markov chain Monte Carlo (MCMC)** avoids gridding and intractable integrals by *sampling* parameter values, spending more time in high-probability regions of the **joint posterior space**. After convergence and warm-up, the draws are (approximately) samples from the posterior $p(\\theta \\mid D)$.\n\nThe result is a table of draws (rows = draws, columns = parameters). **That table is the payoff.** From here, we move to predictions via the posterior predictive distribution $p(\\tilde{y} \\mid D)$ and summaries such as its expectation $E[\\tilde{y} \\mid D]$.\n\n\n## A simple example: the matrix structure\n\nNow, how do posterior **parameter** draws become **predictions**? Think in matrices.\n\nEach column corresponds to one posterior draw of $(\\alpha,\\beta,\\sigma)$. For each draw and each observation $i$, compute the linear predictor $\\mu_i = \\alpha + \\beta x_i$. Stack those $\\mu$ values into a matrix: rows = observations, columns = draws.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# IMPORTANT: This uses only 3 draws for illustration.\n# Real analyses use 1,000‚Äì4,000+ draws.\n\npredicted_matrix <- matrix(c(\n  2.0, 2.1, 1.9,  # Obs 1: Œº‚ÇÅ from posterior draws 1, 2, 3\n  1.0, 1.2, 1.1,  # Obs 2: Œº‚ÇÇ from posterior draws 1, 2, 3\n  4.0, 3.8, 4.2,  # Obs 3: Œº‚ÇÉ from posterior draws 1, 2, 3\n  1.0, 1.1, 0.9   # Obs 4: Œº‚ÇÑ from posterior draws 1, 2, 3\n), nrow = 4, byrow = TRUE)\n\nrownames(predicted_matrix) <- c(\"Obs 1\", \"Obs 2\", \"Obs 3\", \"Obs 4\")\ncolnames(predicted_matrix) <- c(\"Draw 1\", \"Draw 2\", \"Draw 3\")\n```\n:::\n\n\n**Why this matters for our trio:**  \n- The columns come from $p(\\theta\\mid D)$.  \n- The matrix of $\\mu$ values is the scaffold we use to simulate $p(\\tilde{y}\\mid D)$.  \n- Row summaries of that simulated distribution give $E[\\tilde{y}\\mid D]$ and intervals.\n\nLet's display the matrix with row means (i.e., expected predictions of the mean response for this toy example):\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Prediction Matrix: Each row is an observation, each column is a posterior draw\n\n|      | Draw 1| Draw 2| Draw 3| Row Mean|\n|:-----|------:|------:|------:|--------:|\n|Obs 1 |      2|    2.1|    1.9|      2.0|\n|Obs 2 |      1|    1.2|    1.1|      1.1|\n|Obs 3 |      4|    3.8|    4.2|      4.0|\n|Obs 4 |      1|    1.1|    0.9|      1.0|\n\n\n:::\n:::\n\n\n<br>\n\nOnce predictions are organized this way, everything reduces to simple arithmetic across columns:\n\n- **Expected predictions** $E[\\tilde{y}\\mid D]$: row means of $\\mu_i$\n- **Credible intervals** for the **mean response**: rowwise quantiles of $\\mu_i$\n- Next we‚Äôll add $\\sigma$ to generate draws of $\\tilde{y}$ ‚áí $p(\\tilde{y}\\mid D)$ and prediction intervals\n\n## The three-level hierarchy\n\nTo pull our ideas together, here‚Äôs the compact relationship we‚Äôll now use repeatedly:\n\n```\nParameters Œ∏  ‚Üí  Data y\np(Œ∏|D)        ‚Üí  p(·ªπ|D)      ‚Üí  E[·ªπ|D]\nposterior        posterior       mean of the\ndistribution     predictive       posterior predictive\n                 distribution     (expected prediction)\n```\n\n| Level | Distribution | What it describes | Uncertainty captured |\n|------:|--------------|-------------------|----------------------|\n| 1 | $p(\\theta\\mid D)$ | What we believe about parameters after seeing data | **Parameter** uncertainty |\n| 2 | $p(\\tilde{y}\\mid D)$ | What we believe about future/unobserved data | **Parameter + observation** uncertainty |\n| 3 | $E[\\tilde{y}\\mid D]$ | Mean of the posterior predictive distribution | Center only (a summary) |\n\nPlain language: posterior over parameters ‚áí simulate data from it ‚áí average those simulations.\n\n## Understanding the two kinds of predictions\n\n### Expected predictions: $E[\\tilde{y}\\mid D]$\n\nFor a Normal linear regression with $y_i \\sim \\text{Normal}(\\mu_i,\\sigma)$ and $\\mu_i = \\alpha + \\beta x_i$, $E[y\\mid\\mu,\\sigma] = \\mu$, so $E[\\tilde{y}\\mid D] = E[\\mu\\mid D]$. We can compute this as the mean of the $\\mu_i$ across posterior draws.\n\n**Why this works:** The Normal distribution is centered at $\\mu$. When we \naverage over many possible future observations (each drawn from $\\text{Normal}(\\mu, \\sigma)$), \nwe get $\\mu$ back -- the noise $\\sigma$ averages out to zero. Then we just average \nthose $\\mu$ values across our posterior draws.\n\n**When this breaks:** In logistic regression, your linear predictor might \naverage to 0, but $\\text{logit}^{-1}(0) = 0.5$ while the average probability \ncould be 0.6. Non-linear transformations don't commute with averaging. This is \nwhy we simulate full draws from $p(\\tilde{y}|D)$ first, then summarize -- it \nworks for any model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Expected predictions: mean of Œº·µ¢ across posterior draws\nepred <- rowMeans(predicted_matrix)\nprint(epred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nObs 1 Obs 2 Obs 3 Obs 4 \n  2.0   1.1   4.0   1.0 \n```\n\n\n:::\n:::\n\n\n\n### Posterior predictions: samples from $p(\\tilde{y}\\mid D)$\n\nTo see the full range of plausible observations (not just the mean response), we add observation noise $\\sigma$ to the $\\mu$ values and simulate $\\tilde{y}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\n# Posterior draws of œÉ (one for each column)\nsigma_draws <- c(0.5, 0.6, 0.55)\n\n# Generate ·ªπ·µ¢ ~ Normal(Œº·µ¢, œÉ) for each draw\ny_pred_matrix <- predicted_matrix\nfor (j in 1:3) {\n  y_pred_matrix[, j] <- predicted_matrix[, j] + rnorm(4, 0, sigma_draws[j])\n}\n```\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Posterior predictions: samples from p(·ªπ|D)\n\n|      | Draw 1| Draw 2| Draw 3|\n|:-----|------:|------:|------:|\n|Obs 1 |   1.72|   2.18|   1.52|\n|Obs 2 |   0.88|   2.23|   0.85|\n|Obs 3 |   4.78|   4.08|   4.87|\n|Obs 4 |   1.04|   0.34|   1.10|\n\n\n:::\n:::\n\n\n<br> \n\nThese $\\tilde{y}$ values vary more than the $\\mu$ values because they include $\\sigma$: the *irreducible* variation around the mean response.\n\n### Two types of intervals (and what they mean)\n\n**Credible intervals (for the mean response)**--from the $\\mu_i$ matrix (parameter uncertainty only):\n\n\n::: {.cell}\n\n```{.r .cell-code}\napply(predicted_matrix, 1, quantile, probs = c(0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Obs 1 Obs 2 Obs 3 Obs 4\n2.5%  1.905 1.005  3.81 0.905\n97.5% 2.095 1.195  4.19 1.095\n```\n\n\n:::\n:::\n\n\n**Prediction intervals (for future observations)**--from the $\\tilde{y}$ matrix (parameter + observation uncertainty):\n\n\n::: {.cell}\n\n```{.r .cell-code}\napply(y_pred_matrix, 1, quantile, probs = c(0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Obs 1     Obs 2   Obs 3     Obs 4\n2.5%  1.532107 0.8563872 4.11169 0.3756778\n97.5% 2.154682 2.1618326 4.86855 1.0947654\n```\n\n\n:::\n:::\n\n\nPrediction intervals are always wider: they include both uncertainty about the mean response and the natural spread of outcomes.\n\n::: callout-tip\n## Decision guide (tidybayes/R workflow)\n\n| Question | Use | What you get | Function |\n|----------|-----|--------------|----------|\n| What's the average outcome? | Expected predictions | $E[\\tilde{y}\\mid D]$ | `add_epred_draws()` |\n| What might I observe? | Posterior predictions | draws from $p(\\tilde{y}\\mid D)$ | `add_predicted_draws()` |\n| Is my model realistic? | Posterior predictions | compare simulations to data | `add_predicted_draws()` |\n| What's the effect of X? | Expected predictions | change in $E[\\tilde{y}\\mid D]$ | `add_epred_draws()` |\n| Do I need prediction intervals? | Posterior predictions | full uncertainty range | `add_predicted_draws()` |\n:::\n\n**Mini-recap:** we start with $p(\\theta\\mid D)$, simulate $p(\\tilde{y}\\mid D)$, and summarize with $E[\\tilde{y}\\mid D]$.\n\n## A complete example with real tools\n\nLet's fit a model and produce both kinds of predictions with `rstanarm` and `tidybayes`.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstanarm)\nlibrary(tidybayes)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Simulate some data: y = 2 + 0.5x + noise\nset.seed(123)\nn <- 100\nx <- rnorm(n)\ny <- 2 + 0.5 * x + rnorm(n, sd = 1)\ndata <- data.frame(x = x, y = y)\n\n# Fit Bayesian regression (MCMC to get posterior samples)\nfit <- stan_glm(y ~ x, data = data,\n                family = gaussian(),\n                chains = 4, iter = 2000,\n                refresh = 0)\n```\n:::\n\n\n## Posterior predictive checks: Is your model any good?\n\nBefore we use our model for predictions, we should check: **does data simulated from our model look like our actual data?** If not, our model is missing something important.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bayesplot)\n\n# Extract posterior predictive samples\ny_rep <- posterior_predict(fit)\n\n# Compare distribution of observed vs. simulated data\nppc_dens_overlay(y, y_rep[1:100, ]) +\n  labs(title = \"Posterior predictive check: Density comparison\",\n       subtitle = \"Light blue = 100 draws from p(·ªπ|D), Dark blue = observed data\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/ppc-visual-1.png){width=672}\n:::\n:::\n\n\n**What to look for:** Do the light blue curves (simulated data) cover the dark blue curve (observed data)? If observed data looks like \"just another draw\" from the model, that's good. If it's systematically different (e.g., observed has heavier tails, bimodality, etc.), your model may need revision.\n\n**In our case:** The simulated data (light blue curves) nicely envelope the observed data (dark blue), suggesting our Normal linear model with constant variance is reasonable for this dataset.\n\n### When PPCs reveal problems\n\nIf posterior predictive checks fail, common issues include:\n\n- **Outliers not captured**: Consider robust likelihoods (Student-t instead of Normal)\n- **Non-constant variance**: Add variance modeling (e.g., `sigma ~ x`)\n- **Non-linearity missed**: Add polynomial terms or smooths\n- **Clustering ignored**: Consider hierarchical/mixed models\n\n\n## Visualizing predictions\n\nNow that we've validated our model, let's see both types of predictions in action:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now we can extract both types of predictions:\n# - add_epred_draws(): expected predictions E[·ªπ|D]\n# - add_predicted_draws(): full posterior predictive samples p(·ªπ|D)\n# Let's visualize both simultaneously:\n\ndata |>\n  add_epred_draws(fit) |>\n  ggplot(aes(x = x, y = y)) +\n  # Blue bands: credible intervals for E[Œº|D]\n  stat_lineribbon(aes(y = .epred), .width = c(0.61, 0.89)) +\n  # Black points: observed data\n  geom_point(data = data, size = 2, alpha = 0.8) +\n  # Red points: 50 samples from p(·ªπ|D) showing full predictive distribution\n  geom_point(data = data |> add_predicted_draws(fit) |> sample_draws(50),\n             aes(y = .prediction),\n             color = \"red\", alpha = 0.15, size = 1) +\n  scale_fill_brewer() +\n  labs(title = \"Expected predictions (blue) vs. Posterior predictions (red)\",\n       subtitle = \"Blue bands: E[Œº|D] credible intervals; Red: samples from p(·ªπ|D)\",\n       caption = \"Black = observed data | Blue = credible intervals for mean response | Red = posterior prediction samples\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/visualize-predictions-1.png){width=672}\n:::\n:::\n\n\n### What the figure is showing\n\n- **Black points**: observed data  \n- **Blue bands**: credible intervals for the mean response $E[\\mu\\mid D] = E[\\tilde{y}\\mid D]$ (for this linear-Gaussian model)  \n- **Red points**: samples from $p(\\tilde{y}\\mid D)$--plausible observations that include $\\sigma$\n\nWhy are the red points more dispersed? They include $\\sigma$, the irreducible variation around the mean response. If you re-ran the same study, new observations would scatter around the blue line much like those red points.\n\n\n### Comparing credible intervals vs. prediction intervals\n\nThe plot above shows both types of uncertainty, but let's make the distinction even clearer with a direct comparison:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\n\n# Create new data for prediction\nnewdata <- data.frame(x = seq(-2, 2, length.out = 50))\n\n# Get both types of intervals\nepred_intervals <- newdata |>\n  add_epred_draws(fit) |>\n  median_qi(.epred, .width = 0.89)\n\npred_intervals <- newdata |>\n  add_predicted_draws(fit) |>\n  median_qi(.prediction, .width = 0.89)\n\n# Plot credible intervals (for mean response)\np1 <- ggplot(epred_intervals, aes(x = x, y = .epred)) +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = 0.3, fill = \"blue\") +\n  geom_line(color = \"blue\", size = 1) +\n  geom_point(data = data, aes(x = x, y = y), alpha = 0.3, size = 1) +\n  labs(title = \"89% Credible Intervals for E[Œº|D]\",\n       subtitle = \"Where we expect the mean response to be\",\n       y = \"y\") +\n  theme_minimal() +\n  coord_cartesian(ylim = c(-3, 6))\n\n# Plot prediction intervals (for observations)\np2 <- ggplot(pred_intervals, aes(x = x, y = .prediction)) +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = 0.3, fill = \"red\") +\n  geom_line(color = \"red\", size = 1) +\n  geom_point(data = data, aes(x = x, y = y), alpha = 0.3, size = 1) +\n  labs(title = \"89% Prediction Intervals for p(·ªπ|D)\",\n       subtitle = \"Where we expect future observations to fall\",\n       y = \"y\") +\n  theme_minimal() +\n  coord_cartesian(ylim = c(-3, 6))\n\np1 + p2\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/interval-comparison-1.png){width=960}\n:::\n:::\n\n\n**Key insight:** Both plots show the same fitted line, but:\n\n- **Left (blue)**: Narrow bands showing uncertainty about the *average* relationship. \"If I could repeat this experiment infinite times, where would the mean response at each $x$ value fall?\"\n\n- **Right (red)**: Wide bands showing uncertainty about *individual observations*. \"If I collect one new data point at this $x$ value, where will it likely fall?\"\n\nNotice how the observed data points (black dots) mostly fall within the red bands but often outside the blue bands -- exactly as they should. The blue bands don't try to capture individual observations; they capture the mean trend.\n\n**Practical implication:** \n\n- Planning a policy based on expected outcomes? Use credible intervals (blue).\n- Predicting whether a specific patient will respond? Use prediction intervals (red).\n- Building a forecast with uncertainty bounds? Use prediction intervals (red).\n\n\n## Takeaway: from complex to simple\n\n**Hard part:** compute $p(\\theta\\mid D)$ with MCMC (explore a high-dimensional space; replace intractable integrals with samples).\n\n**Easy part:** treat those samples like columns in a matrix and do arithmetic to move through the trio:\n\n- Posterior $p(\\theta\\mid D)$ ‚áí simulate the linear predictor $\\mu$ for each draw  \n- Add noise to get posterior predictive draws $p(\\tilde{y}\\mid D)$  \n- Summarize by row means/quantiles to get $E[\\tilde{y}\\mid D]$, credible intervals (for means), and prediction intervals (for outcomes)\n\nKeep the trio in mind and everything else becomes mechanical.\n\n## Further reading\n\n- McElreath's [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/) has excellent coverage of posterior prediction and makes these concepts very intuitive\n- The [`tidybayes` documentation](https://mjskay.github.io/tidybayes/) provides great examples of working with posterior draws in a tidy format\n- Gelman et al.'s *Bayesian Data Analysis* (Chapter 7) covers posterior predictive checking in detail\n- Kay's [`tidybayes` vignettes](https://mjskay.github.io/tidybayes/articles/) show practical workflows for Bayesian analysis in R\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}